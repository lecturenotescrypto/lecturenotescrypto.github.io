[["index.html", "Cryptography lecture notes Front page", " Cryptography lecture notes Front page Latest update: 09/06/2025. This page contains some useful information related to the course and the lecture notes. Note that there are some useful buttons above the text, in particular one to download the notes as a well-formatted PDF, for offline use. Prerequisites The course assumes that you are familiar with previous maths courses from your degree. In particular, we will be using concepts from Probability theory and Discrete mathematics, and make extensive use of modular arithmetic. Refreshers for these topics can be found in Appendix D and Section 6.3. Exercises Besides the exercise lists that you will have for practices or seminars, these notes also have some exercises embedded into the explanations. These are mostly easy exercises, designed to be a sort of ‘sanity check’ before moving to the next topic. Hence, our recommendation is that you stop and think about every exercise you encounter in these notes, instead of rushing through the content. You might not always be able to write a full and formal solution, but make sure to get at least an intuition on each exercise before moving on. Furthermore, at the end of each chapter, there are relevant exercises with their solutions. These will both help you understand the concepts presented deeper as well as guide you to formally arguaing and presenting solutions to problems. Exercises marked with (*) are either more challenging or introduce interesting topics that are outside the scope of the course. You can safely skip them the first time you read the material. When you fairly understand a chapter’s content and the rest of the exercises, feel free to come back to these to either further deepen your understanding and/or take a glimpse into more advance and interesting topics related to the chapter. SageMath SageMath (often called just Sage) is a powerful computer algebra system that we will use during the course to illustrate many concepts. It follows the Python syntax, which you will be familiar with, and comes equipped with many functions that are useful for cryptography. These notes will sometimes provide chunks of Sage code, so that you can play with some of the schemes that we will introduce. You are also encouraged to try and implement other schemes, or use Sage to double-check your solutions to exercises. You can find in Appendix E a Sage cookboot giving short, self contained “recipies” for solving and experimenting with problems relevant to the course. You are encouraged to play with these recipies and try to combine them to handle more complex constructions. There are two ways that you can use Sage: Download it from https://www.sagemath.org/download.html. This will allow you to run SageMath locally. It also comes packaged with a Jupyter-style notebook, for ease of use. Use it online at https://cocalc.com/app. This also comes in both terminal and notebook flavors. CoCalc has a freemium model, and in the free version you will get an annoying message telling you that your code will run really slow. Nevertheless, the free version is more than enough for the purpose of this course. The documentation at https://doc.sagemath.org/ is pretty good, although most of the functions that we will use are self-explanatory. Bibliography Boaz Barak. An Intensive Introduction to Cryptography. Available freely from https://intensecrypto.org/public/. Richard Crandall and Carl B Pomerance. Prime numbers: a computational perspective, volume 182. Springer Science &amp; Business Media, 2006. Jonathan Katz and Yehuda Lindell. Introduction to modern cryptography. CRC press, 2020. Christof Paar and Jan Pelzl. Understanding cryptography: a textbook for students and practitioners. Springer Science &amp; Business Media, 2009. Mike Rosulek. The Joy of Cryptography, 2017. Available freely from http://web.engr.oregonstate.edu/~rosulekm/crypto. Changelog 30/05/2025. Add Section 12 about zero knowledge proofs. 30/05/2025. Add alternative definition for adversary’s advantage in Section 4. 25/02/2025. Changes the \\(\\mathbb{Z}_p\\) to \\(\\mathbb{Z}_q\\) in Section 11. 31/01/2025. Added discussion for security definitions in the public key setting in Section 9. 23/01/2025. Added Section 4. 02/01/2025. Exercises with their solutions added at the end of each chapter. Sage cookbook added at Appendix E. Various corrections were made throughout the notes. AES128 diffusion and confusion experiments with Sage added. Remove Linear feedback shift registers subsection. 24/02/2021. Sections 10 and 11 and Appendix C added. 12/02/2021. Section 9 added. Some typos fixed in Sections 7, 8 and Appendix B. 05/02/2021. Section 8 and Appendix B added. Minor fixes in Section 7. CRT moved to Section 6. 26/01/2021. Section 7 and Appendices A and D.4 added. 21/01/2021. Minor fixes and additions in Sections 5 and 6. 18/01/2021. Remainder of Section 6 uploaded. Some typos fixed in Sections 3 and 5. 13/01/2021. Section 5 uploaded. 08/01/2021. Section 3 uploaded. 07/01/2021. Section 6.3 and refreshers in Appendix D added. Added code for LFSR. Some typos fixed in Sections 1 and 2. 04/01/2021. Sections 1 and 2 uploaded. Acknowledgements The notes are based on and inteded to accompany the UPF course Cryptography and Security as taught by Vanesa Daza and Carla Ràfols. First iteration of the notes were written by Javier Silva, using Bookdown and pandoc. A second iteration along with solved exercises and the SageMath cookbook was written by Alexandros Zacharakis. Both iterations have been supervised by Vanesa Daza and Carla Ràfols. Aquesta iniciativa es realitza en el marc dels fons del Pla de Recuperació, Transformació i Resiliència, finançats per la Unió Europea (Next Generation), el projecte del Govern d’Espanya que marca el full de ruta per a la modernització de l’economia espanyola, la recuperació del creixement econòmic i la creació d’ocupació, per a la reconstrucció econòmica sòlida, inclusiva i resilient després de la crisi de la COVID19, i per respondre als reptes de la propera dècada. This initiative is part of the Recovery, Transformation and Resilience Plan funds, financed by the European Union (Next Generation). The Spanish government’s project sets out the roadmap for modernizing the country’s economy, restoring growth and creating jobs, building a strong, inclusive and resilient recovery after the COVID-19 crisis, and meeting the challenges of the coming decade. "],["introduction-to-security.html", "1 Introduction to security 1.1 What cryptography is and is not 1.2 Fundamental security principles 1.3 Security parameter 1.4 Security level Solved exercises", " 1 Introduction to security We use cryptography on a daily basis: our wireless communications or web traffic are encrypted, companies protect their data with cryptographic algorithms, and so on. We all have a basic or intuitive understanding of how cryptographic algorithms work. In this chapter, we want to make this intuition more precise and give you tools to think about cryptographic algorithms more formally, and reason scientifically about security. Therefore, in this section we will: Introduce three basic principles of cryptographic algorithm design; Introduce the notion of security parameter and security level. 1.1 What cryptography is and is not Cryptography is a field that lies halfway between mathematics and computer science, and is occupied with building algorithms that protect communications in some way, for example ensuring privacy or integrity of a message sent through an insecure channel. In this course, we will describe some of the most important cryptographic algorithms. They are the foundation for many security mechanisms and protocols that are part of the digital world. Thus, when you finish this course, you will have the basis to understand these mechanisms. But it is also important that you understand what is not covered in this course, and what are the limitations of what you will learn. In particular, a well-known course on cryptography1 mentions three warnings that you should take into consideration: Cryptography is not the solution to all security problems; Cryptography is not reliable unless implemented and used properly; Cryptography is not something that you should try to invent yourself, as there are many and many examples of broken ad-hoc designs. 1.2 Fundamental security principles Let us consider a common example. When we type our WiFi password to connect to a network, we are assuming that what we are doing is “secure\" because only us have some secret information (the password), that allows us to do this. In the context of cryptography, we call this secret information the secret key. But what if an attacker tries all the possible keys until he finds the right one? This is what is called a brute-force attack. We will consider a few different scenarios: Our secret key is a 4-digit number. Then, in the worst case, the attacker will need to try \\[10^4=10000\\] potential keys. Assuming one try per second, this will take a bit less than three hours. Our secret key is a 12-character string of digits and English letters. Since there are 10 possible digits and 26 possible letters for each position, the number of potential keys is \\[36^{12}=4738381338321616896.\\] At the same rate, the attacker will need, approximately, \\(1.5\\cdot 10^{11}\\) years to try all of them. For reference, this number is roughly half of the number of stars in the Milky Way. Exercise 1.1 A WiFi password is \\(10\\) bits long. Assume that an attacker tries one password per second. How long does it take to find the key by brute force? What if the password is \\(\\lambda\\) bits long? The idea is that, if our password is generated in a good way, this will take too long! So we are implicitly thinking that our scheme is secure because an attacker has limited time or limited money to buy hardware to perform very fast attacks and find our password. This leads to the first fundamental principle of modern cryptography: Principle 1. Security depends on the resources of the attacker. We say that a cryptographic scheme is secure if there are no efficient attacks. Cryptographic algorithms need to be carefully reviewed by the scientific community, and directions for implementation and interoperability must be given before they are adopted. This is done by institutions such as NIST, the National Institute of Standards and Technology in the US.2 Thus, coming up with new, secure algorithms is difficult. In fact, the algorithms that are used in practice are public, known to everyone, and in particular to potential attackers. For instance, in the case of a WiFi password, it is a publicly-known fact that WPA is used. This is a general design principle in cryptography: security must come from the choice of a secret key and not from attackers not knowing which algorithm we are using. Principle 2 (Kerckhoffs’s principle). Design your system so that it is secure even if the attacker knows all of its algorithms.3 So what makes our systems secure is the fact that, although the attacker knows the algorithm, it does not know the secret key that we are using. Back to the WiFi example, an attacker knows that the WPA standard is used, but they just don’t know our password. Another implicit assumption that we are making when we think that our connection is “secure\" is that our secret key is sufficiently random, that is, that there are many possibilities for the secret key. This leads us to the third and last principle. Principle 3. Security is impossible without randomness. As we will see, randomness plays an essential role in cryptographic algorithms. In particular, it is always essential that secret keys are chosen to be sufficiently random (i.e. they should have enough entropy). A bad randomness source almost always translates into a weakness of the cryptographic algorithm. 1.3 Security parameter Obviously, cryptographic algorithms need to be efficient to be used in practice. On the other hand, we have seen that no attack should be efficient at all, i.e. they should be computationally infeasible. Before we go on, we need to determine the meaning of efficiency, so that the concept is formal and quantifiable. At the same time, and because of Principle 1, we need to relate efficiency with security somehow. The way to achieve this is through a natural number that we call the security parameter, usually denoted by \\(\\lambda\\). The information about both security and efficiency will be expressed in terms of the security parameter. Definition 1.1 An algorithm \\(\\mathcal{A}\\) is said to be efficient (or polynomial-time) if there exists a positive polynomial \\(p\\) such that, for any \\(\\lambda\\in\\mathbb{N}\\), when \\(\\mathcal{A}\\) receives as input a bitstring of length \\(\\lambda\\), it finishes in \\(p(\\lambda)\\) steps. We note that here we are interested in having a rough estimate on the running time, so we count each basic bit operation as one step. We use equivalently the terms running time, number of steps, number of operations. An important observation is that a single polynomial must work for any value of \\(\\lambda\\). Otherwise, any algorithm would be considered efficient. The intuition behind the definition is that we allow the running time of the algorithm to grow when the input gets larger, but not “too much too fast”. Let us consider two examples to illustrate the concept of polynomial-time algorithms: Algorithm \\(\\mathcal{A}\\) takes two \\(\\lambda\\)-bit integers \\(m,n\\) and adds them. Algorithm \\(\\mathcal{B}\\) takes a \\(\\lambda\\)-bit integer \\(n\\in\\{0,\\dots,2^{\\lambda}-1\\}\\) and finds its prime factors in the following way: for each \\(i=1,\\dots,n\\), it checks whether \\(i\\) divides \\(n\\), and if that is the case it outputs \\(i\\). Are they efficient? The first one is efficient, because the number of operations is \\(O(\\lambda)\\), while the second one is inefficient because the number of operations is \\(O(2^{\\lambda})\\). In other words, the number of operations grows exponentially when we increase the size of the input. As is well known, exponential functions grow much faster than polynomials, and so in this case we will not be able to find a polynomial to satisfy Definition 1.1. Thus, algorithm \\(\\mathcal{B}\\) is not efficient. Below, you can find a Sage implementation of each of the two algorithms, with the tools to compare their running times for different sizes of \\(\\lambda\\). Observe that, by increasing the security parameter, soon the second algorithm starts taking too long to terminate. # Choose the security parameter sec_param = 12 # Generate two random numbers of bit length lambda n = randrange(2^(sec_param-1),2^(sec_param)-1) m = randrange(2^(sec_param-1),2^(sec_param)-1) print (&quot;n =&quot;,n,&quot;\\nm =&quot;,m) # Define algorithm A, which adds the two numbers def algorithm_a(n,m): n+m # Measure the time it takes to run algorithm A %time algorithm_a(n,m) # Define algorithm A, which tries to factor a number def algorithm_b(n): for i in range(1,n+1): if mod(n,i)==0: i # Measure the time it takes to run algorithm B %time algorithm_b(n) Exercise 1.2 Decide whether the following algorithms run in polynomial time: An algorithm that takes as input two integers \\(n,m\\) (in base 2) and computes the sum \\(n+m\\). An algorithm that takes as input an integer \\(n\\) and prints all the integers from \\(1\\) to \\(\\ell\\), if: \\(\\ell=n\\). \\(\\ell=n/2\\). \\(\\ell=\\sqrt{n}\\). \\(\\ell=10^6\\). \\(\\ell=\\log_2 n\\). We are now in position to discuss the efficiency and security of a cryptographic scheme in more grounded terms. For cryptographic schemes that require secret keys, the security parameter \\(\\lambda\\) is the bit length of the key. All the algorithms that compose some cryptographic scheme, like an encryption or signature scheme, should run in time polynomial in \\(\\lambda\\). A classical example of this is an encryption scheme, which is the cryptographic primitive that will be the focus of most of the course. We first introduce the notion of symmetric encryption scheme.4 Definition 1.2 A symmetric encryption scheme is composed of three efficient algorithms: \\[(\\mathsf{KeyGen},\\mathsf{Enc},\\mathsf{Dec}).\\] The \\(\\mathsf{KeyGen}\\) algorithm chooses some key \\(\\mathsf{k}\\) of length \\(\\lambda\\), according to some probability distribution. The \\(\\mathsf{Enc}\\) algorithm uses the secret key \\(\\mathsf{k}\\) to encrypt a message \\(\\mathsf{m}\\), and outputs the encrypted message \\[\\mathsf{c}=\\mathsf{Enc}_{\\mathsf{k}}(\\mathsf{m}).\\] The \\(\\mathsf{Dec}\\) algorithm uses the secret key \\(\\mathsf{k}\\) to decrypt an encrypted message \\(\\mathsf{c}\\), recovering \\[\\mathsf{m}\\] as \\[\\mathsf{Dec}_{\\mathsf{k}}(\\mathsf{c})=\\mathsf{m}.\\] In this context, \\(\\mathsf{m}\\) is called the plaintext, and \\(\\mathsf{c}\\) is said to be its corresponding ciphertext. Technically, the fact that the algorithms are efficient is expressed as requiring that the three algorithms run in time polynomial in \\(\\lambda\\).5 On the other hand, observe that, if an attacker wants to try all the possible secret keys, it needs \\(O(2^{\\lambda})\\) steps to do so. This is not polynomial time in the security parameter (again, it is exponential), so it is not efficient according to Definition 1.1. 1.4 Security level Ideally, we would like that the best possible attack against a scheme is a brute-force attack, in which an attacker (also called adversary) needs to try all the possibilities. However, very often there exist much more sophisticated attacks that need less time. This motivates the following definition: Definition 1.3 A cryptographic scheme has \\(n\\)-bit security if the best known attack requires \\(2^{n}\\) steps. When the best known attack is a brute-force attack, then \\(n=\\lambda\\), but we will see many examples of the opposite, which makes \\(n\\) significantly smaller. In a few lessons, we will see the example of hash functions, for which, in the best case, \\[n=\\frac\\lambda 2.\\] If we require a security level of \\(80\\) bits, this forces us to choose \\(\\lambda=160\\), at the least. Another example is RSA, which is a famous encryption scheme that we will study later in the course. In that case, \\(\\lambda\\) needs to be \\(1024\\) to achieve a security level of roughly \\(80\\) bits. Although all the algorithms that compose a scheme, like \\((\\mathsf{KeyGen},\\mathsf{Enc},\\mathsf{Dec})\\) in the encryption case, are still efficient, their running time typically increases with \\(\\lambda\\). The impact of this is that, the higher the value of \\(\\lambda\\), the more expensive the computations are. But what is a good security level? Suppose you have some cryptographic algorithm that has \\(n\\)-bit security for key length \\(\\lambda\\). How do you decide what \\(n\\) is appropriate for your scheme to be secure? How is \\(n\\) to be chosen so that it is infeasible (i.e. inefficient for an adversary) to recover the key? There is no unique answer to this question. As we saw in Principle 1, security is a matter of resources. If an adversary needs to use computational power to perform \\(2^n\\) steps to attack your system, this will cost him money (electric power, hardware, etc). If your cryptographic tools are protecting something that is worth only \\(10\\)€, an attacker will not be willing to spend a lot of money attacking it. RFID tags are a good example of this. On the other hand, if you are protecting valuable financial information, or critical infrastructure, you would better make sure that this costs the adversary a lot of resources. A general rule of thumb is that a cryptosystem is expected to give you at the very least an \\(80\\)-bit security level. By today’s computing power levels, this is considered even a bit weak, and acceptable security levels are more around the \\(100\\)-bit mark. This does not mean that any attack below \\(2^{100}\\) can be easily run on your PC at home! The website https://www.keylength.com/ maintains a list of key size recommendations suggested by different organizations. The following table, taken from Mike Rosulek’s book The Joy of Cryptography gives some estimates of computational cost in economic terms.6 Security level Approximate cost Reference 50 $3.50 cup of coffee 55 $100 decent tickets to a Portland Trailblazers game 65 $130000 median home price in Oshkosh, WI 75 $130 million budget of one of the Harry Potter movies 85 $140 billion GDP of Hungary 92 $20 trillion GDP of the United States 99 $2 quadrillion all of human economic activity since 300,000 BC 128 really a lot a billion human civilizations’ worth of effort Exercise 1.3 Determine the security level when: My password consists of \\(20\\) random letters of the Catalan alphabet. Same as above, but including also capital letters. My password is a word of the Catalan dictionary (88500 words). Solved exercises Exercise 1.4 As we will see later in the course, AES is a widely used encryption scheme. It comes in three different flavours, with different key sizes, specifically 128, 192 and 256 bits. Until today, there is no known attack to AES better than bruteforce. What is the bit security of AES? Assume a new attack is discovered that efficiently decides whether the key corresponds to an odd or even number given a ciphertext. What would be the bit security of AES in this case? How much faster would be an attack in this case? Solution. Since the best known attack is bruteforce -which requires \\(2^n\\) operations to find a key of size \\(n\\), the bit security of AES is 128, 192, 256 for each flavour. Learning whether a number is odd or even is equivalent to learning the last bit in its binary representation (0 for even and 1 for odd). Since in this case we know a bit (but nothing more) the best we can do is bruteforce on 127, 191 and 255 bits for each flavour. Therefore, the bit security in this case would be 127, 191 and 255. A bruteforce would require roughly half the time to be performed since we need to perform a search over \\(2^{n-1} = \\frac{2^n}{2}\\) values. Exercise 1.5 Diceware is a method to create secure and easy to remember passwords. Roughly, it works as follows: you have a dictionary of \\(6^5 = 7776\\) unique words (which are public) and are ordered as an ordered list. For example the Electronic Frontier Foundation has published this list which contains number word 11111 abacus 11112 abdomen … … 32342 glitzy … … 66665 zoology 66666 zoom To create a password, you roll 5 dice to create a number and select the corresponding word. The password consists of many words. Let’s look an example of a password from this list. To make a password we select 4 words by rolling four times the 5 dice. For example, if the rolls are 21246, 16316, 33666, 35146, the password using this list is \\[ \\text{crablike cola helpline jinx} \\] The password is easy to remember since the user can make a unique story based on these words to help him, for example “An alien creature that looks like a crab tried for the first time cola but it burned him so it called the helpline and they told him it will forever be jinxed”. Assume you use an encryption scheme that uses Diceware keys and the only known attack is brute-force. If you use a password of four words as above, what is the bit security of the scheme? What is in general the bit security of the scheme when using \\(n\\) words as the password? At least how many words should you use to get bit security greater than \\(128\\)? How many security bits does each new word add? Solution. Since bruteforce is the only possible attack, it is enough to count all the possible combinations when using 4 words. There are \\(7776\\) combinations for the each word, so the total number of combinations is \\(7776^4 = 3656158440062976\\). Recall that scheme has \\(n\\)-bit security if the best known attack requires \\(2^n\\) steps. Here, the best known attack requires \\(3656158440062976\\) so its bit security is \\(\\log_2(3656158440062976)\\approx 51.7\\). In general the total numbers of combinations is \\(7776^n\\) and this corresponds to bit security of \\(\\log_2(7776^n)\\). We need to find the minimum \\(n\\) s.t. \\(\\log_2(7776^n)&gt;128\\). After a few trials we see that we should pick at least 10 words. Specifically, this gives approximately \\(129.24\\) bits of security. Rewriting the relation of question 2 we have \\[ \\log_2(7776^n) = n\\log_2(7776) \\approx 12.92 n \\] This means that when using \\(n\\) words, the bit security is approx. \\(12.92n\\). So each new word contributes approx. \\(12.92\\) extra security bits. Exercise 1.6 Consider four symmetric encryption schemes. The best known attacks for each when using a key of size \\(\\lambda\\) are \\(\\log\\lambda^{\\log\\lambda}\\) \\(2^{(\\log \\lambda)^2}\\) \\(2^{(\\log \\lambda^2)}\\) \\(\\sqrt{\\lambda^{20}}\\) Which of the schemes admit efficient attacks? What is the running time of the attacks when setting \\(\\lambda=10\\)? Reflect and comment on your answer to the previous question. Solution. We simplify the running time of the attacks to compare with polynomial functions. We have for each To see if an attack is efficient, we need to see if it can be performed in polynomial as a function of the input length. In our case, the input length is the key size. \\(\\log\\lambda^{\\log\\lambda} = \\log \\lambda \\cdot \\log \\lambda = (\\log \\lambda )^2\\). This value is asymptotically smaller than some polynomial. Concretely, note that \\(\\log \\lambda\\) grows slower than \\(\\lambda\\) and therefore \\((\\log \\lambda )^2\\) grows smaller than \\(\\lambda^2\\), which is a polynomial. \\(2^{(\\log \\lambda)^2} = 2^{\\log \\lambda\\cdot \\log \\lambda} = (2^{\\log \\lambda})^{\\log \\lambda} = \\lambda^{\\log\\lambda}\\). This value is larger than any polynomial. Indeed, \\(\\log\\lambda\\) grows faster than any fixed constant \\(k\\), hence the function grows faster than any polynomial \\(p(\\lambda) = \\lambda^k\\). \\(2^{\\log \\lambda^2} = 2^{2\\log \\lambda} = 2^{(\\log \\lambda)\\cdot 2} = \\lambda^{2}\\). This value is asymptotically a polynomial, specifically \\(p(\\lambda) = \\lambda^{2}\\). \\(\\sqrt{\\lambda^{20}} = \\lambda^{\\frac{1}{2}\\cdot 20} = \\lambda^{10}\\). This value is asymptotically a polynomial, specifically \\(p(\\lambda) = \\lambda^{10}\\). The second part simply requires to make the calculations. Approximately, the number of steps each algorithm needs is at least: 11 2098 100 10000000000 The attack of the second scheme seems much faster than the attack of the third scheme, although the second is not efficient while the third is. This is because we use a \\(\\lambda\\) that is too small. Definition 1.1 considers the asymptotic behavior of an algorithm. This means it captures how much slower an algorithm becomes when its input becomes bigger. Since the third attack is efficient (i.e. polynomial time), there exists some value \\(\\lambda_0\\) such that the second attack is slower for any value \\(\\lambda&gt;\\lambda_0\\) (can you find it?). Exercise 1.7 Your favorite symmetric encryption scheme has optimal security, the only known attack against it is brute-force. One day, researchers from University of Cryptography discover a novel attack that in a single step recovers the first \\(\\lambda/2\\) bits of the secret key. Is the attack efficient? What actions do you need to take? After a week, they improve the attack. Now, it recovers the whole key except its last \\(\\log\\lambda\\) bits. Is the improved attack efficient? What actions you need to take now? Solution. Since the first half of the key is recovered in one step, to recover the whole key, an attacker needs to guess the second half. Since the only known attack is brute-force and the unknown bits of the key are \\(\\lambda/2\\), doing so requires \\(2^{\\lambda/2}\\) operations. The attack is not efficient since \\(2^{\\lambda/2}\\) is an exponential function and grows larger than any fixed polynomial. To mitigate the attack, you should double the key length. Setting \\(\\lambda&#39;=2\\lambda\\) means an adversary will need to do \\(2^{\\lambda&#39;/2} = 2^{2\\cdot\\lambda/2} = 2^\\lambda\\) operations to break the scheme, hence we get the same security as we had before the attack. In this case, an attacker needs to brute-force to recover the last \\(\\log\\lambda\\) bits. This needs \\(2^{\\log\\lambda}=\\lambda\\) operations which is a polynomial. This means that the attack is efficient. In this case, you should consider the scheme broken and should use another encryption scheme. Remark. Note that in the first question, while we get the same security, this comes with a price. We now need to use a larger key, which will make operations (encryption and decryption) slower. This holds in general: a better attack on your favorite scheme means you need to use larger keys, which in turn means the scheme becomes less efficient to use. D. Boneh. Cryptography I, Coursera. Available at https://www.coursera.org/learn/crypto.↩︎ https://www.nist.gov/.↩︎ Kerckhoffs’s principle is named after Auguste Kerckhoffs, who published the article La Cryptographie Militaire* in 1883.*↩︎ In the second half of the course, we will deal with the notion of asymmetric encryption schemes, in which there are two different keys, a public key that is used for encryption and a secret key that is used for decryption.↩︎ In many cryptography books, you will find that \\(\\mathsf{KeyGen}\\) should be a (probabilistic) polynomial time algorithm that takes as input \\(1^{\\lambda}\\), which is the string with \\(\\lambda\\) ones. This is a way to write that \\(\\mathsf{KeyGen}\\) should be polynomial in \\(\\lambda\\).↩︎ Note that he uses the English definition of billion, that is, \\(10^9\\). Same for the other amounts. Also, the table seems to be based on data from 2018, so the up-to-date numbers might vary slightly.↩︎ "],["randomness-in-cryptography.html", "2 Randomness in cryptography 2.1 One-time pad 2.2 Pseudorandom generators 2.3 True randomness Solved exercises", " 2 Randomness in cryptography As we saw above, and made explicit in Principle 3, we require randomness to guarantee secure cryptography. In this section, we will give some thought to how to obtain this randomness in the first place, and what to do when we do not have enough of it. As a motivating example, we will start by describing a well-known encryption scheme. We will learn about: The one-time pad encryption scheme; Pseudorandom generators; Sources of randomness. 2.1 One-time pad The one-time pad (OTP) is an old encryption scheme, which was already known in the late 19th century, and was widely used in the 20th century for many military and intelligence operations. The idea is extremely simple. Let us first recall the exclusive or (\\(\\mathsf{XOR}\\)) logic operation. Given two bits \\(b_0,b_1\\in\\{0,1\\}\\), the operation is defined as \\[\\mathsf{XOR}(b_0,b_1)=b_0\\oplus b_1=\\left\\{\\begin{array}{ll} 0 &amp; \\text{if }b_0=b_1, \\\\ 1 &amp; \\text{if }b_0\\neq b_1. \\\\ \\end{array}\\right.\\] Equivalently, the operation corresponds to the following truth table: \\(b_0\\) \\(b_1\\) \\(b_0\\oplus b_1\\) 0 0 0 0 1 1 1 0 1 1 1 0 We extend the notation to bitstrings of any length, i.e., given two bitstrings \\(\\mathbf b_0\\) and \\(\\mathbf b_1\\) of the same length, we define \\[\\mathbf b_0 \\oplus \\mathbf b_1\\] to be the bitstring that results from \\(\\mathsf{XOR}\\)’ing each bit of \\(\\mathbf b_0\\) with the bit in the same position of \\(\\mathbf b_1\\). Assume that Alice wants to send an encrypted message to Bob. The one-time pad works as follows. Key generation consists of choosing as a secret key a uniformly random bitstring of length \\(\\lambda\\) as the key: \\[\\mathsf{k}=k_1k_2\\dots k_\\lambda.\\] We denote this process by \\(\\mathsf{k}\\gets\\{0,1\\}^\\lambda\\). Let \\(m\\) be a message that Alice wants to encrypt, written as a bitstring7 \\[\\mathsf{m}=m_1m_2\\dots m_\\lambda\\] of the same length. Then, the one-time pad encryption scheme works by \\(\\mathsf{XOR}\\)’ing each message bit with the corresponding key bit. More precisely, for the \\(i\\)th bit of the message, we compute \\[\\mathsf{c}= \\mathsf{m}\\oplus \\mathbf k,\\] which is sent to Bob. Note that, because the \\(\\mathsf{XOR}\\) operation is its own inverse, the decryption algorithm works exactly like encryption. That is, Bob can recover the message by computing \\[\\mathsf{m}= \\mathsf{c}\\oplus\\mathsf{k}.\\] The first property that we want from any encryption scheme is correctness, which means that for any message \\(\\mathsf{m}\\) and any key \\(\\mathsf{k}\\), we have that \\[\\mathsf{Dec}_{\\mathsf{k}}(\\mathsf{Enc}_{\\mathsf{k}}(\\mathsf{m}))=\\mathsf{m},\\] that is, if we encrypt and decrypt, we should recover the same message. Otherwise Alice and Bob will not be able to communicate. Proposition 2.1 The one-time pad is a correct encryption scheme. Proof. Using the definitions of encryption and decryption, we have that \\[\\mathsf{Dec}_{\\mathsf{k}}(\\mathsf{Enc}_{\\mathsf{k}}(\\mathsf{m}))=\\mathsf{Dec}_{\\mathsf{k}}(\\mathsf{m}\\oplus\\mathsf{k})=(\\mathsf{m}\\oplus\\mathsf{k})\\oplus \\mathsf{k}= \\mathsf{m}\\oplus (\\mathsf{k}\\oplus\\mathsf{k})=\\mathsf{m}\\oplus\\mathbf 0=\\mathsf{m},\\] where \\(\\mathbf 0\\) means the string of zeroes of size \\(\\lambda\\). In the last two steps, we used, respectively, that \\(\\mathsf{XOR}\\)’ing any string with itself produces \\(\\mathbf 0\\), and that \\(\\mathsf{XOR}\\)’ing any string with \\(\\mathbf 0\\) does not change the string. Here is a straightforward implementation of the one-time pad. In this example, we want to send a message with \\(12\\) ASCII characters, so each character will require \\(8\\) bits. Thus, we choose a key length of \\(96\\). from sage.crypto.util import ascii_integer from sage.crypto.util import bin_to_ascii # Set a security parameter sec_param = 96 # Define the XOR operation: def xor(a,b): return mod(int(a)+int(b),2) # You will learn why this is equivalent # to XOR later in the course ### KEY GENERATION # Generate a random key of length sec_param k = random_vector(GF(2),sec_param) ### ENCRYPTION # Choose a message m = &quot;Hello there.&quot; # Process the message into a bitstring m_bin = str(BinaryStrings().encoding(m)) # Encrypt the message bit by bit c = &quot;&quot; if (len(m_bin)&lt;=sec_param): for i in range(len(m_bin)): c += str(xor(m_bin[i],k[i])) print(&quot;Ciphertext: &quot;+c) else: print(&quot;Message too long. Need a longer key.&quot;) ### DECRYPTION # We use the same ciphertext obtained in the encryption part. # Decrypt the ciphertext bit by bit m_bin = &quot;&quot; if (len(c)&lt;=sec_param): for i in range(len(c)): m_bin += str(xor(c[i],k[i])) print(&quot;Plaintext: &quot;+bin_to_ascii(m_bin)) else: print(&quot;Ciphertext too long. Need a longer key.&quot;) The one-time pad receives its name from the fact that, when the key is used only once, the scheme has perfect secrecy. This means that the ciphertext produced reveals absolutely no information about the underlying plaintext, besides its length. We formalize this by saying that, given a ciphertext and two messages, the ciphertext has the same probability of corresponding to each of the messages. Definition 2.1 An encryption scheme has perfect secrecy when, for a uniformly random key \\(\\mathsf{k}\\), all ciphertexts \\(\\mathsf{c}\\) and all pairs of messages \\(\\mathsf{m}_0,\\mathsf{m}_1\\), \\[\\Pr[\\mathsf{c}=\\mathsf{Enc}_{\\mathsf{k}}(\\mathsf{m}_0)]=\\Pr[\\mathsf{c}=\\mathsf{Enc}_{\\mathsf{k}}(\\mathsf{m}_1)].\\] Intuitively, the perfect secrecy of the OTP stems from these two observations: Look again at the truth table of the \\(\\mathsf{XOR}\\) operation, and observe that a \\(0\\) in the plaintext could equally come from a \\(0\\) or a \\(1\\) in the plaintext, depending on the key bit. Similarly, a \\(1\\) in the ciphertext could also come from a \\(0\\) or a \\(1\\) in the plaintext. In other words, if the key is chosen uniformly at random, each bit of the ciphertext has a probability of \\(1/2\\) of coming from a \\(0\\), and a probability \\(1/2\\) of coming from a \\(1\\). Because of the above, an adversary that intercepts a ciphertext \\(c_1c_2\\dots c_\\lambda\\) cannot know the corresponding plaintext, as any given plaintext can be encrypted to any bitstring of length \\(\\lambda\\). In other words, for every ciphertext \\(\\mathsf{c}\\) and every message \\(\\mathsf{m}\\), there exists a key \\(\\mathsf{k}\\) and a message such that \\[\\mathsf{Enc}_{\\mathsf{k}}(m)=\\mathsf{c}\\qquad\\text{and}\\qquad\\mathsf{Dec}_{\\mathsf{k}}(\\mathsf{c})=m.\\] So any ciphertext could correspond to any message, and there is no way to do better, regardless of the computational power of the attacker! We formalize the above discussion in the following result. Proposition 2.2 The one-time pad encryption scheme has perfect secrecy. Proof. By the discussion above, we have that for any key \\(\\mathsf{k}\\), message \\(\\mathsf{m}\\) and ciphertext \\(\\mathsf{c}\\), \\[\\Pr[\\mathsf{c}=\\mathsf{Enc}_{\\mathsf{k}}(\\mathsf{m})]=\\frac{\\#\\{\\text{keys $\\mathsf{k}$ such that } \\mathsf{c}=\\mathsf{Enc}_{\\mathsf{k}}(\\mathsf{m})\\}}{\\#\\{\\text{possible keys}\\}}=\\frac{1}{2^\\lambda}.\\] Exercise 2.1 We said above that for every message \\(\\mathsf{m}\\) and any ciphertext \\(\\mathsf{c}\\), there is always exactly one key \\({\\mathsf{k}}\\) such that \\[\\mathsf{Enc}_{\\mathsf{k}}(m)=\\mathsf{c}\\qquad\\text{and}\\qquad\\mathsf{Dec}_{\\mathsf{k}}(\\mathsf{c})=m.\\] For arbitrary \\(\\mathsf{m}\\) and \\(\\mathsf{c}\\), which is that key, expressed in terms of \\(\\mathsf{m}\\) and \\(\\mathsf{c}\\)? This is all well and good, but obviously there’s a catch. While the security of one-time pad is as good as it gets, it is simply impractical for a very simple reason: we need a key as large as the message, and moreover, we need a new key for each message. Moreover, if we want perfect secrecy, this is unavoidable. Proposition 2.3 Any encryption scheme with perfect secrecy requires a key that is as long as the message, and it cannot be reused. One reason that highlights how reusing keys in OTP breaks perfect secrecy is the following. Assume that we use the same key \\(\\mathsf{k}\\) for two messages \\(\\mathsf{m}_0,\\mathsf{m}_1\\). Then, an attacker intercepts the ciphertexts \\[\\mathsf{c}_0=\\mathsf{m}_0\\oplus\\mathsf{k}, \\qquad \\mathsf{c}_1=\\mathsf{m}_1\\oplus\\mathsf{k}.\\] The adversary can compute \\[\\mathsf{c}_0\\oplus\\mathsf{c}_1=(\\mathsf{m}_0\\oplus\\mathsf{k})\\oplus (\\mathsf{m}_1\\oplus\\mathsf{k})= \\mathsf{m}_0\\oplus\\mathsf{m}_1\\oplus(\\mathsf{k}\\oplus\\mathsf{k})=\\mathsf{m}_0\\oplus\\mathsf{m}_1\\oplus\\mathbf 0=\\mathsf{m}_0\\oplus m_1.\\] That is, the adversary can get the \\(\\mathsf{XOR}\\) result of the two messages. Even if they do not know any of the messages on their own, this leaks partial information (e.g. a \\(0\\) in any position means that the two messages have the same value on that position). So it’s clear that for OTP to work we need keys as long as the messages, and there is no way around that. But how much of a big deal is that? An issue that we have not addressed yet is the fact that, for any of this to happen, the two parties involved need to agree on a common key \\({\\mathsf{k}}\\), that must remain secret for anyone else. If an insecure channel is the only medium for communication available: they cannot share the key unencrypted, since an attacker could be listening, and grab the key to decrypt everything that comes afterwards. they cannot encrypt the key, since they don’t have a shared key to use encryption yet! Later in the course, we will see that there are ways to securely share a key over an insecure channel. But for now, it suffices to say that these methods exist. However, sharing a new key of the size of the message, and a new one for each message, is simply not practical most of the time. Imagine the key sizes for sending audio or video over the Internet. This, ultimately, is what kills the one-time pad. 2.2 Pseudorandom generators Before we move on, let us see if there is still some hope for the one-time pad. What if we start from a short uniformly random key \\(\\mathsf{k}\\), and try to expand it to a longer key? Let us assume that Alice and Bob wish to communicate using the one-time pad, and Alice wants to send a message of length \\(h\\). But they have only shared a key \\({\\mathsf{k}}\\in\\{0,1\\}^\\ell\\), for some \\(\\ell&lt;h\\), so they proceed as follows: They agree on a public function \\[G:\\{0,1\\}^\\ell\\rightarrow\\{0,1\\}^{h}.\\] That is, \\(G\\) receives a bitstring of length \\(\\ell\\) and outputs another of length \\(h\\). Since the function is deterministic, they can both compute \\[{\\mathsf{k}}&#39;=G({\\mathsf{k}})\\] on their own. Now they both know \\({\\mathsf{k}}&#39;\\in\\{0,1\\}^{h}\\). They use the one-time pad with key \\({\\mathsf{k}}&#39;\\). Observe that, since they have already “stretched” the key once, they could potentially take parts of \\(\\mathsf{k}&#39;\\) and apply the function \\(G\\) again to generate new keys on demand. The scheme that results from stretching the randomness of a short shared key to an arbitrary length and encrypt the message through the \\(\\mathsf{XOR}\\) operation is known as a stream cipher. The initial key used is called the seed, and the subsequent keys generated are called the key stream. The function \\(G\\) must be deterministic, otherwise Alice and Bob will not arrive at the same key, and they will not be able to communicate. Also note that, although \\(G\\) is public, \\({\\mathsf{k}}\\) is not, so an attacker has no way of learning the new key \\({\\mathsf{k}}&#39;\\). However, there are some caveats to this. Since the input of the function is a set of size \\(2^\\ell\\), there are at most \\(2^\\ell\\) outputs, whereas if we had used a uniformly random key of length \\(h\\), we would have \\(2^{h}\\) potential keys. Recall that perfect secrecy strongly relied on the keys being uniformly random, which clearly will not be the case here. But, what if the output of \\(G\\) looks “close enough” to random? By this, we mean that no efficient algorithm can distinguish the output distribution of \\(G\\) and the uniform distribution in \\(\\{0,1\\}^{h}\\). Then, if an adversary cannot tell that we are using a non-uniform distribution, they will not be able to exploit this fact in their attacks, and so our scheme will remain secure. Is any function \\(G\\) good enough for our purposes? Exercise 2.2 Consider the stream cipher presented above, with the following choices for the function \\(G\\), for \\(h=2\\ell\\). \\(G\\) outputs a string of \\(2\\ell\\) zeroes. \\(G\\) outputs the input, followed by a string of \\(\\ell\\) zeroes. \\(G\\) outputs two concatenated copies of the input. In each of these cases, discuss whether the scheme is still secure. The above exercise shows that we need to be careful when choosing our function \\(G\\). This leads us to the following definition. Definition 2.2 A pseudorandom number generator (PRNG) is a function \\[G:\\{0,1\\}^\\ell\\rightarrow\\{0,1\\}^h\\]such that no efficient adversary can distinguish the output distribution of \\(G\\) from the uniform distribution on \\(\\{0,1\\}^h\\). We emphasize the importance of randomness here. A function \\(G\\) whose output cannot be distinguished from uniform randomness by any (efficient) algorithm implies that, for all practical purposes, the output of \\(G\\) can be considered uniformly random in \\(\\{0,1\\}^h\\). In particular, informally this means that a key stream generated with a PRNG is unpredictable, i.e., given some output bits of \\(G\\), there is no way to predict the next in polynomial time, with a success rate higher than \\(50\\%\\). This contrasts with non-cryptographic PRNGs, in which it is enough that the output passes some statistical tests, but might not be completely unpredictable. Exercise 2.3 Assume that there is a very bad PRNG that outputs one bit at a time, and that bit is a \\(0\\) with probability \\(3/4\\). This PRNG is used in a stream cipher to produce a ciphertext \\[c=01.\\] In OTP, the probability of the corresponding plaintext being \\(00\\), \\(01\\), \\(10\\) or \\(11\\) would be \\(1/4\\) each. Compute the corresponding probabilities when the bad PRNG described above is in use. An interesting property of PRNGs is that, if we manage to build one that stretches the key by just a little, then we can produce an infinitely large key stream, and still maintain essentially the same security guarantees. To illustrate this, let us again consider a function \\[G:\\{0,1\\}^\\ell\\rightarrow\\{0,1\\}^{2\\ell},\\] and let us assume that it is a PRNG.8 Consider the following construction of a new function \\[H:\\{0,1\\}^\\ell\\rightarrow\\{0,1\\}^{3\\ell},\\] which works as follows: on input \\({\\mathsf{k}}\\), First compute \\(G({\\mathsf{k}})\\in\\{0,1\\}^{2\\ell}\\). Split the result in two halves \\(\\mathbf{x},\\mathbf{y}\\), each of length \\(\\ell\\). Compute \\(\\mathbf{z}=G(\\mathbf{y})\\in\\{0,1\\}^{2\\ell}\\). Output \\((\\mathbf{x},\\mathbf{z})\\in\\{0,1\\}^{3\\ell}\\). Proposition 2.4 If \\(G\\) is a PRNG, then \\(H\\), constructed as described above, is also a PRNG. We have already seen some bad PRNGs, so what about the good ones? Although there exist some proposals of PRNGs that are believed to be secure and are built “from scratch” (for example Salsa20 and ChaCha20), what happens in practice is that, when one wants a PRNG, it is common to build it from a block cipher, which is a topic that we will cover later in the course, so we delay the examples of cryptographic PRNGs until then. 2.3 True randomness We have dealt with the problem of stretching a tiny bit of randomness into something usable. But where does this initial randomness come from? It cannot really come from our computers, since these are deterministic, so the answer lies out in the physical world.9 The general idea is to look for unpredictable processes from which to extract randomness. Some examples are radioactive decay, cosmic radiation, hardware processes like the least significant bit of the timestamp of a keystroke. These processes might not produce uniformly random outputs, but from our perspective we have little to none information about their output distribution. These values are not used raw, but processed by a random number generator (RNG), which refines them into what we assume to be uniformly random outputs. These can now be fed into our PRNGs to stretch them. Solved exercises Exercise 2.4 Consider someone uses OTP to encrypt a single bit. They make a mistake and use the same key to encrypt two bits. You observe the ciphertexts 1 and 0. What can you say about the two messages? Assume you intercept a ciphertext \\(\\mathbf{c}\\) produced by OTP. You know that this message corresponds to a known message \\(\\mathbf{m}\\) (for example you know that the first message of Bob is “hello there!”). You now receive a second ciphertext \\(\\mathbf{c}&#39;\\) encrypted using the same key (this should have not happened!). How can you retrieve the second message? Solution. Let \\(k\\) be the common key used \\(m_1 \\oplus k = 0\\) and \\(m_2\\oplus k_2=1\\). Applying \\(\\oplus k\\) in both sides of equation 1, we get10 \\[ m_1 \\oplus k \\oplus k= 0 \\oplus k \\] Noting that \\(k\\oplus k = 0\\) and \\(k\\oplus 0 = k\\) we get that \\(k = m_1\\). Replacing this in the second equation we get \\(m_2\\oplus m_1 = 1\\). This means that the messages \\(m_1, m_2\\) are different. One approach is to use the answer of question 1 (after generalizing it to many bits). We present another way. You know that for some \\(\\mathbf{k}\\), \\(\\mathbf{c} = \\mathbf{m}\\oplus\\mathbf{k}\\). Applying \\(\\oplus \\mathbf{m}\\) on both sides we get \\[ \\mathbf{m}\\oplus\\mathbf{c} = \\mathbf{m}\\oplus\\mathbf{m}\\oplus\\mathbf{k} = \\mathbf{k} \\] Therefore, xor-ing the message with the ciphertext gives us the full key \\(\\mathbf{k}\\)! Therefore, we can now decrypt \\(\\mathbf{m}&#39; = \\mathbf{c}&#39;\\oplus \\mathbf{k}\\). Exercise 2.5 Suppose you want to encrypt messages over the English alphabet. To do this you use a substitution cipher. Such a cipher works as follows: the key consists of a random permutation \\(\\pi\\) of the alphabet. To encrypt a message you apply the permutation on each letter and to decrypt you apply the inverse permutation. For example, if the permutation defines \\[ \\pi(c) = q,\\ \\pi(d) = l,\\ \\pi(e) = a,\\ \\pi(o) = y,\\ \\ldots \\] we have \\[ \\textsf{Enc}_{\\pi}({code}) = {qyla},\\qquad \\textsf{Dec}_{\\pi}({qyla}) = {code} \\] Describe a real world example where the substitution cipher is not sufficient for security. Prove that the substitution cipher is not perfectly secure. Solution. Assume you know that a super secret party starts at either “five” or “nine”. You intercept a message that corresponds to “partyatfive” or “partyatnine”. With a substitution cipher, same letters always maps to the same cryptogram. Noting that “nine” has the first and third letter the same while five does not, the intercepted ciphertext will look like \\[ \\_\\ \\_\\ \\_\\ \\_\\ \\_\\ \\_\\ \\_\\ X\\ \\_\\ Y\\ \\_ \\text{ or } \\_\\ \\_\\ \\_\\ \\_\\ \\_\\ \\_\\ \\_\\ X\\ \\_\\ X\\ \\_ \\] where \\(X\\neq Y\\) are alphabet letters. In the first case the party is at five and in the second at nine. Recall the definition for perfect secrecy. Definition 2.1 requires that for any ciphertext \\(\\mathsf{c}\\) and any pair of messages \\(\\mathsf{m}_0,\\mathsf{m}_1\\), the probability (over the choice of key) that \\(\\mathsf{c}\\) is an encryption of \\(\\mathsf{m}_0\\) is the same as the probability that it is an encryption of \\(\\mathsf{m}_1\\). We will use the attack we described before to show that it is not the case. Since the property must hold for any ciphertext and any pair of messages it is enough to find such values that do not satisfy it (this is like a counterexample). Let’s choose the messages “five” and “nine” and the ciphertext “hghq”. First, note that there exists no key such that “five” is encrypted to “hghq” since f and v must map to different elements. So, \\(\\Pr[\\text{hghq} = \\textsf{Enc}(\\text{five})] = 0\\). On the other hand, there exist keys such that “nine” is an encryption of “hghq”. In particular, any permutation \\(\\pi\\) for which \\[ \\pi(\\text{n}) = \\text{h},\\quad \\pi(\\text{i}) = \\text{g},\\quad \\pi(\\text{e}) = \\text{q},\\quad \\] will do. There are many such keys but we don’t need to compute the exact probability, since it is already not zero. Therefore, \\[ \\Pr[\\text{hghq} = \\textsf{Enc}(\\text{five})] = 0 &lt; \\Pr[\\text{hghq} = \\textsf{Enc}(\\text{nine})] \\] which concludes the proof. Exercise 2.6 Alice has found a map to a secret treasure and she prepares to go on a quest to find it! She doesn’t care about the treasure but rather the adventure itself! If she manages to find it she wants her good friends Bob and Carol to keep it. However, she knows that Bob and Carol are greedy and they might rush to keep the treasure for themselves. How can she use cryptography to make sure she can communicate the location of the map in a way that neither Bob nor Carol alone can rush to the map? Solution. Before she leaves, Alice picks two one time pad keys \\(\\mathsf{k}_B, \\mathsf{k}_C\\) that are long enough to encrypt coordinates. She gives the first to Bob and the second to Carol. If she finds the treasure she encrypts the coordinates with the key \\(\\mathsf{k}=\\mathsf{k}_B\\oplus \\mathsf{k}_C\\), that is she computes \\(\\mathsf{c} = \\mathsf{m}\\oplus \\mathsf{k}\\). She then sends the ciphertext to both (or simply posts it online). Together, Bob and Carol can compute the one-time pad key \\(\\mathsf{k}\\) and learn the coordinates. Could one of them learn something on its own? The answer is no. To see this, note that that \\(\\mathsf{k_B}=\\mathsf{k}\\oplus \\mathsf{k}_C\\), so the key part that Bob has is a one time pad encryption of the key \\(\\mathsf{k}\\) that can only be decrypted by the key that Carol has. This means that Bob alone has no information about the key \\(\\mathsf{k}\\) and therefore no information about the sent message \\(\\mathsf{m}\\) when he only sees the ciphertext \\(\\mathsf{c}\\). Similarly, we can write \\(\\mathsf{k_C}=\\mathsf{k}\\oplus \\mathsf{k}_B\\) and make the same argument for Carol. Therefore, their only hope of success is to collaborate to learn the key \\(\\mathsf{k}\\) and use it to decrypt \\(\\mathsf{c}\\). Exercise 2.7 Alice loves OTP because it has perfect secrecy. But she does not want -understandably- to use a different key for each message. She knows she cannot use the same key, but she comes up with the following idea: when she sends the \\(i\\)-th ciphertext \\(\\mathbf{c}_i = \\mathbf{m}_i \\oplus \\mathbf{k}_i\\), she will use the \\(i\\)-th message as the next key, that is, she will set \\(\\mathbf{k}_{i+1} = \\mathbf{m}_i\\) and she will encrypt \\(\\mathbf{m}_{i+1}\\) as \\(\\mathbf{c}_{i+1} = \\mathbf{m}_{i+1} \\oplus \\mathbf{k}_{i+1} = \\mathbf{m}_{i+1} \\oplus \\mathbf{m}_i\\). She claims that since OTP is perfectly secret, no information is leaked about \\(\\mathbf{m}_i\\) and therefore it is like a fresh key. Is she correct? If yes, prove it. If not, what information is leaked? If an attacker knows any message, can he learn any other messages? Solution. Consider the \\((i+1)\\)-th ciphertext. They will be computed as \\[ \\begin{aligned} \\mathbf{c}_{i+1} &amp;= \\mathbf{m}_{i+1} \\oplus \\mathbf{k}_{i+1} = \\mathbf{m}_{i+1} \\oplus \\mathbf{m}_{i} \\\\ \\end{aligned} \\] and so on, so each ciphertext is the XOR of the previous two messages. Assume the attacker knows the \\(j\\)-th message. If \\(j = 1\\) she learns the key since \\(\\mathbf{k}_1 = \\mathbf{m}_1 \\oplus \\mathbf{c}_1\\) and she can decrypt all next messages (she knows the key for the second message (\\(\\mathbf{m}_1\\)) then it gets \\(\\mathbf{m}_2\\) which is the key for the 3rd message and so on). If \\(j &gt; 1\\), working as in the previous case, the attacker can get the messages for any \\(i&gt;j\\). What about the rest? It is enough to show how to get the \\((j-1)\\)-th message. If we do this, we can continue in the same way until the first. Recall that \\[ \\mathbf{c}_{j} = \\mathbf{m}_{j} \\oplus \\mathbf{k}_{j} = \\mathbf{m}_{j} \\oplus \\mathbf{m}_{j-1} \\] From this we get \\(\\mathbf{m}_{j-1} = \\mathbf{m}_j\\oplus \\mathbf{c}_j\\). Note that we know both values so we can indeed compute \\(\\mathbf{m}_{j-1}\\). Therefore an attacker that intercepts the ciphertexts and knows a single message can decrypt all other messages, deeming the scheme not secure. Exercise 2.8 (*) Let \\(G: \\{0,1\\}^{\\ell} \\rightarrow \\{0,1\\}^{2\\ell}\\) be a pseudorandom generator. You do the following experiment: flip a coin and if it is heads you are given a value \\(\\mathsf{k} = G(s)\\) for a random \\(s\\in\\{0,1\\}^{\\ell}\\). If it is tails you are given a random \\(\\mathsf{k}\\in\\{0,1\\}^{2\\ell}\\). Describe an algorithm that decides what was the coin flip. The algorithm does not need to be efficient (i.e. polynomial time) and does not need to always give the correct result, but rather guess with a good probability. What is the probability that your algorithm succeeds if the coin flip is heads? If the coin flip is tails? What is the success probability of your algorithm in general? Solution. The algorithm does the following: for all \\(s\\in\\{0,1\\}^\\ell\\), it computes \\(G(s)\\) and saves the result in a (big) table \\(T\\). Then it receives the value \\(\\mathsf{k}\\) and it outputs “heads” if \\(\\mathsf{k}\\in T\\) and “tails” otherwise. The intuition why this is a good strategy is that there are way less elements in \\(T\\) than in \\(s\\in\\{0,1\\}^{2\\ell}\\). Therefore, if the coin is tails, it is very unlikely that an element of \\(T\\) was chosen. If the coin is “heads” the algorithm always succeeds. Indeed, the result is always in T and the algorithm always guesses “heads”. If it “tails”, the algorithm fails if some value \\(\\mathsf{k}\\) is chosen such that there exists an \\(s\\in\\{0,1\\}^\\ell\\) for which \\(G(s)=\\mathsf{k}\\). We need to count how many elements exists in this set and how many in total. The small set contains at most \\(2^{\\ell}\\) (each element \\(s\\) can correspond to at most one) and the big one \\(2^{2\\ell}\\). Thus, the probability that we sample an element of the small set is at most \\(2^{\\ell}/2^{2\\ell} = 1/2^{\\ell}\\). This is the probability that our algorithm fails. It guesses correctly with probability at least \\(1- 1/2^{\\ell}\\). More formally, denoting \\(b\\) the coin and \\(\\mathcal{A}\\) our algorithm, we have: \\[ \\begin{aligned} \\Pr_{\\mathsf{k}}[\\mathcal{A}(\\mathsf{k}) = \\text{&quot;heads&quot;}\\mid b = \\text{&quot;heads&quot;}] &amp;= 1 \\\\ \\Pr_{\\mathsf{k}}[\\mathcal{A}(\\mathsf{k}) = \\text{&quot;tails&quot;}\\mid b = \\text{&quot;tails&quot;}] &amp;\\geq 1- 1/2^{\\ell} \\end{aligned} \\] At this point, the question can be answered by a “simple” probabilistic analysis. We want to find the probability \\[\\Pr_{\\mathsf{k},b}[\\mathcal{A}(\\mathsf{k}) = b]\\] By the law of total probability, we get \\[ \\begin{aligned} \\Pr_{\\mathsf{k},b}[\\mathcal{A}(\\mathsf{k}) = b] &amp;= \\Pr_{\\mathsf{k}}[\\mathcal{A}(\\mathsf{k}) = b | b = \\text{&quot;tails&quot;}]\\cdot\\Pr_{b}[ b = \\text{&quot;tails&quot;}]\\\\ &amp;+ \\Pr_{\\mathsf{k}}[\\mathcal{A}(\\mathsf{k}) = b | b = \\text{&quot;heads&quot;}]\\cdot\\Pr_{b}[ b = \\text{&quot;heads&quot;}] \\end{aligned} \\] We can rewrite this as \\[ \\begin{aligned} \\Pr_{\\mathsf{k},b}[\\mathcal{A}(\\mathsf{k}) = b] &amp;= \\Pr_{\\mathsf{k}}[\\mathcal{A}(\\mathsf{k}) = \\text{&quot;tails&quot;} | b = \\text{&quot;tails&quot;}]\\cdot\\Pr_{b}[ b = \\text{&quot;tails&quot;}] \\\\ &amp;+ \\Pr_{\\mathsf{k}}[\\mathcal{A}(\\mathsf{k}) = \\text{&quot;heads&quot;} | b = \\text{&quot;heads&quot;}]\\cdot\\Pr_{b}[ b = \\text{&quot;heads&quot;}] \\end{aligned} \\] Noting that \\(\\Pr_{b}[ b = \\text{&quot;heads&quot;}] = \\Pr_{b}[ b = \\text{&quot;tails&quot;}] = 1/2\\) and using the result of the previous question, we get: \\[ \\begin{aligned} \\Pr_{\\mathsf{k},b}[\\mathcal{A}(\\mathsf{k}) = b] &amp;\\geq \\frac{1}{2}(1+1-1/2^{\\ell}) = 1 - \\frac{1}{2}\\frac{1}{2^{\\ell}} = 1 - \\frac{1}{2^{\\ell+1}} \\end{aligned} \\] Observe that as \\(\\ell\\) grows, the algorithms succeeds with growing probability. It fails with negligible probability in \\(\\ell\\). Remark. The exercise tells us on a more “philosophical” level that we cannot hope to have unconditionally good PRGs. We need to base such construction on computational assumptions. This means that a pseudorandom string is always “distinguishable” from a random one but not efficiently distinguishable. Since we are able to only make efficient computations, the pseudorandom string “looks” random to us for all practical purposes. If the message is written with a different set of characters, like English letters, it is first processed into a bitstring, e.g. by associating to each letter its ASCII code in binary (https://en.wikipedia.org/wiki/ASCII).↩︎ We set the output length to be \\(2\\ell\\) for simplicity, but the idea could easily be adapted to any other output length.↩︎ Assuming, of course, that the universe is not completely deterministic.↩︎ Recall the XOR operation is associative, that is \\((a\\oplus b )\\oplus c = a\\oplus (b \\oplus c)\\). Therefore, we omit the parenthesis since the order of the operations do not matter.↩︎ "],["block-ciphers.html", "3 Block ciphers 3.1 Overview of block ciphers 3.2 Modes of operation 3.3 DES and AES Solved exercises", " 3 Block ciphers In this section, we focus on block ciphers, which are a more popular alternative to stream ciphers. Block ciphers are interesting not only for encryption, but they also have some interesting theoretical implications, since many other cryptographic primitives (like pseudorandom generators) can be built from block ciphers. In this section, we will learn: What is a block cipher, and what are the properties of a good block cipher; The different modes of operation of a block cipher; Two prime examples of block ciphers: DES and AES. 3.1 Overview of block ciphers Recall that the one-time pad, and stream ciphers in general, encrypt bits one by one. In contrast, block ciphers will split our plaintext in blocks of fixed length, and encrypt each of this as a single unit. Definition 3.1 A block cipher of length \\(\\ell\\) is an encryption scheme that encrypts a message of fixed length \\(\\ell\\). When encrypting an arbitrarily large message, we will split it into blocks of length \\(\\ell\\) and encrypt each block, using the same key, unlike in the previous section where we tried to stretch the key. Because of this, we will require a good block cipher to satisfy two new properties, that we informally describe below: Confusion: each bit of the ciphertext depends on several parts of the key. In other words, the relation between key and ciphertext must not be clear to any attacker. Diffusion: small changes in the plaintext result in significant changes in the ciphertext. More precisely, in any modern block cipher, it is expected that a single bit change in the plaintext should result in at least half of the bits of the ciphertext changing. Later in this section, we will see some concrete examples of block ciphers used in practice. For now, let us assume that we already have some block cipher \\[(\\mathsf{KeyGen},\\mathsf{Enc},\\mathsf{Dec}),\\] that we will use as a black box. That is, for now we do not know what happens inside each of the algorithms, only that they work and they are secure. For example, we assume that \\(\\mathsf{Enc}\\) takes as input a plaintext \\(\\mathsf{m}\\) of length \\(\\ell\\) and produces a ciphertext \\(\\mathsf{c}\\) corresponding to \\(\\mathsf{m}\\). We represent this by the diagram This will allow us to discuss block ciphers in a more general way. 3.2 Modes of operation Assume that we want to encrypt a message \\(\\mathsf{m}\\) of length \\(\\ell n\\) with a block cipher. When the message length is not a multiple of the block length, some extra bytes are added to complete the last block. This is called padding.11 We start by splitting the message in blocks \\[\\mathsf{m}_1,\\dots,\\mathsf{m}_n,\\] each of them of length \\(\\ell\\), so that they can be fed into our block cipher. The question is: do we encrypt each block in parallel? Is that secure? Or should we somehow make the blocks influence each other? The way we proceed here is determined by the mode of operation that we choose. 3.2.1 Electronic codebook (ECB) mode In ECB mode, we take the most straightforward approach, and encrypt each block on its own: \\[\\mathsf{c}_i=\\mathsf{Enc}_k(\\mathsf{m}_i).\\] The ECB mode is represented in the following diagram: The main advantage of this approach is that, since each block is independent, we can make the operations in parallel, potentially saving computation time. However, this mode presents several weaknesses. For example, since each block is encrypted in exactly the same way, two identical messages result in two identical ciphertexts. So an eavesdropper can see when the same message was sent twice. Even if he does not know the content, this provides the attacker with some partial information, which is something that we would like to avoid. Furthermore, the ECB mode is particularly bad when encrypting “meaningful” information. A very visual example comes from encrypting an image. Assume that we split the image into small squares of pixels, so that the bit length of each of these matches the length of our block cipher, and then use ECB-mode encryption on each square. Below you can see the result on an example image.12 Because the blocks are encrypted independently, a human eye can still easily distinguish the underlying information. This illustrates ECB mode’s lack of diffusion. 3.2.2 Cipher block chaining (CBC) mode So we have seen that we want our blocks to interact in some way. To achieve this, the CBC mode takes the following approach: the idea is to create a feedback loop, in which each ciphertext produced by the block cipher is fed back into the input of the next iteration, by computing the \\(\\mathsf{XOR}\\) with the new input. More precisely: \\[\\mathsf{c}_i=\\mathsf{Enc}_k(\\mathsf{m}_i\\oplus \\mathsf{c}_{i-1}).\\] The CBC mode is represented in the following diagram: Note that this does not work for the first block, since there is no previous ciphertext, and so we introduce something to replace it, which we call the initialization vector (often denoted by IV). By choosing the IV at random, we also introduce randomness in our scheme, making the procedure non-deterministic. Observe also that, due to the recursive nature of the definition, the encryption of a block is not only influenced by the previous block, but by every block that came before, and also the IV. There is no need for the IV to be secret, although it should not be reused, so if a new encryption sessions starts, a new IV should be chosen. With this approach, we achieve a much higher diffusion. Looking again at the same picture, the result is now very different: This is due to the fact that the encryption of each block influences the next. Thus, two identical blocks (for example, two squares of white in the corner of the picture) do not produce the same output anymore. The downside of this approach is that, since we need a ciphertext before we can compute the next, we cannot parallelize the computations. 3.2.3 Output feedback (OFB) mode The next mode of operation actually turns a block cipher into a stream cipher, by recomputing a key each time through the \\(\\mathsf{Enc}\\) algorithm. That is, it is a stream cipher in which the key stream is produced in blocks of length \\(\\ell\\): \\[\\begin{aligned} &amp; \\mathbf w_i = \\mathsf{Enc}_k(\\mathbf w_{i-1}), \\\\ &amp; \\mathsf{c}_i=\\mathsf{m}_i\\oplus \\mathbf w_i. \\\\ \\end{aligned}\\] The OFB mode is represented in the following diagram: Again, we need an IV to feed into \\(\\mathsf{Enc}\\) in the first iteration. Observe that the block cipher and the feedback loop do not involve the message at all, which is simply \\(\\mathsf{XOR}\\)’ed with the result of each iteration of the loop to produce the ciphertext, as in any stream cipher. While, as the CBC mode, the computation cannot be performed in parallel, the fact that the loop does not depend on the message at all allows us to pre-compute a bunch of key blocks in advance, for later use with a message. 3.2.4 Counter (CTR) mode Similarly to the OFB mode, the CTR mode produces a stream cipher from a block cipher. It works by keeping a public counter \\(\\mathsf{ctr}\\) that is chosen randomly, and is increased by \\(1\\) after each iteration. The counter works as an IV that updates after encrypting each block. \\[\\begin{aligned} &amp; \\mathsf{ctr}_i = \\mathsf{ctr}_{i-1}+1, \\\\ &amp; \\mathsf{c}_i=\\mathsf{m}_i\\oplus \\mathsf{Enc}_k(\\mathsf{ctr}_i). \\\\ \\end{aligned}\\] After the last iteration, the current value of the counter is also sent, together with the ciphertext. The CTR mode is represented in the following diagram: Exercise 3.1 We have not discussed the decryption procedure of any of the modes of operation. Given a \\(\\mathsf{Dec}\\) algorithm that recovers plaintexts encrypted with \\(\\mathsf{Enc}\\), describe how decryption works for each mode of operation. Recall that the values of the IV and counter are public. 3.3 DES and AES Now that we know how to use block ciphers, let us look a bit into some of the most famous ones: the Data Encryption Standard (DES), and its successor the Advanced Encryption Standard (AES). DES was designed by a team at IBM in 1974, and became the first official encryption standard in the US in 1977. It remained as the recommended encryption scheme until 1999, when it was replaced by AES.13 3.3.1 Data Encryption Standard (DES) DES is a block cipher of length \\(64\\), which uses a key of \\(56\\) bits. Essentially, it is composed of \\(16\\) identical rounds, each of them consisting of the following. During each round, a round key \\(\\mathsf{k}_i\\) of 48 bits is derived from the master key. In round \\(i\\), the algorithm receives \\(\\mathsf{m}_{i-1}\\), the output of the previous round (or the original message, for \\(i=1\\)), and computes \\(\\mathsf{m}_i\\), the output of the current round. In between, the following step happen: Split \\(\\mathsf{m}_{i-1}\\) in two halves \\(\\mathbf L_{i-1}, \\mathbf R_{i-1}\\) of 32 bits each. Derive the round key \\(\\mathsf{k}_i\\) from \\(\\mathsf{k}\\). Set \\(\\mathbf L_{i}=\\mathbf{R}_{i-1}\\) and \\(\\mathbf R_{i}=\\mathbf{L}_{i-1}\\oplus f(\\mathbf R_{i-1},\\mathsf{k}_i)\\). Return \\(\\mathbf m_i=(\\mathbf L_i, \\mathbf R_i)\\) After round \\(16\\), the end result \\(\\mathsf{m}_{16}\\) is the ciphertext. Decryption of DES is almost the same as encryption, starting from the last round key. Observe that one half of the input of each round is not encrypted, just moved around, and so in total each half of the plaintext is encrypted \\(8\\) times by \\(\\mathsf{XOR}\\)’ing it with a function of the round key. There are a couple of details that we have not specified yet: How to derive round keys from the master key \\(\\mathsf{k}\\). Without getting into much detail, the \\(\\mathsf{k}_i\\) is obtained from \\(\\mathsf{k}\\) by performing some rotations and permutations on the positions of the bits, and then some bits are ignored. How the function \\(f\\) works. First, the function expands the 32-bit input \\(\\mathbf R_{i-1}\\) to a 48-bit string, by repeating some of the bits in specified positions. The result is then \\(\\mathsf{XOR}\\)’ed with the round key \\(\\mathsf{k}_i\\). The result from this operation is then split into \\(8\\) blocks of \\(6\\) bits each, and fed into what is known as substitution boxes (S-boxes), which are functions specified by a lookup table. Each box outputs a string of 4 bits, so in total we have a string of \\(32\\) bits. Finally, the positions of the bits in this string are permuted, and the result is the output of the function \\(f\\). The design of the cipher, specially the function \\(f\\), might look arcane. Indeed, since the design and standardization of DES was not a public process, the reason behind some design choices is still not completely clear. What is known, however, is that the \\(f\\) function and the \\(S\\)-boxes were designed to thwart any attack known at the time (and even some that were not known to the public). The takeaway message here is that the \\(S\\)-boxes and the final permutation play a big role in achieving a good level of diffusion, propagating change through the whole ciphertext in the following rounds. Indeed, we have the following result. Proposition 3.1 By the end of the fifth round of DES, every single bit of the current ciphertext depends on all the bits of the plaintext and all the bits of the key. No sophisticated efficient attacks are known against DES to date. However, the key size is simply too small for today’s standards (look again at the table at the end of section 1), and so it was eventually replaced by AES. Some variants of DES, like 3DES, which essentially means applying DES three times in a row, are still in use, and have withstood any attacks so far. 3.3.2 Advanced Encryption Standard (AES) AES is a block cipher with block length \\(128\\) bits. Unlike DES, which used \\(56\\)-bit keys, AES supports keys of bit length \\(128\\), \\(192\\) and \\(256\\), and has between \\(10\\) and \\(14\\) rounds, depending on the key length. Moreover, while in DES only half of the block was encrypted in each round, the full block is encrypted in every round now. On a very high level, each round consists of the following steps, called layers: Key addition layer: a round key of length \\(128\\) is derived from the master key, in a process called key schedule. Byte substitution layer: similarly to DES, AES uses \\(16\\) S-boxes defined by lookup tables, replacing each byte of the message by a new byte specified by the corresponding \\(S\\)-box. This layer introduces confusion. Diffusion layer: the position of the bytes are permuted. Then, blocks of four bytes are combined using some matrix operations. Regarding security, no attack more efficient than brute force is known to date. Thus, the security level provided by AES is \\(\\lambda\\), where \\(\\lambda\\in\\{128,192,256\\}\\) is the bit size of the key. The following video has a very clear and concise overview of the inner workings of AES. Let us now demonstrate how AES satisfies diffusion and confusion. To do this we will do the follwoing: Confusion: Sample a random message of 128 bits and encrypt it with a random key \\(k\\). Then encrypt it with another key \\(k&#39;\\) that is the same as \\(k\\) except that it has a random bit fliped and compare the two ciphertexts. Diffusion: First encrypt a single AES block of a random message with a key \\(k\\). Then flip a single bit of the initial message and compare the two ciphertexts. What is the expected outcome in the experiments? In both cases we would want the two produced ciphertexts to be completly unrelated. That is, the two ciphertext should look like randomly sampled strings (although they are not!). In two random strings, the probability that the \\(i\\)-th bit is different is 50%. Therefore, we expect that if the properties hold, half of the bits will be different. Note that this is a probabilistic argument and the result concernes the expected value. In other words, repeat the experiment sufficiently many times and the average of changed bits will converge to 64 (=128/2). We first present a diffusion_confusion_util.sage code snippet that contains two functions: 1. flip_random_bit: flips a random bit from a 16 byte (=128 bit) message, 2. count_bit_diff: counts the bits in which two byte strings differ. import random def flip_random_bit(s): &quot;&quot;&quot; Flips a bit of a 16 byte string as follows: 1. Convert k from bytes to bits, 2. Choose a random position in the range [0,..,127] 3. Flip the bit of that position 4. Change back to bytes &quot;&quot;&quot; #Convert to bits s_bits = &#39;&#39;.join(f&#39;{byte:08b}&#39; for byte in s) # Select a random bit to flip random_bit_index = random.randint(0, len(s_bits) - 1) # Flip the selected bit s_prime_bits = ( # before index it remains the same s_bits[:random_bit_index] + # flip index (&#39;1&#39; if s_bits[random_bit_index] == &#39;0&#39; else &#39;0&#39;) + # after index it remains the same s_bits[random_bit_index + 1:] ) # Convert back to bytes s_prime = bytes(int(s_prime_bits[i:i+8], 2) for i in range(0, 127, 8)) return s_prime def count_bit_diff(b1, b2): &quot;&quot;&quot; Counts how many bits are different in b1 b2 &quot;&quot;&quot; diff_count = 0 # repeat for each byte for byte1, byte2 in zip(b1, b2): # Convert each byte to an 8-bit binary string and compare bit by bit for bit1, bit2 in zip(f&#39;{byte1:08b}&#39;, f&#39;{byte2:08b}&#39;): if bit1 != bit2: diff_count += 1 return diff_count We are now ready to conduct the experiments. To this end we use the AES recipie from the cookbook. Run these experiments to verify that in both experiments the results converge to 64 as expected. # load the utils and the aes128 recipie load(&#39;diffusion_confusion_util.sage&#39;) load(&#39;aes128.sage&#39;) def diffusion_experiment(): &quot;&quot;&quot; Encrypts a random message with AES128 under a key k. Then flip a bit of m and encrypt the result with k. Output the number of bits where the two ciphertexts differ. &quot;&quot;&quot; k = aes_keygen() # sample a random message of 16 bytes m1 = b&#39;&#39;.join(randrange(0,256).to_bytes() for _ in range(16)) # encrypt the message c1 = aes_enc(k, m1) # flip a bit of the message and encrypt m2 = flip_random_bit(m1) c2 = aes_enc(k, m2) # Count the number of bits c1, c2 differ return count_bit_diff(c1, c2) def confusion_experiment(): &quot;&quot;&quot; Encrypts a random message with AES128 under a key k. Then flip a bit of k and encrypt the same message again. Output the number of bits where the two ciphertexts differ. &quot;&quot;&quot; k1 = aes_keygen() # sample a random message of 16 bytes m = b&#39;&#39;.join(randrange(0,256).to_bytes() for _ in range(16)) # encrypt the message c1 = aes_enc(k1, m) # flip a bit of the key and encrypt k2 = flip_random_bit(k1) c2 = aes_enc(k2, m) # Count the number of bits c1, c2 differ return count_bit_diff(c1, c2) # run n iterations of each experiment n = 100000 diffusion_result = 0 confusion_result = 0 for i in range(n): diffusion_result = diffusion_result + diffusion_experiment() confusion_result = confusion_result + confusion_experiment() diffusion_result = float(diffusion_result / n) confusion_result = float(confusion_result / n) print(diffusion_result) print(confusion_result) Solved exercises Exercise 3.2 Consider the following mode of operation: to encrypt a block apply the operation \\(\\mathsf c_i = \\mathsf{Enc_{k}}(\\mathsf{Enc_{k}}(\\mathsf m_i)\\oplus\\mathsf m_{i-1})\\). Let \\(\\mathsf m_1,\\ldots,\\mathsf m_n\\) be a message you want to encrypt. How would the first message \\(\\mathsf m_1\\) be encrypted? How would decryption work? Solution. For \\(\\mathsf m_1\\) we don’t have a corresponding \\(\\mathsf m_0\\) to apply formula. We need to send an IV that will be public and correspond to \\(\\mathsf m_0\\). Simply set \\(\\mathsf m_0 = \\text{IV}\\) for a random IV and apply the formula. In this type of exercises, you should always keep in mind that you need to “solve” the equation for \\(\\mathsf m_i\\). To “move” stuff in the left and right hand side, use the inverse functions. For \\(\\mathsf{Enc}\\) this should be \\(\\mathsf{Dec}\\) and for \\(\\oplus x\\) it is itself (\\(a \\oplus x \\oplus x = a\\oplus 0 = a)\\). You should be careful to apply the operations in the correct order. Let’s now start recovering \\(\\mathsf{m}_i\\). First we remove the outer encryption layer by applying \\(\\mathsf{Dec}\\) on both sides of the equation. The new equation becomes: \\[ \\mathsf{Dec_k}(\\mathsf c_i) = \\mathsf{Enc_k}(\\mathsf{m}_i)\\oplus \\mathsf m_{i-1} \\] We now move \\(\\mathsf m_{i-1}\\) to the left side by applying in both sides \\(\\mathsf m_{i-1}\\). The new equation becomes. \\[ \\mathsf{Dec_k}(\\mathsf c_i)\\oplus \\mathsf m_{i-1} = \\mathsf{Enc_k}(\\mathsf{m}_i) \\] Finally we can recover \\(\\mathsf{m}_i\\) by “canceling” the encryption of the right hand side. To this end we apply \\(\\mathsf{Dec}\\) on both sides. This gives also the final decryption procedure: \\[ \\mathsf{m}_i = \\mathsf{Dec_k}(\\mathsf{Dec_k}(\\mathsf c_i)\\oplus \\mathsf m_{i-1}) \\] Exercise 3.3 You want to encrypt your personal computer’s hard drive. After reviewing the options you have, you decide to use AES as the block cipher but you are unsure whether it would be better to use CBC mode or Counter mode. What would you choose and why? Note that both modes are secure so there is no security concern. Solution. Since security is not a concern, you should consider efficiency. How do the modes compares to each other efficiency wise? The crucial observation is that CBC is sequential while Counter mode is not. This means that in order to decrypt the \\(i\\)-th block using CBC, you need to first decrypt all blocks from \\(1\\) to \\(i-1\\). In contrast to that, you can directly use Counter mode to decrypt it. Since you certainly wouldn’t want to decrypt a bunch of blocks each time you access the \\(i\\)-th block, Counter mode is the clear choice. Note also that since Counter mode is not sequential, you can also encrypt/decrypt blocks in parallel. Exercise 3.4 You are using a block cipher with counter mode to encrypt a message consisting of \\(n\\) blocks. The \\(i\\)-th block is corrupted during transit (some bits are flipped). Which blocks would be affected by that? What would have happened if you used CBC mode instead? Solution. First, let’s recall how decryption of counter mode works. To decrypt the \\(j\\)-th block, we compute \\(\\mathsf{m}_j=\\mathsf{c}_j\\oplus\\textsf{Enc}_k(\\mathsf{ctr}_j)\\). This means each ciphertext is not affected by the previous and next ciphertexts. Thus, only the \\(i\\)-th block corresponding to \\(\\mathsf{m}_i\\) will not be correctly decrypted. First, let’s recall how decryption of CBC works. To decrypt the \\(j\\)-th block, we compute \\(\\mathsf{m}_j=\\textsf{Dec}_k(\\mathsf{c}_j)\\oplus \\mathsf{c}_{j-1}\\). For the message to be correct, both the values \\(\\mathsf{c}_j\\) and \\(\\mathsf{c}_{j-1}\\) need to be correct. In our case, we have a corrupted \\(\\mathsf{c}_i\\) block. This means that when \\(j=i\\) and when \\(j-1=i\\) we will not retrieve the correct plaintext. Equivalently, we will retrieve wrong messages \\(\\textsf{m}_{i}&#39;\\neq\\textsf{m}_{i}\\) and \\(\\textsf{m}_{i+1}&#39;\\neq\\textsf{m}_{i+1}\\). All other messages will be correctly decrypted. Exercise 3.5 You use the counter mode but decide to make a modification (spoiler alert: this is almost never a good idea!). Instead of picking a random IV, you always start with \\(IV = 1\\). Is your custom mode of operation secure? Why? Solution. This custom mode is not secure! The problem is that you use the same IV each time you encrypt a message. Consider for simplicity the case where you encrypt two messages \\(\\mathsf m = (\\mathsf m_1, \\ldots, \\mathsf m_n)\\) and \\(\\mathsf m&#39; = (\\mathsf m&#39;_1, \\ldots, \\mathsf m&#39;_n)\\). An adversary sees the two corresponding ciphertexts. It now knows that \\[ \\mathsf c_i = \\mathsf m_i \\oplus \\mathsf{Enc_k} (i),\\qquad \\mathsf c&#39;_i = \\mathsf m&#39;_i \\oplus \\mathsf{Enc_k} (i) \\] XORing the two ciphertexts, it can learn the XOR of the two plaintexts! Indeed, \\[ \\begin{aligned} \\mathsf c_i\\oplus \\mathsf c&#39;_i &amp;= \\mathsf m_i \\oplus \\mathsf{Enc_k}(i)\\oplus \\mathsf m&#39;_i \\oplus \\mathsf{Enc_k}(i) \\\\ &amp;= \\mathsf m_i \\oplus \\mathsf m&#39;_i \\oplus \\mathsf{Enc_k}(i)\\oplus \\mathsf{Enc_k}(i) \\\\ &amp;= \\mathsf m_i \\oplus \\mathsf m&#39;_i \\end{aligned} \\] Note that this type of attack has other implications as well. You should make sure that each counter is not used twice. Therefore the block should be large enough so the probability that you use the same counter is negligible as long as you sample it randomly. Exercise 3.6 Recall that OTP is not secure if you use the same key twice. If you use the same key to encrypt a message \\(\\mathsf m_1,\\ldots,\\mathsf m_n\\) with OTP, what mode of operation are you using? You decide to use OTP as the block cipher in CBC mode. That is, you pick a random key \\(\\mathsf{k}\\) and use \\(\\oplus \\mathsf k\\) as the block cipher function. Is this a secure approach? Solution. This corresponds to the ECB mode. By definition, the ECB mode applies the same block cipher in parallel. This is not a secure approach. It suffers from the problems that we encountered when using OTP with the same key. Let us be more concrete. To encrypt a message you do the following: \\[ \\mathsf c_i = \\mathsf{Enc}_k(\\mathsf m_i \\oplus \\mathsf c_{i-1}) = \\mathsf m_i \\oplus \\mathsf c_{i-1} \\oplus \\mathsf k \\] Now, consider the next block. It’s encryption would be \\[ \\mathsf c_{i+1} = \\mathsf m_{i+1} \\oplus \\mathsf c_{i} \\oplus \\mathsf k \\] Substituting \\(\\mathsf{c}_i\\) we get \\[ \\begin{aligned} \\mathsf c_{i+1} &amp;= \\mathsf m_{i+1} \\oplus \\mathsf m_i \\oplus \\mathsf c_{i-1} \\oplus \\mathsf k \\oplus \\mathsf k \\\\ &amp;= \\mathsf m_{i+1} \\oplus \\mathsf m_i \\oplus \\mathsf c_{i-1} \\end{aligned} \\] and therefore \\[ \\begin{aligned} \\mathsf c_{i+1} \\oplus \\mathsf c_{i-1} = &amp;= \\mathsf m_{i+1} \\oplus \\mathsf m_i \\end{aligned} \\] and therefore we can learn the XOR value of each block. One must be careful when choosing padding, as some choices are vulnerable to certain attacks in some modes of operation. An example of this is the padding oracle attack: https://en.wikipedia.org/wiki/Padding_oracle_attack.↩︎ Source: https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation. Images by Larry Ewing (lewing@isc.tamu.edu) and GIMP (https://www.gimp.org/).↩︎ Some interesting bits of history around DES and NSA involvement can be found in Chapter 3 of Christof Paar and Jan Pelzl. Understanding cryptography: a textbook for students and practitioners. Springer Science &amp; Business Media, 2009..↩︎ "],["security-definitions.html", "4 Security definitions 4.1 Key recovery experiment 4.2 Indistinguishability under ciphertext only experiment 4.3 Indistinguishability under chosen plaintext attack 4.4 Indistinguishability under chosen ciphertext attack 4.5 Notes on defining and interpreting security Solved exercises", " 4 Security definitions So far we are a bit informal about what a “secure” cryptosystem means. For example one-time-pad is “perfectly secure”, but if we reuse the same key it is completely broken. Or ECB mode (never use ECB mode!) could be used if the same message is not encrypted twice. It seems so far that we define schemes and later consider “caveats”; certain use cases and scenarios where the constructions are broken. However, the stakes are often too high to allow such ad-hoc arguing. What happens for example if we fail to identify a specific “misuse” until it is too late? In the end, everything is secure until it isn’t! Modern cryptography aims to overcome such issues by introducing formal, well defined properties a scheme should satisfy and then proving (in a mathematical sense) that a scheme indeed satisfies them. Such a proof is indisputable evidence that the scheme is secure14. Or to be more precise, it is not vulnerable to the class of attacks that are prevented in the framework the scheme is analyzed in. But how can one define “security” in a formal sense? This seems difficult since the security notion depends on the intended use. For example, do we intend to send a single message or encrypt multiple messages with the same key? Or what if some eavesdropper knows information about the encrypted messages (for example that they are either an encryption of “YES” or “NO”)? The way cryptographers have solved this problem is by “abstraction”. The general idea is as follows: define an “experiment” that involves two parties, the challenger that uses the encryption scheme and the adversary that tries to break it. In these experiments, both parties must follow some rules and at the end the adversary tries to solve a challenge. If it succeeds the scheme is insecure (under the specific experiment). On the other hand, if all possible adversaries fail then the scheme is secure! This might sound a bit confusing at first. To make things clearer, in this section we first present two such experiments to warm up: the Key recovery experiment and the Indistinguishability under ciphertext only experiment. After that we present two more experiments that are widely used in practice: Indistinguishability under chosen plaintext attack and Indistinguishability under chosen ciphertext attack. 4.1 Key recovery experiment The most basic requirement to characterize an encryption scheme as secure is that an eavesdropper, observing transmitted ciphertexts, cannot guess the secret key. Indeed, if this was possible, the used scheme would be completely useless, since recovering the key gives the ability to encrypt/decrypt any message! Let’s try to formalize this as an experiment between a challenger and an adversary. The two entities play the following game: The challenger samples a secret key \\(k\\) and encrypts a message \\(m\\) of its choice. It sends the ciphertext \\(c\\) to the adversary. The adversary provides a guess for the secret key \\(k&#39;\\). The adversary wins if and only if the guess is correct, that is if \\(k=k&#39;\\). We next present the key recovery experiment (KR) schematically: Note that the experiment is probabilistic. Specifically, the challenger samples a secret key which is a probabilistic event and uses the encryption algorithm which might be probabilistic as well15. We also allow the adversary to be probabilistic. Thus, when we talk about the outcome of the experiment, we care about the probability that an adversary succeeds. Exercise 4.1 Consider the one-time-pad encryption scheme for encrypting a message of length \\(\\lambda\\). Describe an adversary that succeeds in key recovery with probability \\(1/2^{\\lambda}\\). Clearly, an adversary can sample a key at random and with probability \\(1/2^{\\lambda}\\) this will be the correct key! In fact, note that this is true for any encryption scheme: sample a random key and use it to answer the challenge. This is basically a trivial attack that applies everywhere. An encryption scheme should be considered secure if there is no significantly better attack than the trivial one. We next give the formal definition. Definition 4.1 An encryption scheme \\((\\mathsf{KeyGen}, \\mathsf{Enc}, \\mathsf{Dec})\\) is secure under KR if for any probabilistic polynomial time adversary \\(\\mathcal{A}\\), the probability that the adversary succeeds in the KR experiment is at most \\(1/2^\\lambda + \\mathsf{negl}(\\lambda)\\). Let’s now consider how this definition could be used to assess security of a scheme. Suppose we try to test some cryptosystem X secure under key recovery attacks. There are two possible ways to proceed: either describe a concrete adversary \\(\\mathcal{A}\\), that acts in a specific way that results in a successful key recovery with “good” probability, or show that all possible adversaries fails to win the experiment with “good” probability. The first way aims to show that the scheme is insecure by demonstrating a concrete attack, while the latter proves that the scheme is secure by proving that no attack exists. Remark. Note the “for all” quantifier in the second case. This is an extremely powerful statement. If the second case is proven, it means not only that all currently known attacks of the scheme fail, but also that any possible attack -including attacks we have not even though of- will fail! The statement is so strong, that it essentially rules out the possibility that in the future some clever strategy will leak the key! Let’s now try to reflect on what the definition actually means in the real world. Let’s assume that a scheme is secure under the key recovery experiment. What is the implication of such a statement? Unfortunately, this does not give us too many guarantees. Essentially, we can only deduce that an eavesdropper (for example our internet service provider that transmits our messages) cannot learn the secret key if it sees a single message. This is not a very good model in the sense that it does not abstract well the reality. In particular, the definition does not capture many possibilities that could happen in the real world: What happens if we send many messages using the same key? What if the eavesdropper knows some part of the plaintext (for example our plaintext might start with a date)? What if the eavesdropper can successfully guess part of the key (for example the last bit)? What if the eavesdropper can actually learn the message by looking at the ciphertext without using the key at all? It seems that to abstract reality we need stronger models that capture more complex use-cases. Essentially, we need to change the “rules” of the experiment in favour of the adversary. If our scheme remains secure, then we get more guarantees about the security of our scheme! Consider for example the first issue. We could modify the experiment by requiring the challenger to send two (or more) ciphertexts to the adversary. This is clearly a stronger experiment! We next present a stronger experiment, aiming at abstracting the last two bullets. 4.2 Indistinguishability under ciphertext only experiment When using an encryption scheme, our ultimate goal is not to protect the secret key but rather to hide the plaintexts that we encrypt. Keeping the secret key secret is in fact a means and not the goal. We can try to capture this directly in an experiment. In fact, this is a good idea since there is no need to believe that knowing the secret key is the only way to decrypt a message! The most natural way to proceed would be to modify the KR experiment by requiring the adversary to output the plaintext \\(m\\) instead of the key \\(k\\). However, this is a week definition that still leaves a lot of attack possibilities open. For example, it does not capture that the adversary might have partial information about the message or that it might be able to deduce small parts of it! Let’s try to make the definition stronger so that it captures also the above scenarios. Instead of requiring the adversary to guess the message, we present it with two possible messages \\(m_0, m_1\\) and we give the encryption of one of them. The challenge of the adversary is to guess which message we encrypted. Clearly this is a much easier task than decrypting the message and therefore an encryption scheme satisfying this definition is better! In fact, we will make the task of the adversary even easier: we allow the adversary to choose the two challenge messages \\(m_0, m_1\\)! We only add the restriction that the messages are of the same length. We present the indistinguishability under ciphertext only experiment (IND-CO) experiment next.   Before diving in the details of the experiment, let us focus on a somewhat subtle point: we require that the adversary outputs messages of the same length. This means that in the real world we don’t intend to hide the length of the encrypted messages! Indeed, by observing the traffic of a network we can learn information about the message sizes since in most (if not all) applications, the ciphertext size depends linearly on the plaintext size. Note that this is neither good nor bad. The system designers and users set the requirements of an application and in encryption, it is standard to require that the contents of transmitted messages are hidden but their sizes are not. And our abstract model cleanly captures this by requiring the adversary to output challenge messages of equal sizes. Let’s now define security more formally. As in the previous game, there is a trivial attack that succeeds with probability 1/2: simply guess the bit \\(b\\) at random! We quantify the success of an adversary \\(\\mathcal{A}\\) by measuring how much better \\(\\mathcal{A}\\) can do compared to this trivial attack. Definition 4.2 Consider an execution of the experiment IND-CO with an encryption scheme \\(\\mathcal{E}\\) and with an adversary \\(\\mathcal{A}\\). Let \\(p(\\lambda)\\) be the probability that \\(\\mathcal{A}\\) wins the experiment. We define the advantage of the adversary as \\(\\textsf{Adv}_{\\mathcal{E},\\text{IND-CO}}^{\\mathcal{A}}(\\lambda) = |p(\\lambda) - 1/2|\\). There is also an equivalent definition, that is easier to use when proving schemes secure. Definition 4.3 Consider an execution of the following experiments with an encryption scheme \\(\\mathcal{E}\\) and with an adversary \\(\\mathcal{A}\\): \\(\\text{IND-CO}_0\\) which is the same as IND-CO where we always choose \\(b=0\\), \\(\\text{IND-CO}_1\\) which is the same as IND-CO where we always output \\(b=1\\). Let \\(W_0\\) be the probability that \\(\\mathcal{A}\\) outputs 1 in \\(\\text{IND-CO}_0\\) and \\(W_1\\) be the probability that \\(\\mathcal{A}\\) outputs 1 in \\(\\text{IND-CO}_1\\) and let \\(p_0(\\lambda)\\) and \\(p_1(\\lambda)\\) be the probability that \\(W_0, W_1\\) happen respectively. We Define the advantage as \\(\\textsf{Adv}_{\\mathcal{E},\\text{IND-CO}}^{\\mathcal{A}}(\\lambda) = \\frac{1}{2}|p_0(\\lambda) - p_1(\\lambda)|\\). Why are these definitions equivalent? We can show it with a simple probabilistic arguement. Recall that \\(p(\\lambda)\\) is the probability that \\(\\mathcal{A}\\) guesses correctly, that is, \\(b&#39;=b\\). We have: \\[ \\begin{aligned} p(\\lambda) &amp;= \\Pr[\\mathcal{A} \\text{ guesses } b&#39;=b] \\\\ &amp;= \\Pr[\\mathcal{A} \\text{ guesses } b&#39;=1|b=1]\\Pr[b=1] + \\Pr[\\mathcal{A} \\text{ guesses } b&#39;=0|b=0]\\Pr[b=0] \\\\ &amp;= \\frac{1}{2}\\Pr[\\mathcal{A} \\text{ guesses } b&#39;=1|b=1] + \\frac{1}{2}\\Pr[\\mathcal{A} \\text{ guesses } b&#39;=0|b=0] \\\\ &amp;= \\frac{1}{2}\\Pr[\\mathcal{A} \\text{ guesses } b&#39;=1|b=1] + \\frac{1}{2}(1 - \\Pr[\\mathcal{A} \\text{ guesses } b&#39;=1|b=0]) \\\\ &amp;= \\frac{1}{2} + \\frac{1}{2}\\Pr[\\mathcal{A} \\text{ guesses } b&#39;=1|b=1] - \\frac{1}{2}\\Pr[\\mathcal{A} \\text{ guesses } b&#39;=1|b=0]) \\\\ &amp;= \\frac{1}{2} + \\frac{1}{2}(p_1(\\lambda) - p_0(\\lambda)) \\end{aligned} \\] and therefore \\[ \\left|p(\\lambda) - \\frac{1}{2}\\right| = \\left|\\frac{1}{2} + \\frac{1}{2}(p_1(\\lambda) - p_0(\\lambda)) -\\frac{1}{2} \\right| = \\frac{1}{2}\\left|p_1(\\lambda) - p_0(\\lambda)\\right| \\] Finally, we are ready to define what it means for an encryption scheme to satisfy IND-CO security. Essentially, we require that the advantage of any probabilistic polynomial time adversary is negligible in \\(\\lambda\\). In other words, no adversary can do noticably better than simply guessing the bit \\(b\\). Definition 4.4 An encryption scheme \\(\\mathcal{E}\\) is IND-CO secure if for all PPT adversaries \\(\\mathcal{A}\\), their advantage is negligible in \\(\\lambda\\), that is, \\(\\textsf{Adv}_{\\mathcal{E},\\text{IND-CO}}^{\\mathcal{A}}(\\lambda) = \\textsf{negl}(\\lambda)\\). Remark. For security, we require that the advantage of the adversary is negligible. The factor \\(\\frac{1}{2}\\) in the alternative definition is not important for this: \\(\\frac{1}{2}\\left|p_1(\\lambda) - p_0(\\lambda)\\right|\\) is negligible in \\(\\lambda\\) if and only if \\(\\left|p_1(\\lambda) - p_0(\\lambda)\\right|\\) is negligible in \\(\\lambda\\). Therefore, one can also define the adversary’s advantage as \\(\\textsf{Adv}_{\\mathcal{E},\\text{IND-CO}}^{\\mathcal{A}}(\\lambda) = |p_0(\\lambda) - p_1(\\lambda)|\\). Again, let’s reflect a bit on the experiment and its relation to the real world. Clearly, if an adversary cannot win the IND-CO experiment with good probability, it means it cannot guess the decryption of a ciphertext \\(c\\). Indeed, if it could guess the decryption, it would see if \\(\\textsf{Dec}_k(c) = m_0\\) or \\(\\textsf{Dec}_k(c) = m_1\\) and easily answer the challenge! In fact, we get much stronger guarantees! The adversary can deduce no information about the plaintext by looking at the ciphertext (with good probability). To see this, consider the following: assume the adversary can learn any property \\(P\\) of the underlying message. Then it can simply do the following: Select \\(m_0\\) that has the property \\(P\\) and \\(m_1\\) that does not. Examine the ciphertext \\(c\\) and deduce if the underlying plaintext satisfies \\(P\\) or not. If it does, answer the challenge with \\(b&#39;=0\\) and otherwise \\(b&#39;=1\\). If some adversary could learn anything about the underlying plaintext with some probability, it would win the IND-CO experiment with the same probability. Which in turn means that if an encryption scheme is secure under IND-CO, no information of the plaintext is leaked by the ciphertext. Exercise 4.2 Show that if an encryption scheme is secure under IND-CO, then it secure under KR. This definition is quite strong. However, we should be careful when interpreting the real world consequences. Here, we can only guarantee that an encryption scheme satisfying this definition implies that no information is leaked about a message as long as this is the only message used! No guarantee is given for the case where the same key is used to encrypt more messages. 4.3 Indistinguishability under chosen plaintext attack In this section we will present a stronger experiment. In particular we will modify the IND-CO experiment by allowing the adversary to ask for encryptions of messages of its choice before sending the messages \\(m_0, m_1\\) to be challenged on16. Note that this is clearly a stronger requirement than IND-CO. Therefore, it should be harder to come up with schemes that satisfy the stronger definition and if we do, the security guarantees are better. Before presenting the details though, it would be worthy to reflect on the choice of allowing an adversary to get encryptions. After all, it seems unrealistic since one needs the secret key to encrypt messages17. The reason is because in the real world, the adversary might predict what messages are encrypted and therefore collect valid plaintext-ciphertext pairs. As a good measure, we consider the worst possible scenario in our abstraction: the adversary has full control over these pair of messages. This is not a far-fetched scenario or cryptographers being unjustifiably conservative, but rather something that has happened in various occasions! We first briefly present one such occasion and then we formally define the IND-CPA experiment. 4.3.1 Casestudy: the battle of Midway In May 1942 the battle of Midway took place. Midway is a small island in the middle of the Pacific ocean with important strategic significance. The Battle of Midway is widely considered a turning point in the Pacific War. And this turning point is in big parts due to cryptography. The US Navy cryptanalysts, after having intercept various Japan communication, had partially broken the Japanese Navy’s JN-25b code. They had concluded that there was going to be an upcoming operation on location “AF” but where unsure what “AF” stood for. Suspecting “AF” might refer to the Midway island, the US Navy came up with a devious plan: they broadcast unencrypted messages stating that Midway’s water purification system had broken down. The Japanese -ignoring the fact that these communication was unencrypted- starting sharing the news amongst them and the “AF” was present in various communications, asserting that “AF” indeed corresponding to Midway island! With this information at hand, the US Navy prepared for the upcoming operation leading them to the victory. How is this connected with our abstract experiments? In an abstract level, what the US Navy actually did was to “ask” for an encryption of Midway and the Japan Navy naively responded with “AF”. It is such situations we will capture in our IND-CPA experiment next. 4.3.2 The IND-CPA experiment We are now ready to define our experiment. Essentially, it is the same as IND-CO but enhanced: the adversary can ask encryptions of arbitrary messages and the challenger provides them. We present the experiment in the next figure.   The experiment starts with the adversary asking encryptions of messages of its choice and the challenger provides them. This is referred to as the query phase. When the adversary is happy with the pairs of messages it has, the challenge phase starts: the adversary sends two equal-length messages and the challenger flips a coin and encrypts the corresponding message. The adversary wins if it finds what was the coin flip (or equivalently which message was encrypted). We define security similarly to the IND-CO case. First we define the advantage of the adversary, i.e. how much better it is than simply guessing \\(b\\) at random, and we characterize a scheme as secure if this advantage is negligible. We present the two definitions next. Definition 4.5 Consider an execution of the experiment IND-CPA with an encryption scheme \\(\\mathcal{E}\\) and with an adversary \\(\\mathcal{A}\\). Let \\(p(\\lambda)\\) be the probability that \\(\\mathcal{A}\\) wins the experiment. We define the advantage of the adversary as \\(\\textsf{Adv}_{\\mathcal{E},\\text{IND-CPA}}^{\\mathcal{A}}(\\lambda) = |p(\\lambda) - 1/2|\\). Definition 4.6 An encryption scheme \\(\\mathcal{E}\\) is IND-CPA secure if for all PPT adversaries \\(\\mathcal{A}\\), their advantage is negligible in \\(\\lambda\\), that is, \\(\\textsf{Adv}_{\\mathcal{E},\\text{IND-CPA}}^{\\mathcal{A}}(\\lambda) = \\textsf{negl}(\\lambda)\\). We also present the alternative definition: Definition 4.7 Consider an execution of the following experiments with an encryption scheme \\(\\mathcal{E}\\) and with an adversary \\(\\mathcal{A}\\): \\(\\text{IND-CPA}_0\\) which is the same as IND-CO where we always choose \\(b=0\\), \\(\\text{IND-CPA}_1\\) which is the same as IND-CO where we always output \\(b=1\\). Let \\(W_0\\) be the probability that \\(\\mathcal{A}\\) outputs 1 in \\(\\text{IND-CPA}_0\\) and \\(W_1\\) be the probability that \\(\\mathcal{A}\\) outputs 1 in \\(\\text{IND-CPA}_1\\) and let \\(p_0(\\lambda)\\) and \\(p_1(\\lambda)\\) be the probability that \\(W_0, W_1\\) happen respectively. We Define the advantage as \\(\\textsf{Adv}_{\\mathcal{E},\\text{IND-CPA}}^{\\mathcal{A}}(\\lambda) = \\frac{1}{2}|p_0(\\lambda) - p_1(\\lambda)|\\). 4.4 Indistinguishability under chosen ciphertext attack The reader at this point might suspect where we are heading. We started with a single indistinguishability experiment and “enhanced” it by allowing the adversary to ask for encryptions of messages of its choice. Why not also allow it to ask for decryptions of ciphertexts? This is exactly what we will do, but it would be usefull at first to reflect on whether this makes any sense at all. In the end, how would someone in the real world get ciphertext decryptions without the secret key? In the previous case we saw that this models some partial knowledge of what the messages correspond to, but in this case it remains unclear. And we better have a good reason since making our experiments stronger means it is harder to come up with schemes that are secure under them and usually such schemes are less efficient. Before we present the new experiment, we will show a devastating attack on block ciphers under CBC mode. The attack essentially takes advantage the an adversary can learn partial information about plaintexts by manipulating the ciphertexts. This partial information is enough to completely break security. This attack is a real world example (among others) that actually leads us to consider a broader class of attacks which the new model will cover. 4.4.1 Malleability of CBC mode Before we present the attack, we will first show that CBC mode is malleable. This means that we can manipulate the ciphertext in a way that causes a predictable effect on the plaintext. Consider encrypting a single block using CBC mode. To encrypt the message \\(m\\), we first sample a random \\(IV\\) and compute \\(c = \\textsf{Enc}_k(m\\oplus IV)\\). The final ciphertext is \\((IV, c)\\). To decrypt the message we need to compute \\(m = \\textsf{Dec}_k(c)\\oplus IV\\) Now, what happens if we try to decrypt the ciphertext \\((IV\\oplus x, c)\\) for some \\(x\\)? The decryption will give \\[ \\textsf{Dec}_k(c)\\oplus (IV \\oplus x) = (\\textsf{Dec}_k(c)\\oplus IV) \\oplus x = m\\oplus x \\] This means that manipulating the ciphertext \\((IV, c)\\) by XORing the \\(IV\\) with some value \\(x\\) causes a predictable change to the underlying plaintext \\(m\\): it results in the plaintext \\(m\\) XORed with the same value \\(x\\)! 4.4.2 Padding oracle attack The padding oracle attack allows to decrypt any message that was encrypted with CBC mode using only minimal information: given a ciphertext, we need to know if the underlying plaintext is well formed! Let’s now give some meaning to the “well formed” part. As we show in the previous sections, block ciphers only work with messages of specific lengths: multiples of the block size. But what happens if our message is not such a multiple? In this case we need to pad the message, i.e. fill the missing bytes with some default values. There are many ways to do such padding. We will focus on the ANSI X.923 standard. This is a very simple padding: fill the missing bytes with zeros and use the last byte to say how many bytes were padding bytes. Note that it is easy to “undo” the padding and get the actual message. Let’s see some examples: the following messages with length 16 bytes are correctly padded: \\[ \\begin{array}{cccccccccccccccc} \\textsf{83} &amp; \\textsf{14} &amp; \\textsf{A8} &amp; \\textsf{3F} &amp; \\textsf{BA} &amp; \\textsf{CD} &amp; \\textsf{3C} &amp; \\textsf{D7} &amp; \\textsf{D3} &amp; \\textsf{34} &amp; \\textsf{93} &amp; \\textsf{A8} &amp; \\textsf{00} &amp; \\textsf{00} &amp; \\textsf{00} &amp; \\textsf{04} \\\\ \\textsf{2A} &amp; \\textsf{BA} &amp; \\textsf{9F} &amp; \\textsf{A9} &amp; \\textsf{91} &amp; \\textsf{34} &amp; \\textsf{00} &amp; \\textsf{00} &amp; \\textsf{00} &amp; \\textsf{00} &amp; \\textsf{00} &amp; \\textsf{00} &amp; \\textsf{00} &amp; \\textsf{00} &amp; \\textsf{00} &amp; \\textsf{0A} \\\\ \\textsf{3A} &amp; \\textsf{FF} &amp; \\textsf{43} &amp; \\textsf{AC} &amp; \\textsf{D9} &amp; \\textsf{DD} &amp; \\textsf{45} &amp; \\textsf{9B} &amp; \\textsf{FD} &amp; \\textsf{F8} &amp; \\textsf{32} &amp; \\textsf{19} &amp; \\textsf{00} &amp; \\textsf{23} &amp; \\textsf{14} &amp; \\textsf{01} \\end{array} \\] while the following are not \\[ \\begin{array}{cccccccccccccccc} \\textsf{A9} &amp; \\textsf{93} &amp; \\textsf{32} &amp; \\textsf{00} &amp; \\textsf{0F} &amp; \\textsf{03} &amp; \\textsf{1A} &amp; \\textsf{BB} &amp; \\textsf{CA} &amp; \\textsf{DA} &amp; \\textsf{F9} &amp; \\textsf{91} &amp; \\textsf{EF} &amp; \\textsf{E9} &amp; \\textsf{A0} &amp; \\textsf{23} \\\\ \\textsf{93} &amp; \\textsf{88} &amp; \\textsf{7A} &amp; \\textsf{03} &amp; \\textsf{50} &amp; \\textsf{00} &amp; \\textsf{01} &amp; \\textsf{85} &amp; \\textsf{92} &amp; \\textsf{AB} &amp; \\textsf{B1} &amp; \\textsf{54} &amp; \\textsf{52} &amp; \\textsf{99} &amp; \\textsf{00} &amp; \\textsf{03} \\end{array} \\] The first is invalid because the standard requires at least one padding byte18 while the second is invalid because it claims it uses 3 padding bytes but the ending is not 00 00 03. Now consider a server that uses a block cipher in CBC mode to communicate with its clients. The above padding is used. Assume that the server receives a message. If the message is correctly padded it replies with “OK” and if it is not, it assumes there was an error during transmission and replies with “BAD REQUEST”. This is our oracle! We can manipulate intercepted ciphertexts, send them to the server and learn if they are well formed or not! This in combination with the XOR malleability of CBC mode will be enough to completely break the encryption! We will “decrypt” the message byte by byte. Consider we intercept a ciphertext \\((IV, c_1, c_2, \\ldots, c_\\ell)\\). We will first learn the message corresponding to \\(c_1\\). First, note that \\((IV, c_1)\\) is also a CBC ciphertext. Now, let’s assume the 15-th byte of \\(m_1\\) is not zero. That is, the message is of the form \\[ ??\\ ??\\ ??\\ ??\\ ??\\ ??\\ ??\\ ??\\ ??\\ ??\\ ??\\ ??\\ ??\\ ??\\ \\text{YY}\\ \\text{XX} \\] with YY being non-zero. In this case, the server will reply BAD REQUEST unless XX=01! Now, consider we send the following messages to the server: \\[ \\begin{aligned} &amp;(IV\\oplus 00\\ 00\\ \\ldots\\ 00\\ 00, c)\\\\ &amp;(IV\\oplus 00\\ 00\\ \\ldots\\ 00\\ 01, c)\\\\ &amp;(IV\\oplus 00\\ 00\\ \\ldots\\ 00\\ 02, c)\\\\ &amp;\\qquad\\qquad\\qquad\\vdots\\\\ &amp;(IV\\oplus 00\\ 00\\ \\ldots\\ 00\\ FE, c)\\\\ &amp;(IV\\oplus 00\\ 00\\ \\ldots\\ 00\\ FF, c)\\\\ \\end{aligned} \\] and note that the corresponding decryption for \\((IV\\oplus 00\\ 00\\ \\ldots\\ 00\\ b, c)\\) is \\(m\\oplus 00\\ 00\\ \\ldots\\ 00\\ b\\) due to the malleability of CBC mode! Since only the message ending with 01 will cause the server to answer with OK, we know that exactly 1 out of the 256 messages we sent will cause this! It is now enough to see which message it was that caused this response. Say that the malformed IV XORed with \\(00\\ \\ldots\\ 00\\ b\\) caused this response and denote the 16th byte of \\(m_1\\) as \\(m_{1,16}\\). Then, it must be the case that \\(m_{1, 16}\\oplus b = 01\\) and therefore \\(m_{1,16} = b \\oplus 01\\)! In this case we successfully learn the last byte of the first block. What happens if \\(YY=00\\) though? In that case both messages ending in \\(01\\) and \\(02\\) (and maybe more depending on the previous bytes) will cause the server to reply “OK”. We handle this as follows: we send the 256 message as described above and if we get a single “OK” reply, we know \\(YY\\neq 00\\) and we compute \\(m_{1,16}\\) as described above. If more than one values yield “OK” we know that \\(YY=00\\) and we repeat the above while forcing \\(YY\\) to become non-zero. Specifically, we now send the messages \\[ \\begin{aligned} &amp;(IV\\oplus 00\\ 00\\ \\ldots\\ 01\\ 00, c)\\\\ &amp;(IV\\oplus 00\\ 00\\ \\ldots\\ 01\\ 01, c)\\\\ &amp;\\qquad\\qquad\\qquad\\vdots\\\\ &amp;(IV\\oplus 00\\ 00\\ \\ldots\\ 01\\ FF, c)\\\\ \\end{aligned} \\] Now it is guaranteed that \\(YY\\neq 00\\) and arguing as before we can retrieve the last byte of the first block! Note that we needed to do only \\(512\\) tries. How do we continue from here though? Well, at this point we know \\(m_{1,16}\\). We will now try to make the initial message end in \\(00\\ 02\\)! To do this we send to the server the messages \\[ (IV\\oplus 00\\ 00\\ \\ldots\\ b\\ \\ m_{1,16}\\oplus 02, c)\\\\ \\] for all possible \\(b\\). Note that the last byte now will correspond to \\[ m_{1,16}\\oplus m_{1,16}\\oplus 02 = 00\\oplus 02 = 02 \\] There will be a single value \\(b\\) that will cause the server to send “OK” -the value causing the modified plaintext to be \\(00\\)- and then we can compute \\(m_{1,15}=b\\oplus 00 = b\\). We can now continue in this fashion inductively, to retrieve every single byte of the first block! And we can in fact continue block by block to retrieve the whole message. To see this, consider the ciphertext \\((IV, c_1, c_2,\\ldots, c_\\ell)\\). Assume we already know that \\(c_1\\) corresponds to \\(m_1\\). Then we can construct the valid CBC ciphertext \\((m_1, c_2, \\ldots, c_\\ell)\\), that is we set the new IV as \\(m_1\\) and we delete the first part of the ciphertext \\(c_1\\). We can now follow the same procedure to learn the decryption of the second block and continue this way until we decrypt the whole message! Let’s now consider whether the attack is efficient or not. Let’s focus on the case of a single block. Assuming block length \\(\\ell\\) bytes, a bruteforce attack would require \\(256^\\ell\\) tries since we need to try all possible bytes (256) for each position (\\(\\ell\\)). The padding oracle attack needs (roughly) \\(256\\ell\\) tries for each block. This is an exponential improvement compared to the bruteforce! This does not only makes the attack practical, it actually makes it a devastating one19! It is worthwhile at this point to reflect a bit and see what actually made this attack possible. It essentially boils down on two things: the malleability of the CBC mode, which allows us to predict how the plaintext will change if we perform certain operations on the ciphertext, the minimal information we received about the decryption of a ciphertext: whether the underlying ciphertext is correctly padded or not! Note that each query to the server conveys a single bit of information: valid/invalid. 4.4.3 The IND-CCA experiment The padding oracle attack shows that even minimal information about the ciphertexts can deem an encryption scheme insecure. Since it is always better to be conservative when defining experiments, we capture this by allowing the adversary to decrypt arbitrary ciphertexts of its choice. This not only covers the case that the adversary has some partial knowledge, but rather a very powerful adversary that can learn all the information about arbitrary ciphertexts that it is allowed to choose. There is however a caveat: we restrict the adversary from asking the decryption of the challenge ciphertext. If we don’t do this the definition is unsatisfiable: no matter the encryption scheme we are using, the adversary can simply ask to decrypt the challenge and win with probability 1. We present next the experiment. First there is a CPA phase, where the adversary asks for encryptions of messages of its choice, then we have the challenge phase where the adversary chooses two messages to be challenged on and finally we have the CCA phase, where the adversary gets decryption of any ciphertext it chooses apart from the challenge ciphertext. We note that, as in the IND-CPA case, there are more complex version of this experiment where the CPA and CCA queries can happen both before and after the challenge phase but we omit this for the shake of simplicity. We present the experiment in the next figure.   We define security similarly to the IND-CO and IND-CPA cases. First we define the advantage of the adversary, i.e. how much better it is than simply guessing \\(b\\) at random, and we characterize a scheme as secure if this advantage is negligible. We present the two definitions next. Definition 4.8 Consider an execution of the experiment IND-CCA with an encryption scheme \\(\\mathcal{E}\\) and with an adversary \\(\\mathcal{A}\\). Let \\(p(\\lambda)\\) be the probability that \\(\\mathcal{A}\\) wins the experiment. We define the advantage of the adversary as \\(\\textsf{Adv}_{\\mathcal{E},\\text{IND-CCA}}^{\\mathcal{A}}(\\lambda) = |p(\\lambda) - 1/2|\\). We also present the alternative definition: Definition 4.9 Consider an execution of the following experiments with an encryption scheme \\(\\mathcal{E}\\) and with an adversary \\(\\mathcal{A}\\): \\(\\text{IND-CCA}_0\\) which is the same as IND-CO where we always choose \\(b=0\\), \\(\\text{IND-CCA}_1\\) which is the same as IND-CO where we always output \\(b=1\\). Let \\(W_0\\) be the probability that \\(\\mathcal{A}\\) outputs 1 in \\(\\text{IND-CCA}_0\\) and \\(W_1\\) be the probability that \\(\\mathcal{A}\\) outputs 1 in \\(\\text{IND-CCA}_1\\) and let \\(p_0(\\lambda)\\) and \\(p_1(\\lambda)\\) be the probability that \\(W_0, W_1\\) happen respectively. We Define the advantage as \\(\\textsf{Adv}_{\\mathcal{E},\\text{IND-CCA}}^{\\mathcal{A}}(\\lambda) = \\frac{1}{2}|p_0(\\lambda) - p_1(\\lambda)|\\). Definition 4.10 An encryption scheme \\(\\mathcal{E}\\) is IND-CCA secure if for all PPT adversaries \\(\\mathcal{A}\\), their advantage is negligible in \\(\\lambda\\), that is, \\(\\textsf{Adv}_{\\mathcal{E},\\text{IND-CCA}}^{\\mathcal{A}}(\\lambda) = \\textsf{negl}(\\lambda)\\). 4.5 Notes on defining and interpreting security It should be clear at this point that abstracting the real world into simple experiments that are easier to analyze is no easy task. In fact, one can argue that it is more of an art rather than some systematic process! The goal of a security definition is to abstract the real world. To devise a “good” experiment, one should first understand the real world: what are the intended use cases, how the actors behave, what is the resources of real world adversaries etc. Then, it must show that the proposed abstraction sufficiently captures the above parameters. While the analysis under the models is systematic, the definition on the models themselves requires to some extent intuition and insights. Also, how strong should a model be? In general, cryptographers try to be “conservative” in their definitions. It is better to use a stronger definition since the stronger the definition, the more attacks it rules out. However, if one uses a very strong definition, it might be impossible (or very impractical) to construct a scheme to satisfy it! Furthermore, it is always crucial to understand what a definition implies. Recall that an encryption scheme is never proven just “secure” but rather prover “secure under experiment X”. When actually using the encryption scheme for some real world application, you must make sure that the intended use and the security requirements are actually captured by the experiment X. Failing to do so results in no security guarantees whatsoever! Think for example the one-time-pad. It can be easily shown IND-CO secure. If one, however, uses it in a context where many messages are encrypted under the same key, it is -as we have seen multiple times- completely insecure. Solved exercises Exercise 4.3 Construct an insecure encryption scheme (meaning that we can decrypt any message) that is KR secure. Solution. We will rely on the fact that it could be in principle possible to decrypt messages without using the secret key. Consider the following encryption scheme \\(\\mathcal{E}\\) that works as follows: \\(\\mathsf{KeyGen}(1^\\lambda)\\): Sample a random key in \\(\\{0,1\\}^\\lambda\\). \\(\\mathsf{Enc}_k(m)\\): Output \\(c:=m\\). \\(\\mathsf{Dec}_k(c)\\): Output \\(m:=c\\). It is evident that the scheme is insecure. In particular, it uses the identity function to encrypt/decrypt. Nevertheless we can easily prove that it is KR secure! In particular, consider any adversary \\(\\mathcal{A}\\) playing the KR game with a challenger using \\(\\mathcal{E}\\). The challenger presents a encryption \\(c\\) of an arbitrary message \\(m\\). By constuction of \\(\\mathcal{E}\\) \\(m=c\\) and during the experiment no information of the sample key is reveled. Therefore, the only possible strategy for \\(\\mathcal{A}\\) is to make a random guess as to what key was sampled. Therefore, the probability that \\(\\mathcal{A}\\) succeeds is exactly \\(1/2^\\lambda\\) and the construction KR secure. Remark. Of course this is a contrived example that noone would ever use in practice. The point of the exercise is to demonstrate limitations of the KR experiment: as demonstrated above, even schemes that are provably secure under this definition are completely insecure in reality. Exercise 4.4 Show that if an encryption scheme is deterministic, it is not CPA secure. Solution. Consider an encryption scheme \\(\\mathcal{E}\\) where the \\(\\mathsf{Enc}\\) algorithm is deterministic. We will construct an adversary \\(\\mathcal{A}\\) that wins the IND-CPA experiment with probability 1. The adversary \\(\\mathcal{A}\\) works as follows: It samples two messages \\(m_0\\neq m_1 \\in \\mathcal{M}\\). In the query phase it asks the challenger to give the encryption of \\(m_0\\). It receives \\(c_0\\) from the challenger. It presents \\(m_0, m_1\\) as its challenge. The challenger flips a coin and encrypts \\(m_b\\). It sends \\(c\\) to \\(\\mathcal{A}\\). The adversary replies to the challenge with \\(b&#39;=0\\) if \\(c=c_0\\) and with \\(b&#39;=1\\) otherwise. It is easy to see that \\(\\mathcal{A}\\) wins with probability 1. Indeed, since the encryption scheme is deterministic, everytime \\(m_0\\) is encrypted it results in the same ciphertext. Therefore if the challenge ciphertext \\(c\\) equals \\(c_0\\) then it must be the case that the challenger encrypted the first message \\(m_0\\) (therefore \\(b=0\\)) and if it is not it must have encrypted \\(m_1\\) (therefore \\(b=1\\)). Remark. Note how this generic argument automatically captures in an elegant way cases we have already analyzed in the course: OTP is a deterministic scheme and therefore it cannot be CPA secure. Indeed we have seen that reusing the same key for two messages reveals information about the encrypted plaintexts (try turn this into an explicit IND-CPA attack!). An encryption scheme based on a block cipher in ECB mode is deterministic and therefore not CPA secure. Note that the attack uses the exact same vulnerability we saw on ECB mode: encrypting the same message twice results in the same ciphertext. It is crucial to only use randomized encryption for real world applications, since CPA security is the absolute minimum requirement an encryption scheme must satisfy. Exercise 4.5 Consider an encryption scheme \\(\\mathcal{E}\\) that uses a blockcipher \\(F\\) in CBC mode. Show that if the selection of the \\(IV\\) is predictable \\(\\mathcal{E}\\) is not secure. Solution. Recall than in CBC mode encryption is done by computing \\(c_i = F_k(m_i\\oplus c_{i-1})\\) where \\(c_0\\) is the \\(IV\\). The idea is to use the knowledge of the IV to manipulate how the ciphertext looks like. Specifically, consider encrypting a message with a single block. If we know that a specific IV will be used, we can “cancel” the effect of the \\(IV\\): the encryption of \\(m\\oplus IV\\) will be \\[ (IV, F_k(m\\oplus IV\\oplus IV) = (IV, F_k(m)) \\] We construct an adversary \\(\\mathcal{A}\\) that uses the above fact to win the IND-CPA experiment by utilizing this fact. We next present how \\(\\mathcal{A}\\) works: First, \\(\\mathcal{A}\\) makes a CPA query to get an encryption of \\(m=0\\). The challenger responds with \\((IV_0, F_k(IV_0\\oplus 0)) = (IV_0, F_k(IV_0))\\). It then makes another CPA query to get again an encryption of \\(m=0\\). Note that in each query the challenger uses fresh randomness. It replies with \\((IV_1, F_k(IV_1))\\) Then \\(\\mathcal{A}\\) defines the challenge messages as follows: \\[ m_0 = IV\\oplus IV_0, \\qquad m_1 = IV\\oplus IV_1 \\] where \\(IV\\) is the value that will be used to produce the challenge ciphertext. The challenger replies with \\((IV, c)\\). \\(\\mathcal{A}\\) replies to the challenge with \\(b=0\\) if \\(c=F_k(IV_0)\\) and with \\(b=1\\) if \\(c=F_k(IV_0)\\). The adversary knows the values \\(F_k(IV_0)\\) and \\(F_k(IV_1)\\) from its CPA queries. We claim that these are the only two possibilities and that \\(\\mathcal{A}\\) will win with probability 1. To see this let’s observe what happens for each of the cases \\(b=0\\) and \\(b=1\\). Case \\(b=0\\): the challenger computes the encryption of \\(m_0 = IV\\oplus IV_0\\) using \\(IV\\) as the initialization vector. It therefore produces the ciphertext \\[ (IV, c) = (IV, F_k((IV\\oplus IV_0)\\oplus IV)) = (IV, F_k(IV_0)) \\] and the adversary seeing \\(c=F_k(IV_0)\\) replies with \\(b&#39;=0\\) and wins the challenge. Case \\(b=1\\) is similar: the challenger computes the encryption of \\(m_1 = IV\\oplus IV_1\\) using \\(IV\\) as the initialization vector. It therefore produces the ciphertext \\[ (IV, c) = (IV, F_k((IV\\oplus IV_1)\\oplus IV)) = (IV, F_k(IV_1)) \\] and the adversary seeing \\(c=F_k(IV_1)\\) replies with \\(b&#39;=1\\) and wins the challenge. In both cases \\(\\mathcal{A}\\) successfully guesses \\(b\\) and therefore wins the IND-CPA experiment with probability 1. Exercise 4.6 Show that if an encryption scheme is malleable, it is not CCA secure. Solution. Recall that malleability means that a change in the ciphertext causes a predictable change on the underlying plaintext. We first present the high level idea and then write it a bit more formally. In the IND-CCA experiment, the adversary is not allowed to query the challenge ciphertext \\(c\\). However, it can slightly modify the ciphertext \\(c\\) to some \\(c&#39;\\) and then ask its decryption \\(m&#39;\\). Due to malleability, the effect on the plaintext is predictable. Now there are two cases: either applying the change to \\(m_1\\) gives \\(m&#39;\\), or applying the change to \\(m_2\\) gives \\(m&#39;\\). Seeing which is the case reveals which ciphertext was encrypted and \\(\\mathcal{A}\\) wins with probability \\(1\\)! We will first define malleability more formally with the help of a pair of non-trivial functions \\(f:\\mathcal{M}\\rightarrow\\mathcal{M}\\) and \\(g:\\mathcal{C}\\rightarrow\\mathcal{C}\\). An encryption scheme is malleable if for any key \\(k\\) and message \\(m\\) s.t. \\(c=\\mathsf{Enc}_k(m)\\) it holds that: \\[ \\mathsf{Dec}_k(g(c)) = f(m) \\] Now assume some encryption scheme \\(\\mathcal{E}\\) is malleable w.r.t. to functions \\(f\\), \\(g\\) as defined above. We construct an adversary \\(\\mathcal{A}\\) that wins the IND-CCA experiment. The adversary \\(\\mathcal{A}\\) works as follows: In makes no CPA queries. It samples two messages \\(m_0\\neq m_1 \\in \\mathcal{M}\\). It presents \\(m_0, m_1\\) as its challenge. The challenger flips a coin and encrypts \\(m_b\\). It sends \\(c\\) to \\(\\mathcal{A}\\). The adversary computes \\(c&#39; = g(c)\\). It then makes a single CCA query. It asks for a decryption of \\(c&#39;\\). The challenger replies with the decryption \\(m&#39;\\). If \\(m&#39; = f(m_0)\\) the adversary replies the challenge with \\(b&#39;=0\\) and if \\(m&#39; = f(m_1)\\) the adversary replies the challenge with \\(b&#39;=1\\) and Now we argue that \\(\\mathcal{A}\\) always wins. Let \\(m\\) be the decryption of \\(c\\). Due to the malleability of \\(\\mathcal{E}\\), when decrypting \\(c&#39;=g(c)\\) we are guaranteed to take as a response \\(m&#39;=f(m)\\). and since \\(m=m_0\\) or \\(m=m_1\\) it should be the case that \\(m&#39;=f(m_b)\\). Since \\(\\mathcal{A}\\) checks for which \\(b\\) this relation holds and give this \\(b\\) as its answer. This is not entirely accurate. Often, we rely on computational assumptions to prove constructions secure. For example, we could get the guarantee that “if factoring is computationally hard, then encryption scheme X is secure”. Of course if our assumption is proven wrong (i.e. factoring is an easy problem), we get no guarantee about the security of scheme X.↩︎ We will in fact see that this algorithm must be probabilistic to satisfy minimal security requirements.↩︎ Stronger versions are also considered where the adversary can ask for encryptions of messages after sending \\(m_0, m_1\\).↩︎ We will see later in this course that this is not always the case. A family of encryption schemes allows to encrypt messages without knowing any secret information. The secret key is only used for decryption. As we will see later, this is not a disadvantage, it is in fact in many cases a desirable property with numerous practical and fascinating applications.↩︎ If this was not the case there would be ambiguity: a message ending with 01 could either be interpreted as a message with a single padding byte or with no padding bytes.↩︎ For reference, in the case of AES where \\(\\ell=16\\), the attack would require ~4096 steps while a bruteforce attack would require ~340282366920938463463374607431768211456 steps!↩︎ "],["hash-functions.html", "5 Hash functions 5.1 Some issues in cryptocurrencies 5.2 Hash functions 5.3 Birthday attacks 5.4 The Merkle-Damgård transformation Solved exercises", " 5 Hash functions In this section, we take a detour from encryption to look at other cryptographic primitives. You might have encountered hash functions before, in a different field. However, we will see that hash functions in cryptography require some special properties. We will: Briefly discuss some issues in cryptocurrencies, and how they can be solved with hash functions. Define hash functions and their main properties. Learn about the birthday paradox attack on hash functions. Learn how to extend the domain of a hash function through the Merkle-Damgärd transformation. 5.1 Some issues in cryptocurrencies Traditional currency is centralized, which means that there is an authority that dictates money policy, establishes ownership, and manages the whole system. On the other hand, in recent decades there has been a substantial effort in using cryptographic tools to build what we know as cryptocurrencies, which aim to be completely decentralized. In this section, we discuss some issues that arise in decentralized systems. This is a very high level overview, based on the Bitcoin20 approach, and omits many technicalities for the sake of the exposition. Nevertheless, it will be enough to motivate the use of hash functions. A coin, the monetary unit of a cryptocurrency, is nothing more than a unique bitstring \\(ID\\) that identifies is, and is accordingly called its identifier. An immediate problem arises regarding transferring ownership of a coin. Problem 1 (Double-spending). Suppose that \\(A\\) buys something from \\(B\\) on the internet and pays with a coin \\(ID\\). What prevents \\(A\\) from using the same coin \\(ID\\) to buy something else from a different party \\(C\\)? On very general terms, the solution is to publish every transaction that happens, so that the journey of each coin can be traced and thus its ownership can be established. In the problem above this means that, when \\(A\\) buys from \\(B\\), the message “\\(A\\) transfers the coin \\(ID\\) to \\(B\\)” is added to the public ledger. Then, after the ledger awards \\(B\\) ownership of the coin, they can send whatever \\(A\\) bought. Moreover, if \\(A\\) tries to spend the same coin again, \\(C\\) will notice in the ledger that the coin no longer belongs to \\(A\\), and the transaction will be denied. So, ignoring the logistics of checking and storing and increasingly large ledger, we would have solved the issue. But we still have to deal with the following problem. Problem 2. Who keeps track of this public ledger? Who adds the new transactions? If there is no central authority, how do users agree on which transactions happened? More concretely, imagine that there is a ledger of transactions \\[t_1,t_2,\\dots,t_n,\\] and two different options \\(t_{n+1},t_{n+1}&#39;\\) are claimed to be the next transaction by different parties: This situation is called a fork. The system is designed in such a way that users are encouraged to keep a consensus on a ledger of valid transactions. The general idea is that you have more of a say if you have more computational power, or more precisely if you have spent more CPU cycles in adding transactions to the ledger. So we need a way to “prove” that you have spent these cycles. Let \\[H:\\{0,1\\}^k\\rightarrow\\{0,1\\}^\\ell,\\] for some \\(k,\\ell\\in\\mathbb{N}\\), where in general \\(k\\) is much larger than \\(\\ell\\), be an efficiently computable function. Suppose that we are interested in the problem of finding \\(\\mathbf x\\) such that \\(H(\\mathbf x)=\\mathbf 0\\), where \\(\\mathbf 0\\) is the string of zeros of length \\(\\ell\\). If we know nothing about \\(H\\), the best we can do is try random inputs until we find a good one. On average, it would require \\(2^\\ell\\) attempts to find such \\(\\mathbf x\\). That is, whoever shows a solution \\(\\mathbf x\\) has “proven” that he spent \\(O(2^\\ell)\\) evaluations of \\(H\\) in solving the problem. This concept is known as a proof of work. But back to transactions and the ledger: assume that the transaction \\(t_{n+1}\\), involving coin \\(ID\\), is to be added to the ledger. Then, our proof of work consists of producing \\(\\mathbf x\\) such that the first \\(T\\) bits of \\(H(ID|\\mathbf x)\\) are \\(0\\), for some \\(T\\). Once you have the solution, you can add the transaction, including \\(ID\\) and \\(\\mathbf x\\), to the ledger. Note that solving the problem takes time \\(O(2^T)\\), whereas checking a solution is efficient, as it amounts to evaluating the function \\(H\\) just once with the transaction as input. But isn’t this a lot of trouble to get someone’s transaction up in the ledger? The solution here is extremely simple: motivate the users of the network by awarding them newly mint coins when they successfully add a new transaction to the ledger. These users are the so-called miners, and the act of mining a cryptocurrency is just finding the right preimages of the function \\(H\\). Thus, each addition to the ledger has two outcomes: transferring ownership of existing money and minting new money. And here’s the catch: as a miner, your new money is just valid a hundred transactions after your contribution to the ledger. Give a fork, honest users are encouraged to look at the longest ledger and ignore the rest. This encourages miners to work on the single longest ledger too, because the alternatives will be rejected by the users and thus the transactions in them virtually never happened. Therefore, miners might spend CPU cycles for nothing if they decide to work on shorter ledgers of a fork. Another issue is that we want each new transaction to be “bound” to the previous ones. If the new transaction did not depend on the previous, nothing would prevent a malicious user from double-spending. This is also achieved through the function \\(H\\). Given previous transactions \\(t_1,\\dots,t_n\\), the new transaction \\(t_{n+1}\\) will include its own transaction identifier \\(H(t_1,\\dots,t_n)\\) besides \\(ID\\) and \\(\\mathbf x\\). However, the ledger gets larger and larger, and we want the identifier to be small to keep things efficient. So \\(H\\) is mapping a very large set into a smaller one. The upshot is that there is no way for transaction identifiers to be unique. Problem 3. What if there are two sets of transactions \\(t_1,\\dots,t_n\\) and \\(t_1&#39;,\\dots,t_n&#39;\\) with the same identifier \\(H(t_1,\\dots,t_n)=H(t_1&#39;,\\dots,t_n&#39;)\\)? Fortunately, although it is clear that there is no possible function \\(H\\) such that the outputs are unique, it will be enough if it is hard to find a pair of inputs with the same output, so that this issue with the identifiers cannot be exploited in practice. 5.2 Hash functions The central piece of this whole apparatus seems to be the function \\(H\\), which we have not looked into yet. Clearly we will require some unconventional properties from this function. Let us summarize what we discussed about it: The input is larger than the output, possibly by much. Given \\(\\mathbf y\\), it should be hard to find \\(\\mathbf x\\) such that \\(H(\\mathbf x)=\\mathbf y\\). It is hard to find \\(\\mathbf x,\\mathbf x&#39;\\) such that \\(\\mathbf x\\neq \\mathbf x&#39;\\) and \\(H(\\mathbf x)=H(\\mathbf x&#39;)\\). With this intuition in mind, let us introduce the solution to all of our problems: hash functions. At their core, hash functions are nothing more than functions that take an arbitrarily-long bitstring and output a bitstring of fixed length. Definition 5.1 A hash function is an efficiently computable21 function \\[H:\\{0,1\\}^*\\rightarrow\\{0,1\\}^\\ell,\\] for some \\(\\ell\\in\\mathbb{N}\\), and where \\(\\{0,1\\}^*\\) denotes the set of all bitstrings of any length. The process of computing a hash function is often called hashing, and the output is referred to as the hash. Note that, unlike encryption, hash functions do not use any secret key. From a functionality point of view, this is all we need: a function that compresses bitstrings and is efficient enough to compute. However, to ensure the security of the cryptocurrency model described above, we will need an extra property. We observe that hash functions must be public and deterministic, because different parties need to be able to arrive to the same result to verify a transaction. The hash function is taking arbitrarily-large messages and producing fixed-length ones. That it, it is mapping a larger set into a smaller set. Therefore, there must be different strings that produce the same hash. Given a bitstring \\(\\mathbf b\\), there might exist \\(\\mathbf b&#39;\\neq\\mathbf b\\) such that \\[H(\\mathbf b)=H(\\mathbf b&#39;).\\] Nevertheless, we want this pair of bitstrings to be hard to find. This, and the observations at the beginning of Section 4.2, motivate the following set of definitions. Definition 5.2 Let \\(H\\) be a hash function. We say that \\(H\\) is: collision-resistant if it is hard to find two bitstrings \\(\\mathbf b, \\mathbf b&#39;\\) such that \\(\\mathbf b\\neq \\mathbf b&#39;\\) and \\(H(\\mathbf b)= H(\\mathbf b&#39;)\\). In this case, the pair \\((\\mathbf b, \\mathbf b&#39;)\\) is called a collision of \\(H\\). second preimage-resistant if, given \\(\\mathbf b\\), it is hard to find \\(\\mathbf b&#39;\\neq \\mathbf b\\) such that they form a collision. preimage-resistant if, given \\(h\\) sampled uniformly at random, it is hard to find a bitstring \\(\\mathbf b\\) such that \\(H(\\mathbf b)=h\\). These properties are related by the following result. Proposition 5.1 Let \\(H\\) be a hash function \\[H:\\{0,1\\}^*\\rightarrow\\{0,1\\}^\\ell.\\] If \\(H\\) is collision-resistant, then it is second preimage-resistant. Exercise 5.1 Try to prove the proposition above by proving the contrapositive: assume that you can break second preimage resistance, and show how to use that to break collision resistance. Informally, second-preimage resistance implies preimage resistance for any hash function that performs some “meaningful” compression of the input. This means that, for any hash function used in practice, if it is second-preimage resistant then it its preimage resistant, although the statement cannot be formally proven, due to some pathological counterexamples. 5.3 Birthday attacks Assume that we are an adversary trying to attack a hash function \\[H:\\{0,1\\}^*\\rightarrow \\{0,1\\}^\\ell,\\] that is, we are trying to find a collision. The straightforward approach is the following: we choose random strings and compute their hashes, until two strings return the same hash. In the worst case, this requires \\(2^\\ell+1\\) tries, since there are at most \\(2^\\ell\\) different outputs. Therefore, it looks like the brute-force attack takes time \\(O(2^\\ell)\\) to succeed. This would suggest that a hash function with output length \\(\\ell\\) gives us a security level of \\(\\ell\\). In this section, we look into a generic attack that works for any hash function, which is based on the well-known birthday paradox from probability theory, and greatly improves over the above estimation. Consider the following problem. Problem 4. There is a room with \\(40\\) independent students. How likely is that any two of them share the same birthday? On first sight, one might think that this probability is quite low. After all, there are \\(356\\) days in the year, and only \\(40\\) students. Let us compute the actual probability, by solving a related problem: what is the probability of none of the \\(40\\) students sharing their birthday? We start by numbering the students from \\(1\\) to \\(40\\), according to any criterion. To be able to reason more formally about the problem, we introduce the function \\[\\mathsf{bd}:\\{1,\\dots,40\\}\\rightarrow\\{1,\\dots,365\\},\\] which associates to each student its birthday. Then, the probability of student \\(\\#2\\) not sharing a birthday with student \\(\\#1\\) is \\[\\Pr[\\mathsf{bd}(1),\\mathsf{bd}(2)\\text{ are different}]=\\frac{364}{365},\\] since there are \\(364\\) days of the year that are not the birthday of student \\(\\#1\\). Let’s introduce student \\(\\#3\\) into the picture, and let us consider the events: \\(\\mathsf{A}\\): \\(\\mathsf{bd}(3)\\) is different from \\(\\mathsf{bd}(1)\\) and \\(\\mathsf{bd}(2)\\). \\(\\mathsf{B}: \\mathsf{bd}(1),\\mathsf{bd}(2)\\) are different. Clearly, the intersection event is \\(\\mathsf{A}\\cap\\mathsf{B}: \\mathsf{bd}(1),\\mathsf{bd}(2),\\mathsf{bd}(3)\\) are pairwise different. Then, using conditional probabilities, we have that \\[\\Pr[\\mathsf{A}\\cap\\mathsf{B}]=\\Pr[\\mathsf{B}]\\cdot\\Pr[\\mathsf{A}|\\mathsf{B}]\\] We already know \\(\\Pr[\\mathsf{B}]\\), so we are just missing the second term. If the birthdays of students \\(\\#1\\) and \\(\\#2\\) are different, then the probability of \\(\\#3\\) having a different birthday from them is \\[\\Pr[\\mathsf{A}|\\mathsf{B}]=\\frac{363}{365},\\] since there are \\(363\\) days that are neither the birthday of \\(\\#1\\) or \\(\\#2\\). Thus, the probability of the three students having different birthdays is \\[\\Pr[\\mathsf{A}\\cap\\mathsf{B}]=\\frac{364}{365}\\cdot\\frac{363}{365}.\\] By iterating this process for each student, we arrive at the conclusion that the probabilities of all \\(40\\) students having different birthdays is \\[\\frac{364}{365}\\cdot\\frac{363}{365}\\cdot \\dots \\cdot\\frac{326}{365} \\approx 0.108768.\\] In conclusion, the probability of two students sharing a birthday is approximately \\[1-0.108768= 0.891232.\\] This is actually a pretty high probability. This discrepancy between what one might naively expect and what actually happens is known as the birthday paradox. Below, you can find the solutions to Problem 4 for different numbers of students (rounded to six decimal positions). Students Probability \\(10\\) \\(0.116948\\) \\(20\\) \\(0.411438\\) \\(40\\) \\(0.891232\\) \\(80\\) \\(0.999914\\) \\(128\\) \\(0.999999\\) So what does any of this have to do with breaking a hash function? What we have just done is computing the probability of finding two students such that the birthday function returns the same value on them. That is, we have found a collision of the birthday function! In doing so, we have assumed that the output of the birthday function behaves as the uniform distribution on \\(\\{1,\\dots,365\\}\\). But, isn’t that exactly the effect that we want from a good hash function? That outputs look random and unrelated? So the moral of the story is that finding collisions in a hash function is actually much more likely that expected. More precisely, it can be proven with some careful probabilities analysis that, for any hash function \\(H\\) which outputs bitstrings of length \\(\\ell\\), there is a decent probability of finding a collision after \\(\\sqrt{2^\\ell}\\) evaluations. Compare this with our initial estimation. At the beginning of the section, we bounded a brute force attack by \\(O(2^\\ell)\\). However, we now see that an attacker has a good probability of finding a collision in time \\(O(2^{\\frac{\\ell}2})\\). Thus, we conclude that a hash function with output length \\(\\ell\\) gives us \\(\\ell/2\\) bits of security. Or the other way around, if we want \\(\\ell\\) bits of security, we need our hash function to have output length \\(2\\ell\\). 5.4 The Merkle-Damgård transformation As was the case for encryption, we often build hash functions in two steps. First, we build a hash function for fixed-length inputs, e.g. \\[H:\\{0,1\\}^{2\\ell}\\rightarrow\\{0,1\\}^\\ell,\\] and then we extend them to arbitrarily-large input. We will not get into the details of concrete constructions, but will simply mention the SHA family of hash functions, which is the standard used in practice most of the time.22 A common way to realize this second step is to use the Merkle-Damgård transformation,23 which describes how to build from \\(H\\) another hash function \\(\\mathbf{H}\\) that takes as input any string of length at most \\(2^\\ell-1\\), and outputs a hash of length \\(\\ell\\). It is clear that repeated applications of the transformation can make the input go as large as we want. Similar to modes of operations in block ciphers, the Merkle-Damgård transformation starts by splitting the string \\(\\mathbf x\\) of length \\(L\\leq 2^\\ell\\) to be hashed into blocks \\[\\mathbf x_1, \\dots, \\mathbf x_n,\\] each of them of length \\(\\ell\\).24 An additional block \\(\\mathbf x_{n+1}\\) is added, containing a binary encoding of \\(L\\). Note that, because \\(L\\leq 2^n-1\\), we can fit the encoding of \\(L\\) in \\(n\\) bits. Then, we recursively compute \\[\\mathbf z_i=H(\\mathbf z_{i-1}|\\mathbf x_i),\\] for \\(i=1,\\dots,n+1\\), and where \\((\\mathbf z_{i-1}|\\mathbf x_i)\\) means the concatenation of the bitstrings \\(\\mathbf z_{i-1}\\) and \\(\\mathbf x_i\\). Then, the hash of \\(\\mathbf x\\) is \\[\\mathbf{H}(\\mathbf x)=\\mathbf z_{n+1}.\\] As in modes of operation, there is no “previous block” in the first iteration, and so again we introduce an initialization vector \\(\\mathbf z_0\\), which can be set to the string of \\(0\\)’s of length \\(n\\), or any other bitstring. There is no need for the IV to be secret. Proposition 5.2 If \\(H\\) is a collision-resistant hash function, then \\(\\mathbf{H}\\), produced with the Merkle-Damgård transformation, as described above, is also collision-resistant. Solved exercises Exercise 5.2 Consider function \\(f: S\\times S \\rightarrow S\\) that is symmetric, that is, for all \\(a,b\\in S\\): \\(f(a, b) = f(b, a)\\). Can this be a collision-resistant hash function from \\(S\\times S\\) to \\(S\\)? Solution. No, because we the symmetric property implies collisions. Indeed, take any \\(a, b\\in S\\) with \\(a\\neq b\\) and let \\(c = f(a,b)\\). Now, \\(f(a,b) = c = f(b,a)\\). Note that \\((a,b)\\neq (b,a)\\) since \\(a\\neq b\\), but \\(H(a,b)=H(b,a)=c\\). Therefore, we have a collision. Exercise 5.3 Consider the following hash function \\(f_{42}: \\mathbb{Z} \\rightarrow \\{0,\\ldots, 41\\}\\) that maps \\(n \\mapsto n \\mod 42\\) where \\(a \\mod b\\) denotes the remainder of the division \\(a/b\\). Is \\(f_{42}\\) a hash function? Is it collision resistance? Solution. A hash function must be (1) efficiently computable and (2) shrinking. Since integer division is efficiently computable and it maps integers numbers to a fixed-size set it is indeed a hash function. It is not collision resistance. Take any pair \\(n, n+42\\). We claim that these values will have the same hash. Let \\(f_{42}(n) = n\\mod 42 = r\\). This means that \\(n = q\\cdot 42 + r\\) for some \\(r&lt;42\\). We can also write \\[ n+42 = q\\cdot 42 + r+42 = (q+1)\\cdot 42 + r, \\quad r&lt;42 \\] which means that \\(f_{42}(n+42) = (n+42)\\mod 42 = r\\) and therefore \\((n, n+42, r)\\) is a collision for every \\(n\\in\\mathbb{Z}\\). Exercise 5.4 Consider a simplified version of the Merkle-Damgård transform, where only two inputs are compressed (instead of arbitrary). Prove that if the underlying hash function \\(H:\\{0,1\\}^{2\\ell} \\rightarrow \\{0,1\\}^{\\ell}\\) is collision resistance, then the transformed function \\(\\mathbf H:\\{0,1\\}^{4\\ell} \\rightarrow \\{0,1\\}^{\\ell}\\) is collision resistant as well. Solution. We know that \\(H\\) is collision-resistant. We need to show that the transformed function \\(\\mathbf{H}\\) is also collision-resistant. Our strategy is the following: we will assume that \\(\\mathbf{H}\\) is not collision-resistant and use this fact to find a collision for \\(H\\). If we manage to do this, we end up in a contradiction: while \\(H\\) is collision resistant we found a collision! It must be the case that our assumption “\\(\\mathbf{H}\\) is not collision-resistant” is false and therefore the proposition holds. We first present a diagram for the simplified transformation: We assume \\(\\mathbf{H}\\) is not collision-resistant. Therefore, there exists an efficient algorithm \\(\\mathcal{A}\\) that outputs a collision. We will use this algorithm to construct a collision for \\(H\\). Running \\(\\mathcal{A}\\) will output a collision, that is, it will output \\((\\mathbf z_0, \\mathbf x_1, \\mathbf x_2)\\neq (\\mathbf z_0&#39;, \\mathbf x_1&#39;, \\mathbf x_2&#39;)\\) such that \\[ \\mathbf{H}(\\mathbf x_1, \\mathbf x_2; \\mathbf z_0) = \\mathbf y = \\mathbf{H}(\\mathbf x&#39;_1, \\mathbf x&#39;_2; \\mathbf z_0&#39;) \\] Note that the algorithm should also give the randomness used, here the initial IV. Also note that either \\(\\mathbf x_1\\neq \\mathbf x_1&#39;\\) or \\(\\mathbf x_2\\neq \\mathbf x_2&#39;\\). We will now consider the middle values \\(\\mathbf z_1, \\mathbf z_1&#39;\\). We will consider two cases: Case \\(\\mathbf z_1=\\mathbf z_1&#39;\\): This means that \\(H(\\mathbf z_0, \\mathbf x_1) = H(\\mathbf z_0&#39;, \\mathbf x_1&#39;)\\). Now, it should be the case that \\(\\mathbf z_0=\\mathbf z_0&#39;\\) and \\(\\mathbf x_1=\\mathbf x_1&#39;\\). Indeed, if any of pair is different, then \\((\\mathbf{z}_0,\\mathbf{x}_1)\\neq(\\mathbf{z}_0&#39;,\\mathbf{x}_1&#39;), \\mathbf z_1\\) is a collision for \\(H\\)! Therefore, it should be the case that \\(\\mathbf z_0 = \\mathbf z_0&#39;\\), \\(\\mathbf x_1 = \\mathbf x_1&#39;\\) and since the original output of \\(\\mathcal{A}\\) is a collision \\(\\mathbf{x}_2 \\neq \\mathbf x_2&#39;\\). But then we have that \\[ H(\\mathbf z_1, \\mathbf x_2) = \\mathbf y = H(\\mathbf z_1&#39;, \\mathbf x_2&#39;) \\] for a \\(\\mathbf x_2\\neq \\mathbf x_2&#39;\\). Therefore, \\((\\mathbf z_1, \\mathbf x_2)\\neq (\\mathbf z_1&#39;, \\mathbf x_2&#39;), \\mathbf y\\) is a collision for \\(H\\). Case \\(\\mathbf z_1\\neq\\mathbf z_1&#39;\\): In this case, we have \\[ H(\\mathbf z_1, \\mathbf x_2) = \\mathbf y = H(\\mathbf z_1&#39;, \\mathbf x_2&#39;) \\] for a \\(\\mathbf z_1\\neq \\mathbf z_1&#39;\\) (regardless of the values of \\(\\mathbf{x}_2,\\mathbf{x}_2&#39;\\)). Therefore, \\((\\mathbf z_1, \\mathbf x_2)\\neq (\\mathbf z_1&#39;, \\mathbf x_2&#39;), \\mathbf y\\) is a collision for \\(H\\). Exercise 5.5 (*) In this exercise we will demonstrate how to use Hash functions to build a cryptographic primitive that is called Vector Commitment Scheme25. A VC scheme allows Alice to commit to a vector of elements \\(\\mathbf v = (\\mathbf v_1, \\ldots, \\mathbf v_n)\\in S^n\\). Let \\(c\\) be the commitment to the vector. Bob, having only the commitment, wants to learn the \\(i\\)-th element of the committed vector. Alice responds with \\(\\mathbf v_i\\) and she also gives a proof \\(\\pi_i\\) that the \\(i\\)-th element is indeed \\(\\mathbf v_i\\). Bob can now verify the proof and convince itself this is indeed the \\(i\\)-th value. We need two properties: Efficiency: the commitment should be small, specifically it should be independent of the vector dimension \\(n\\). Binding: After committing to \\(c\\), Alice cannot give two valid proofs that the \\(i\\)-th element is both \\(\\mathbf v_i\\neq \\mathbf v_i&#39;\\). You can think of a VC scheme as a short digest of a big database, whose elements can be efficiently and verifiably queried. Let \\(H: \\{0,1\\}^{2\\ell} \\rightarrow\\{0,1\\}^{\\ell}\\) be a collision resistant hash function. Build a vector commitment scheme with \\(S=\\{0,1\\}^{\\ell}\\) using the Merkle-Damgård transformation. What is the bottleneck for efficiency in the above construction? Describe an alternative way of building a VC scheme using \\(H\\). Hint: consider using binary trees. Argue informally that the above construction satisfies the vector commitment properties described above. Solution. Let \\(n\\) be the dimension of the committed vectors. We need to describe how to commit to \\(c\\), how the proof looks like and how to verify the proof. To commit to a vector \\(\\mathbf v\\in(\\{0,1\\}^{\\ell})^n\\) we simply apply the Merkle-Damgård transformation, that is, we compute the commitment as \\(c=\\mathbf{H}(\\mathbf v)\\). The proof for the \\(i\\)-th element consists of the value \\(\\mathbf{z}_0\\) and all but the \\(i\\)-th element of \\(\\mathbf v\\), that is, \\[ \\pi_i = (\\mathbf{z}_0, \\mathbf{v}_1,\\ldots,\\mathbf{v}_{i-1},\\mathbf{v}_{i-1},\\ldots, \\mathbf{v}_n) \\] To verify the proof, Bob recomputes the hash with the given values and \\(\\mathbf v_i\\), that is, it computes \\(c&#39; = \\mathbf{H}(\\mathbf{v}_1,\\ldots,\\mathbf{v}_n;\\mathbf{z}_0)\\) and accepts iff \\(c = c&#39;\\). Let’s now argue that this satisfies the given properties. First, efficiency is satisfied since \\(c\\in\\{0,1\\}^\\ell\\) which is indeed independent of \\(n\\). As for the binding property, assume Alice can convince that a commitment \\(c\\) open’s at position \\(i\\) in two different values \\(\\mathbf v_i\\neq \\mathbf v_i&#39;\\). This means that she must also create two valid profs \\[ \\pi_i = (\\mathbf{z}_0, \\mathbf{v}_1,\\ldots,\\mathbf{v}_{i-1},\\mathbf{v}_{i-1},\\ldots, \\mathbf{v}_n) \\] \\[ \\pi_i&#39; = (\\mathbf{z}_0&#39;, \\mathbf{v}_1&#39;,\\ldots,\\mathbf{v}_{i-1}&#39;,\\mathbf{v}_{i-1}&#39;,\\ldots, \\mathbf{v}_n&#39;) \\] Since both proofs are valid, it must be the case that \\[ \\mathbf{H}(\\mathbf v;\\mathbf{z}_0) = c = \\mathbf{H}(\\mathbf v&#39;;\\mathbf{z}_0&#39;) \\] But \\(\\mathbf v\\neq \\mathbf v&#39;\\) are different at least in one position, namely position \\(i\\), so \\((\\mathbf{z}_0, \\mathbf v) \\neq (\\mathbf{z}_0&#39;, \\mathbf v&#39;), c\\) is a collision for \\(\\mathbf H\\)! Since the transformation is collision-resistant, this is a contradiction and our VC scheme should be indeed binding. The bottleneck is clearly the size of the proof and the verification time. Indeed, the proof is essentially the whole committed vector and verification consists of recomputing the commitment! Now imagine you have a commitment to a database of multiple GB. This could clearly be a problem. As an efficiency requirement, we should add that the proof size is also small. How small? We want it at least to be sublinear in \\(n\\). This would guarantee that we don’t receive something that is roughly as big as the vector itself. We would also want the verification of the proof to happen in time sublinear in \\(n\\). It is a natural property that the verifier is efficient (think of Alice a big cloud company and Bob being a user only holding an old smartphone). The first observation is that if we need to verify \\(\\mathbf v_i\\), we don’t need to know \\(\\mathbf z_0, \\mathbf v_1, \\ldots, \\mathbf v_{i-1}\\). We could start from \\(\\mathbf z_{i-1}\\) and recompute the hash from this point. However, this does not meet our criteria. The proof is still linear in \\(n\\) in the worst case (consider for example \\(i=1\\))! We need something better. Consider the following approach: Instead of computing a big hash \\(c = \\mathbf H(\\mathbf v_1, \\ldots, \\mathbf v_n)\\), we compute two smaller hashes \\(c_L = \\mathbf H(\\mathbf v_1,\\ldots, \\mathbf v_{n/2})\\) and \\(c_R = \\mathbf H(\\mathbf v_{n/2+1},\\ldots, \\mathbf v_{n})\\). We can now apply the inner function \\(H\\) to compute the commitment \\(c=H(c_L, c_R)\\). A proof now would be half the size. Indeed, if \\(i\\) is on the first half, we don’t need to send all the elements of the right part but only \\(c_R\\). And similarly if it is on the right! We halved the proof size in half already! And we can use recursion to make it even better, that is, apply the same strategy on the two halfs themselves and so on. This gives a tree-like structure. Assume w.l.o.g. that \\(n\\) is a power of 2. The prover creates a binary tree as follows: Put the values \\(\\mathbf v_1, \\ldots, \\mathbf v_n\\) to the leafs. The value of each inner node is \\(n = H(n_L, n_R)\\) where \\(n_L, n_R\\) are the left and right children of each node. The commitment is the root of the tree. This construction is called a Merkle tree and is a widely used vector commitment scheme. We next present the construction schematically. Note that indeed in each level the inputs of \\(H\\) are elements of \\(\\{0,1\\}^{2\\ell}\\). We next describe the commitment scheme. To commit to a vector \\(\\mathbf v\\), build a Merkle tree as described above. The proof for the \\(i\\)-th element is the neighbouring nodes of the path from the root to the \\(i\\)-th leaf. To verify the proof, recompute the path from scratch until the root value \\(c&#39;\\) and verify that the latter value is equal to the commitment \\(c\\). As an example, we show schematically what values you need to open \\(\\mathbf v_3\\). First, let’s see efficiency. The commitment is here as well independent of \\(n\\) since it is a single digest. How about the proof size? This is basically a number of hashes equal to the height of the binary tree. The height is \\(\\log n\\), so the proof size is \\(\\mathcal{O}(\\ell\\cdot\\log n)\\) bits, which is sublinear in \\(n\\). Similarly, the verifier needs to perform a single hash computation for each level, so it does \\(\\mathcal{O}(\\log n)\\) hash computations, which is also sublinear in \\(n\\). How about binding? Let’s assume you have two accepting openings for \\(\\mathbf v_i\\neq \\mathbf v_i&#39;\\). Consider the first hash application. Denote \\(\\mathbf x, \\mathbf x&#39;\\) the neighboring nodes at the leaf level. Recall that these are given as part of the proof. You should have \\[ c = H(\\mathbf v_i, \\mathbf x), \\qquad c&#39; = H(\\mathbf v&#39;_i, \\mathbf x&#39;) \\] Now you have two cases: if \\(c=c&#39;\\) then you have a collision! Indeed, regardless of what the values \\(\\mathbf x, \\mathbf x&#39;\\) are, you know that \\(\\mathbf v_i\\neq \\mathbf v_i&#39;\\) but you have the same hash. If they are different compute the hash on the next level for both. You can continue on this way until at some level you get the same hash. This will definitely happen! Since both proofs are verifying, the root node is for both \\(c\\) so this is the case in the end. So there must be at least one step where the hash becomes the same along the way. The first time this happens you get a collision! Indeed, you (1) end up with the same hash and (2) you know that the inputs are different. Indeed the hash computation at this level will be of the form \\[ c_N = H(c_L, \\mathbf x) = H(c_L&#39;, \\mathbf x&#39;) \\] where \\(c_L, c_L&#39;\\) are the hashes from the previous level (note that here we assume w.l.o.g. that they are left children). Since this is the first time the hashes are equal, it means that \\(c_L\\neq c_R\\) which gives the collision. https://bitcoin.org/bitcoin.pdf.↩︎ You might wonder what “efficiently computable” means in this case, if the input size could be anything. To be precise, we say that the function is efficiently computable if it can be evaluated in time polynomial in \\(\\ell\\) when the input is of length polynomial in \\(\\ell\\).↩︎ https://en.wikipedia.org/wiki/Secure_Hash_Algorithms.↩︎ You might also see the same concept named the Merkle-Damgård transform, or the Merkle-Damgård construction.↩︎ As before, use some padding if the length of \\(\\mathbf x\\) is not a multiple of \\(\\ell\\).↩︎ We deal with a simplified and informal version of vector commitment schemes in this exercise. The actual definitions of such a scheme can vary and can have additional properties.↩︎ "],["elementary-number-theory.html", "6 Elementary number theory 6.1 Integer arithmetic 6.2 The euclidean algorithm 6.3 Modular arithmetic 6.4 Modular arithmetic, but efficient Solved exercises", " 6 Elementary number theory The second half of the course relies strongly on some ideas from number theory, which is the branch of mathematics that deals with integer numbers and their properties. This section and the next contain mathematical background that we will require to build some asymmetric cryptography. In this section, we will: Review integer and modular arithmetic. Discuss algorithmic aspects of modular arithmetic. Note. In these notes, we use the convention that \\(\\mathbb{N}\\) does not include \\(0\\). We will refer to the set of non-negative integers by \\(\\mathbb{Z}_{\\geq 0}\\). 6.1 Integer arithmetic Consider the set \\(\\mathbb{Z}\\) of integer numbers. A key concept to the whole section is that of divisibility. Definition 6.1 Let \\(a,b\\in\\mathbb{Z}\\). We say that \\(b\\) divides \\(a\\) if there exists \\(m\\in \\mathbb{Z}\\) such that \\[bm=a.\\] We denote that \\(b\\) divides \\(a\\) by \\(b\\mid a\\). In this case, we also say that \\(b\\) is a divisor or factor of \\(a\\), or that \\(a\\) is divisible by \\(b\\), or that \\(a\\) is a multiple of \\(b\\). Note that it is crucial that \\(m\\in\\mathbb{Z}\\) in the definition above. Otherwise, any number \\(b\\) would divide any other number \\(a\\), since \\[b\\frac{a}{b}=a,\\] and then this notion would be pretty meaningless. Divisibility is related to the notion of integer division. Proposition 6.1 (Integer division) Let \\(a,b\\in \\mathbb{Z}\\), with \\(b\\neq0\\). Then, there exists a unique pair \\(q,r\\in\\mathbb{Z}\\) such that \\(0\\leq r&lt;b\\) and \\[a=bq+r.\\] The integer \\(q\\) is called the quotient of the division, and \\(r\\) is called the remainder. In Sage, quotient and remainder can easily be computed with the commands a // b and a % b, (or mod(a,b)) respectively. By looking at the above proposition and the definition of divisibility, it is easy to see that \\(a\\mid b\\) if and only if the remainder of the division of \\(a\\) by \\(b\\) is \\(0\\). Divisibility allows us to identify a special type of integers that are the building blocks of any other integer number. Clearly, any integer \\(n\\in\\mathbb{Z}\\) is always divisible by \\(1,-1,n\\) and \\(-n\\), which are called its trivial divisors. Some numbers have more divisors, and some do not. Definition 6.2 An integer \\(p&gt;1\\) is said to be a prime number if its only divisors are the trivial divisors. A positive integer that is not prime is said to be composite. Exercise 6.1 Decide whether each of these statements is true or false: \\(35\\) is a divisor of \\(7\\). \\(4\\) is a factor of \\(16\\). \\(99\\) is divisible by \\(9\\). The remainder of dividing \\(-21\\) by \\(8\\) is \\(-5\\). \\(19\\) is a prime number. \\(41\\) is a composite number. The following two results show some interesting properties of prime numbers. Informally, the first states that any number can be decomposed into its prime factors, and that this decomposition is essentially unique, and the second states that, asymptotically, the chance of choosing a random number smaller than \\(n\\) and finding a prime is \\(\\log(n)/n\\). Proposition 6.2 (Fundamental theorem of arithmetic) Let \\(n\\in\\mathbb{Z}\\). Then there exist prime numbers \\(p_1,\\dots,p_\\ell\\) and integers \\(e_1,\\dots,e_\\ell\\) such that \\[n=\\pm p_1^{e_1}\\dots p_\\ell^{e_\\ell}.\\] Moreover, this decomposition (or factorization) is unique, up to reordering of the factors. Proposition 6.3 (Prime number theorem) Let \\(n\\in\\mathbb{N}\\), and let us denote by \\(\\pi(n)\\) the number of prime numbers smaller than \\(n\\). Then \\[\\lim_{n\\rightarrow\\infty}\\frac{\\pi(n)}{n/\\log(n)}=1.\\] On a computational level, one might think that the problem of determining whether a number is prime or composite and the problem of finding the factorization of said number are close problems. However, the surprising truth is that the second is believed to be much harder than the first! More precisely, there exist efficient algorithms for determining whether a number is prime, but no efficient factorization algorithm is known for numbers that are a product of two large primes, despite decades of huge efforts in finding one.26 Sage contains implementations of the best algorithms known for each case. Try increasing the size of the numbers, and observe that the first algorithm is still very fast, but factorization becomes much slower. from sage.misc.prandom import randrange # Choose a security parameter, which will determine the size of our numbers. sec_param = 160 ### Primality testing # Pick a random number of bitlength sec_param n = randrange(2^(sec_param-1),2^(sec_param)) # Run a primality test %time print(n in Primes()) # Sage contains the class Primes(). # By checking whether n is in Primes(), # it is actually running a primality test internally. ### Factorization # Pick two primes of bitlength half of sec_param. # This is so that their product has bitlength sec_param. p = random_prime(2^((sec_param/2)-1),2^(sec_param/2)) q = random_prime(2^((sec_param/2)-1),2^(sec_param/2)) # Compute their product n = p*q %time print(factor(n)) 6.2 The euclidean algorithm With Proposition 6.2 in mind, we observe that two integers \\(a,b\\) can have some factors in common in its prime factorization. This gives rise to the following notion. Definition 6.3 Let \\(a,b\\in\\mathbb{Z}\\) different from \\(0\\). The greatest common divisor of \\(a\\) and \\(b\\), denoted by \\[\\gcd(a,b),\\] is the largest positive integer \\(k\\) such that \\(k\\mid a\\) and \\(k\\mid b\\). Two integers \\(a,b\\) are said to be coprime (or relatively prime) when \\[\\gcd(a,b)=1.\\] Think about the relation between the notions of primality and coprimality. In particular, observe that being prime is a property of a single integer, whereas being coprime refers to a pair of integers. Exercise 6.2 Find a pair \\(a,b\\in\\mathbb{Z}\\) for each of the following cells in this table: \\(a,b\\) prime \\(a\\) prime, \\(b\\) composite \\(a,b\\) composite \\(a,b\\) coprime \\(a,b\\) not coprime Computing the greatest common divisor of two integers can be achieved easily using the Euclidean algorithm, which we describe next. Let \\(a,b\\in\\mathbb{Z}\\) different from \\(0\\). The following procedure outputs \\(\\gcd(a,b)\\): Compute the integer division of \\(a\\) by \\(b\\), obtaining \\(q,r\\) such that \\(0\\leq r&lt;b\\) and \\[a=bq+r.\\] If \\(r=0\\), then output \\(b\\). Otherwise, return to the previous step, replacing \\(a\\) by \\(b\\) and \\(b\\) by \\(r\\). We show an example for the numbers \\(375\\) and \\(99\\). We start by performing the integer division of \\(375\\) by \\(99\\), obtaining \\[375=99\\cdot 3+78.\\] Since the remainder is not \\(0\\), we compute the integer division of \\(99\\) by \\(78\\), obtaining \\[99=78\\cdot 1+21.\\] We continue with this process until the remainder is \\(0\\): \\[\\begin{aligned} &amp; 78=21\\cdot 3+15.\\\\ &amp; 21=15\\cdot 1+6. \\\\ &amp; 15=6\\cdot 2+3. \\\\ &amp; 6=3\\cdot 2+0.\\\\ \\end{aligned}\\] Since the remainder is \\(0\\), the Euclidean algorithm outputs \\(\\gcd(375,99)=3\\). The greatest common divisor satisfies the following property. Proposition 6.4 Let \\(a,b\\in\\mathbb{Z}\\) different from \\(0\\). There exist \\(x,y\\in\\mathbb{Z}\\) such that \\[ax+by=\\gcd(a,b).\\] It turns out that we can slightly tweak the Euclidean algorithm to compute the integers \\(x,y\\) in the proposition above. This is called the extended Euclidean algorithm. The key idea is to use the Euclidean algorithm to compute the greatest common divisor, and then “walk back” through the computations. We illustrate it with the example of \\(375\\) and \\(99\\) from above. We got the sequence of remainders \\(78,21,15,6,3,0\\), so the last one before \\(0\\), in this case \\(3\\), was our greatest common divisor. We proceed by arranging the relations between them obtained above to write each in terms of the previous two. We start with \\[3=15-6\\cdot 2.\\] We now write \\(6\\) in terms of the two previous remainders, \\(15\\) and \\(21\\): \\[6=21-15\\cdot 1.\\] Combining these two expressions, we can obtain an expression of \\(3\\) in terms of \\(15\\) and \\(21\\): \\[3=15-(21-15\\cdot 1)\\cdot 2=-21\\cdot 2+ 15\\cdot 3.\\] Iterating this process, we can work our way back through the sequence of remainders, until we arrive at the beginning, that is, an expression depending only on \\(375\\) and \\(99\\): \\[\\begin{aligned} 3 &amp;= -21\\cdot 2+ 15\\cdot 3 = -21\\cdot 2 + (78 - 21\\cdot 3)\\cdot 3 = \\\\ &amp; = 78\\cdot 3 - 21\\cdot 11 = 78\\cdot 3 - (99 - 78)\\cdot 11 = \\\\ &amp; = -99\\cdot 11 + 78\\cdot 14 = -99\\cdot 11 + (375 - 99\\cdot 3)\\cdot 14 = \\\\ &amp; = 375\\cdot 14 + 99\\cdot (-53). \\end{aligned}\\] Thus, we have found that \\[3=375\\cdot 14+ 99\\cdot(-53).\\] From a computational point of view, both versions of the Euclidean algorithm are very efficient, even for large numbers, as you can check with the following Sage code. from sage.misc.prandom import randrange # Choose a security parameter, which will determine the size of our numbers. sec_param = 128 # Choose a pair of integers of bitlength sec_param a = randrange(2^(sec_param-1),2^(sec_param)) b = randrange(2^(sec_param-1),2^(sec_param)) # Euclidean algorithm %time print(gcd(a,b)) # Extended euclidean algorithm. The three outputs correspond # to gcd(a,b), and the two numbers x,y such that gcd(a,b)=ax+by. %time print(xgcd(a,b)) 6.3 Modular arithmetic Modular arithmetic is, informally, clock arithmetic. Let us take the usual analog 12-hour clock, and say it is 11 o’clock now. After three hours, it is 2 o’clock. But wait a minute, shouldn’t it be \\(11+3=14\\)? Furthermore, after a whole day, shouldn’t the clock show \\(11+24=35\\) o’clock? But it is showing \\(11\\) instead! This example shows that the usual integer arithmetic is not useful for modelling the behaviour of a clock. Let us see how we can modify it so that the passing of time makes sense again. We consider the following problem. Let \\(a\\in\\mathbb{N}\\). Assume that the current position of the clock is \\(12\\) o’clock. What is the position of the clock after \\(a\\) hours? The key observation is that full movements around the clock (that is, multiples of 12) do not matter, as they leave the clock in the same position. Recall that the algorithm of integer division tells us how to compute \\(q,r\\in\\mathbb{Z}\\) such that \\[a=12\\cdot q + r.\\] In the context of our problem, notice that \\(q\\) is the number of full circles around the clock that happen in \\(a\\) hours. Then, the only real change of position in the clock is determined by \\(r\\), and \\(q\\) does not matter at all.27 Generalizing this idea leads to the key concept of modular arithmetic, by replacing \\(12\\) by any positive integer. Moreover, observe that there is no need for \\(a\\) to be a positive integer, as a negative value of \\(a\\) can be interpreted as moving counter-clockwise. Definition 6.4 Let \\(a, n\\in \\mathbb{Z}\\), with \\(n&gt;0\\). We define the remainder (or residue) of \\(a\\) modulo \\(n\\) as the remainder of the integer division of \\(a\\) by \\(n\\), and we denote it by \\[a\\bmod{n}.\\] Exercise 6.3 Compute the following values: \\[25\\bmod 8, \\qquad 1337\\bmod 7, \\qquad 7\\bmod 13,\\qquad -13\\bmod 12.\\] Modular arithmetic behaves in a similar way to usual arithmetic, as reflected in the following result: Proposition 6.5 Let \\(a,b,n\\in\\mathbb{Z}\\), with \\(n&gt;0\\). Then: \\[\\begin{array}{rl} (i) &amp; (a\\bmod n) + (b\\bmod n) = (a+b)\\bmod n. \\\\ (ii) &amp; (a\\bmod n) \\cdot (b\\bmod n) = (a\\cdot b)\\bmod n. \\\\ (iii) &amp; \\text{If }b\\geq0,\\text{ then }(a\\bmod n)^b=(a^b)\\bmod{n}.\\\\ \\end{array}\\] It is clear that, when working modulo \\(n\\), for some positive integer \\(n\\), the only numbers that matter are \\(0,1,\\dots, n-1\\), since every other number can be identified with one of these by reducing modulo \\(n\\). For example, back to the clock example, we have that \\(13\\bmod12=1\\), \\(25\\bmod{12}=1\\), and so on. It is not chance that related numbers are precisely those that represent the same position! The above example seems to suggest that, modulo \\(12\\), the numbers \\[\\phantom{a}\\dotsc-23,-11,1,13,25\\dotsc\\] are “the same”. This motivates the introduction of the idea of congruence. Definition 6.5 Let \\(a,b,n\\in\\mathbb{Z}\\), with \\(n&gt;0\\). We say that \\(a\\) and \\(b\\) are congruent if \\[(a-b)\\bmod{n}=0.\\] In this case, we represent this fact by \\[a\\equiv b\\pmod{n}.\\] We define the set of residue classes modulo \\(n\\) as the set \\[\\mathbb{Z}_n=\\{0,1,\\dotsc,n-1\\},\\] where the operations of addition and multiplication are reduced modulo \\(n\\), according to Proposition 6.5. Then, the above discussion can be rephrased as follows. We can say that any \\(a\\in \\mathbb{Z}\\) is congruent to its residue modulo \\(n\\), that is, \\(a\\bmod n\\). Thus, to work with arithmetic modulo \\(n\\), we can restrict ourselves to the set \\(\\mathbb{Z}_n\\). Proposition 6.6 Let \\(a,\\alpha,b,n\\in\\mathbb{Z}\\), with \\(n&gt;0\\). If \\(a\\equiv \\alpha\\pmod{n}\\), then: \\[\\begin{array}{rl} (i) &amp; a+b\\equiv \\alpha+b\\pmod{n}. \\\\ (ii) &amp; a\\cdot b\\equiv \\alpha\\cdot b\\pmod{n}. \\\\ (iii) &amp; \\text{If }b\\geq0,\\text{ then }a^b\\equiv \\alpha^b\\pmod{n}.\\\\ \\end{array}\\] In particular, this is true when \\(\\alpha=a\\bmod{n}\\). Exercise 6.4 Prove that the logic \\(\\mathsf{XOR}\\) operation and addition modulo \\(2\\) are the same operation, by checking the four possible cases. We now introduce modular inverses. Let \\(a\\neq0\\) be a number, and consider the meaning of \\(b\\) being the inverse of \\(a\\). Over the real numbers, we say that the inverse of \\(3\\) is \\(1/3\\), because \\[3\\cdot \\frac{1}{3}=1.\\] That is, every nonzero number \\(a\\in\\mathbb{R}\\) has an inverse \\(1/a\\in\\mathbb{R}\\). Consider now the same idea, but over \\(\\mathbb{Z}\\). That is, given \\(a\\in\\mathbb{Z}\\), is there any other integer \\(b\\in\\mathbb{Z}\\) such that \\(ab=1\\)? It is clear that, unless \\(a=\\pm1\\), there is no such thing as an “integer inverse”. However, the answer changes when we consider \\(\\mathbb{Z}_n\\) instead of \\(\\mathbb{Z}\\). Definition 6.6 Let \\(n\\in\\mathbb{N}\\), and \\(a\\in\\mathbb{Z}_n\\). We say that \\(b\\in\\mathbb{Z}_n\\) is the inverse of \\(a\\) modulo \\(n\\)* if \\[ab\\bmod n = 1.\\] If the inverse of \\(a\\) modulo \\(n\\) exists, we say that \\(a\\) is invertible. We also say that \\(a\\) is a unit.* As an example, observe that in \\(\\mathbb{Z}_5\\) we have \\[(2\\cdot 3)\\bmod{5}=6\\bmod{5}=1.\\] Therefore, \\(2\\) and \\(3\\) are inverses modulo \\(5\\). In \\(\\mathbb{Z}_3\\), we have \\[(2\\cdot 2)\\bmod{3}=4\\bmod{3}=1.\\] That is, \\(2\\) is its own inverse modulo \\(3\\). Exercise 6.5 Consider \\(\\mathbb{Z}_4\\). Find which of its elements are invertible, and which are not. When working with modular residues in Sage, one nice thing is that we can specify that we are working modulo some \\(n\\), and Sage will take care of the modular reductions for us, without having to specify every time that we want each operation reduced modulo \\(n\\). In the example below, you can see that, when asked about \\(a+b\\), we get \\(7\\) because the operation is automatically reduced modulo \\(17\\), since we have specified that we want our operations involving \\(a,b\\) to be reduced modulo \\(n\\). Similarly, the inverse of \\(a\\) is automatically computed modulo \\(n\\), otherwise we would obtain \\(1/11\\) instead of \\(14\\). n=17 G=Integers(n) G a = G(11) b = G(13) a,b a+b a^(-1) The above examples and exercise highlight that, given some \\(n\\), some elements of \\(\\mathbb{Z}_n\\) have an inverse, and some do not. This motivates the following definition. Definition 6.7 Let \\(n\\in\\mathbb{N}\\). We denote by \\(\\mathbb{Z}_n^*\\) the subset of \\(\\mathbb{Z}_n\\) formed by all the invertible elements of \\(\\mathbb{Z}_n\\). We define the function \\(\\varphi\\) that, on input \\(n\\), returns the size of \\(\\mathbb{Z}_n^*\\). This function is called Euler’s totient (or phi) function. In Sage, Euler’s function on input \\(n\\) can be computed by euler_phi(n). Proposition 6.7 Let \\(n\\in\\mathbb{N}\\). Then \\(\\mathbb{Z}_n^*\\) is composed of all the elements \\(a\\in\\mathbb{Z}_n\\) such that are coprime to \\(n\\). The value of the totient function, and thus the size of \\(\\mathbb{Z}_n^*\\), is determined by the prime factorization of \\(n\\). More precisely, we have the following result. Proposition 6.8 Let \\(n\\in\\mathbb{N}\\). Then: If \\(n=p^e\\), where \\(p\\) is a prime and \\(e\\in\\mathbb{N}\\), we have \\[\\varphi(n)=(p-1)p^{e-1}.\\] If \\(n=pq\\), where \\(p,q\\) are coprime, then \\[\\varphi(n)=\\varphi(p)\\varphi(q).\\] Exercise 6.6 Use the result above to deduce a formula for \\(\\varphi(n)\\), when \\(n\\) is the product of two different prime numbers. To actually find the inverse of an element \\(a\\) modulo \\(n\\), we can use the extended Euclidean algorithm. The key idea is to run the extended Euclidean algorithm on \\(a\\) and \\(n\\). The algorithm produces \\(x,y\\in\\mathbb{Z}_n\\) such that \\[ax+ny=\\gcd(a,n).\\] By reducing both sides of the equation above modulo \\(n\\), we get that \\[ax\\equiv\\gcd(a,n)\\pmod{n}.\\] Due to Proposition 6.7, we know that \\(a\\) will be invertible modulo \\(n\\) if and only if \\(\\gcd(a,n)=1\\). Thus, if this is the case, we would have \\[ax\\equiv1\\pmod{n},\\] which means that \\(x\\) is the inverse of \\(a\\) modulo \\(n\\). If we find that \\(\\gcd(a,n)\\neq1\\), then \\(a\\) is not invertible modulo \\(n\\). We also introduce the Chinese remainder theorem (CRT), which, in its simplest form, states the following. Proposition 6.9 (Chinese remainder theorem) Let \\(a,b\\in\\mathbb{N}\\) be coprime, and let \\(n=ab\\). Then, for each \\(x\\in\\mathbb{Z}_n\\), there exists a unique pair \\((y,z)\\in\\mathbb{Z}_a\\times\\mathbb{Z}_b\\) such that \\[\\begin{aligned} &amp; x\\equiv y\\pmod{a}, \\\\ &amp; x\\equiv z\\pmod{b}. \\\\ \\end{aligned}\\] Moreover, given \\((y,z)\\in\\mathbb{Z}_a\\times\\mathbb{Z}_b\\), we can explicitly recover the corresponding \\(x\\) by computing \\[x=\\left(by\\left(b^{-1}\\bmod{a}\\right)+az\\left(a^{-1}\\bmod{b}\\right)\\right)\\bmod{n}.\\] With the notations of the proposition above, the Sage command that computes the value \\(x\\) given by the Chinese remainder theorem is crt(y, z, a, b). The legend says that this theorem was used in ancient China to count troops, proceeding as follows. Say that you had \\(200\\) troops before battle, and you want to count your losses. Choose a pair of coprime numbers such that their product is at least the upper bound on the number of troops. In this case, we can use \\(11\\cdot19=209\\). Order the troops to stand in columns of length \\(11\\). The last column (possibly incomplete) tells us the remainder modulo \\(11\\), say it is \\(8\\). Reorganize the troops in columns of length \\(19\\). Again, the last column tells us the remainder \\(z\\) modulo \\(19\\), say it is \\(2\\). Using the Chinese remainder theorem, we know that there is a unique number \\(x&lt;209\\) such that \\[\\begin{aligned} &amp; x\\bmod 11 = 9, \\\\ &amp; x\\bmod 19 = 2, \\\\ \\end{aligned}\\] and the second part of the theorem allows us to explicitly compute this number. Since \\[\\begin{aligned} &amp; 19^{-1}\\bmod 11 = 7, \\\\ &amp; 11^{-1}\\bmod 19 = 7, \\\\ \\end{aligned}\\] we have that \\[x=\\left(11\\cdot 9\\cdot 7+19\\cdot 2\\cdot 7\\right)\\bmod{209}=97.\\] Higher factors, or more than two of them, can be used to deal with larger numbers. 6.4 Modular arithmetic, but efficient There are different approaches to actually do computations modulo \\(n\\). The end result will not change, but some ways involve easier computations than others. As an example, say that we want to compute \\[3^{75}\\bmod{191}.\\] The straightforward approach is to compute \\(3^{75}\\), which is \\[608266787713357709119683992618861307,\\] by performing \\(74\\) multiplications by \\(3\\), and then perform the division by \\(191\\). But clearly this is a lot of work. A better approach is to perform the exponentiation in smaller increments, and reduce modulo \\(191\\) before numbers get too big. More precisely, let us write the base-2 expansion of \\(75\\): \\[\\begin{equation} 75=2^6+2^3+2^1+2^0. \\tag{6.1} \\end{equation}\\] Then, we can rewrite \\[\\begin{equation} 3^{75}=3^{2^6}\\cdot3^{2^3}\\cdot3^{2^1}\\cdot3^{2^0}. \\tag{6.2} \\end{equation}\\] Now observe that, for any \\(i\\in\\mathbb{N}\\), we have that \\[3^{2^i}=\\left(3^{2^{i-1}}\\right)^2,\\] and thus each of these terms can be recursively computed from the previous by squaring. We also perform the reductions modulo \\(191\\) at each step, to prevent the numbers from blowing-up in size. Note that this reduction can be done due to point (iii) in Proposition 6.5. \\[\\begin{aligned} &amp; 3^{2^0}\\equiv 3\\pmod{191}, \\\\ &amp; 3^{2^1}\\equiv \\left(3^{2^0}\\right)^2\\equiv 9\\pmod{191}, \\\\ &amp; 3^{2^2}\\equiv \\left(3^{2^1}\\right)^2\\equiv 81\\pmod{191}, \\\\ &amp; 3^{2^3}\\equiv \\left(3^{2^2}\\right)^2\\equiv 6561\\equiv 67\\pmod{191},\\\\ &amp; 3^{2^4}\\equiv \\left(3^{2^3}\\right)^2\\equiv (67)^2\\equiv 4489\\equiv 96\\pmod{191},\\\\ &amp; 3^{2^5}\\equiv \\left(3^{2^4}\\right)^2\\equiv (96)^2\\equiv 9216 \\equiv 48\\pmod{191},\\\\ &amp; 3^{2^6}\\equiv \\left(3^{2^5}\\right)^2\\equiv (48)^2\\equiv 2304 \\equiv 12\\pmod{191}.\\\\ \\end{aligned}\\] Now it simply remains to multiply the four factors of equation (6.2). Again, to avoid big numbers, we reduce modulo 191 after each factor is multiplied: \\[\\begin{aligned} &amp; 3^{2^0}\\cdot 3^{2^1}\\equiv 9\\cdot 81\\equiv 729 \\equiv 156 \\pmod{191}, \\\\ &amp; \\left(3^{2^0}\\cdot 3^{2^1}\\right)\\cdot 3^{2^3}\\equiv 156\\cdot 67\\equiv 10452 \\equiv 138\\pmod{191},\\\\ &amp; \\left(3^{2^0}\\cdot 3^{2^1}\\cdot 3^{2^3}\\right)\\cdot 3^{2^6}\\equiv 138\\cdot 12\\equiv 1656\\equiv 128.\\\\ \\end{aligned}\\] Therefore, we conclude that \\[3^{75}\\bmod{191}=128\\qquad\\text{(or, equivalently, that $3^{75}\\equiv 128\\pmod{191}$).}\\] This algorithm is known as square-and-multiply, and the reason behind the name is clear once we take a step back and slightly rewrite our solution. Observe that, from equation (6.1), we directly deduce that the binary expression of \\(75\\) is \\[[75]_2=1001010.\\] Then, from the base number, in this case \\(3\\), and for each bit in \\([75]_2\\), starting from the right, we Squaring step: compute the square of the previous power of \\(3\\). Multiplication step: if the bit is \\(1\\), multiply the product so far by the new power of \\(3\\). Otherwise, skip this step. Take a moment to review the example above, and convince yourself that it matches the steps described. Solved exercises Exercise 6.7 Compute by hand the value \\(367^{234} \\pmod 9\\). Solution. Doing such computations without applying the properties of modular arithmetic can be tedious and inefficient. The most crucial point is that we need to keep our numbers small, so always reduce the modulo when you can! We start by noting that \\(367\\) is way too big to handle. However note that we can write the above as \\(367\\cdot 367 \\cdots 367\\pmod 9\\). Using the second property of Proposition 6.5, we can first reduce \\(367 \\pmod 9\\) in the above equation. We get the first trick this way: Trick 1: big numbers should first get reduced! After a division we get that \\(367\\equiv 9\\). Our new task is to compute \\(7^{234} \\pmod 9\\).* Now, we need to do fast exponentiation. Basically, one can simply write the exponent as a sum of powers of 2. It is good to learn by hand the first powers of two \\[ 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, \\ldots \\] Trick 2: Write exponents as powers of two! We can write \\[ 234 = 128 + 64 + 32 + 8 + 2 = 2^7 + 2^6 + 2^5 + 2^3 + 2^1 \\] so we can now rewrite \\[ 7^{234}\\equiv 7^{2^7 + 2^6 + 2^5 + 2^3 + 2^1} \\equiv 7^{2^7} 7^{2^6} 7^{2^5} 7^{2^3} 7^{2^1} \\pmod 9 \\] At this point, if we new the values \\(7^{2^i}\\) we could compute the result. But recall that the smaller the numbers, the easier it is to make computations! This gives the third trick: Trick 3: when working \\(\\bmod n\\) add or subtract \\(n\\) to make numbers easier to handle! In our case, we have \\(7 \\equiv 7 - 9 \\equiv -2 \\pmod 9\\). We now need to do the calculation \\[ 7^{2^7} 7^{2^6} 7^{2^5} 7^{2^3} 7^{2^1} \\equiv {(-2)}^{2^7} {(-2)}^{2^6} {(-2)}^{2^5} {(-2)}^{2^3} {(-2)}^{2^1} \\pmod 9 \\] We start computing them one by one. Remember to always reduce so your numbers remain small and the calculations easy! We have \\[ \\begin{aligned} {(-2)}^{2^1} &amp;\\equiv {(-2)}^{2} \\equiv 4\\pmod 9 \\\\ {(-2)}^{2^2} &amp;\\equiv {(-2)}^{2^1}{(-2)}^{2^1} \\equiv 4 \\cdot 4 \\equiv 16 \\equiv 7 \\equiv -2 \\pmod 9 \\\\ {(-2)}^{2^3} &amp;\\equiv {(-2)}^{2^2}{(-2)}^{2^2} \\equiv (-2) \\cdot (-2)\\equiv 4\\pmod 9 \\\\ {(-2)}^{2^4} &amp;\\equiv {(-2)}^{2^3}{(-2)}^{2^3} \\equiv 4 \\cdot 4 \\equiv 16 \\equiv 7 \\equiv -2 \\pmod 9 \\\\ {(-2)}^{2^5} &amp;\\equiv {(-2)}^{2^4}{(-2)}^{2^4} \\equiv (-2) \\cdot (-2)\\equiv 4\\pmod 9 \\\\ {(-2)}^{2^6} &amp;\\equiv {(-2)}^{2^5}{(-2)}^{2^5} \\equiv 4 \\cdot 4 \\equiv 16 \\equiv 7 \\equiv -2 \\pmod 9 \\\\ {(-2)}^{2^7} &amp;\\equiv {(-2)}^{2^6}{(-2)}^{2^6} \\equiv (-2) \\cdot (-2)\\equiv 4\\pmod 9 \\\\ \\end{aligned} \\] We can now replace and do the final calculations. We have \\[ \\begin{aligned} {(-2)}^{2^7} {(-2)}^{2^6} {(-2)}^{2^5} {(-2)}^{2^3} {(-2)}^{2^1} &amp;\\equiv 4\\cdot (-2) \\cdot 4 \\cdot 4 \\cdot 4 \\pmod 9 \\\\ &amp;\\equiv -8 \\cdot 4 \\cdot 4 \\cdot 4 \\pmod 9 \\\\ &amp;\\equiv 1\\cdot 4 \\cdot 4 \\cdot 4 \\pmod 9 \\\\ &amp;\\equiv 16\\cdot 4 \\pmod 9 \\\\ &amp;\\equiv 7\\cdot 4 \\pmod 9 \\\\ &amp;\\equiv 28 \\pmod 9 \\\\ &amp;\\equiv 1 \\pmod 9 \\\\ \\end{aligned} \\] Exercise 6.8 Compute the following inverses: \\(4^{-1} \\pmod {79}\\) \\(83^{-1} \\pmod {3791}\\) Solution. Recall that the inverse of \\(a \\pmod n\\) exists if and only if the greatest common divisor of \\(a, n\\) is 1. This value can be computed via the Extended Euclidian Algorithm. This algorithm is also the computational method to compute the inverse. Indeed, via this algorithm you find values \\(x,y\\) s.t. \\(ax+ny=1\\). Reducing this equation \\(\\bmod n\\) gives \\[ ax + ny \\pmod n \\equiv 1 \\pmod n \\Leftrightarrow ax \\pmod n \\equiv 1 \\pmod n \\] and therefore the product of \\(a\\) and \\(x\\) (in \\(\\bmod n\\)) is 1. Therefore, \\(a^{-1}\\equiv x\\pmod n\\). We next run the EEA for the given \\(a, n\\). Recall the steps of the algorithm: 1. do integer division of \\(a,n\\) to get the remainder \\(r\\). 2. run the EEA for \\(a, r\\) The recursion stops when \\(r=0\\). Let’s do the steps for each. We need to compute \\(4^{-1} \\pmod {79}\\). It is a good idea to also write in each line the remainder as a linear combination of the values \\(a, b\\). \\(a\\) \\(b\\) division linear combination \\(79\\) \\(4\\) \\(79 = 19\\cdot 4 + 3\\) \\(3 = 1\\cdot 79 + (- 19)\\cdot 4\\) \\(4\\) \\(3\\) \\(4 = 1\\cdot 3 + 1\\) \\(1 = 1\\cdot 4 + (-1) \\cdot 3\\) \\(3\\) \\(1\\) \\(3 = 3\\cdot 1 + 0\\) We now need to go “back”. It can be quite confusing not mixing the numbers. We start with the equation that has remainder 1. We work our way up by replacing the remainder of the previous equation. In each line, the equation should be a linear combination of the values \\(a, b\\). In what follows, we use blue to denote the linear combination elements (i.e. the \\(a,b\\)) \\[ \\begin{aligned} a=\\color{blue}{4},&amp;\\quad b=\\color{blue}{3}, &amp;\\qquad 1 &amp;= {1}\\cdot \\color{blue}{4} + ({-1})\\cdot \\color{blue}{3} \\\\ a=\\color{blue}{79},&amp;\\quad b=\\color{blue}{4}, &amp;\\qquad 1 &amp;= {1}\\cdot \\color{blue}{4} + ({-1})\\cdot (1\\cdot \\color{blue}{79} + (- 19)\\cdot \\color{blue}{4}) \\\\ &amp; &amp; &amp;= (-1)\\cdot \\color{blue}{79} + 20\\cdot \\color{blue}{4} \\\\ \\end{aligned} \\] We end up with the equation \\(1 = (-1)\\cdot \\color{blue}{79} + 20\\cdot \\color{blue}{4}\\). Reducing this equation \\(\\bmod 79\\) gives \\(4\\cdot 20 \\equiv 1 \\pmod {79}\\). Therefore, \\(4^{-1} \\equiv 20 \\pmod {79}\\). The numbers are a bit bigger here, but the steps remain exactly the same. \\(a\\) \\(b\\) division linear combination \\(3791\\) \\(83\\) \\(3791 = 45\\cdot 83 + 56\\) \\(56 = 1\\cdot 3791 + (- 45)\\cdot 83\\) \\(83\\) \\(56\\) \\(83 = 1\\cdot 56 + 27\\) \\(27 = 1\\cdot 83 + (- 1)\\cdot 56\\) \\(56\\) \\(27\\) \\(56 = 2\\cdot 27 + 2\\) \\(2 = 1\\cdot 56 + (- 2)\\cdot 27\\) \\(27\\) \\(2\\) \\(27 = 2\\cdot 13 + 1\\) \\(1 = 1\\cdot 27 + (- 13)\\cdot 2\\) \\(2\\) \\(1\\) \\(1 = 2\\cdot 1 + 0\\) We next start from the linear combination of \\(1\\) and work our way back. \\[ \\begin{aligned} a=\\color{blue}{27},&amp;\\quad b=\\color{blue}{2}, &amp;\\qquad 1 &amp;= {1}\\cdot \\color{blue}{27} + (-13)\\cdot \\color{blue}{2} \\\\ a=\\color{blue}{56},&amp;\\quad b=\\color{blue}{27}, &amp;\\qquad 1 &amp;= {1}\\cdot \\color{blue}{27} + (-13)\\cdot (1\\cdot \\color{blue}{56} + (- 2)\\cdot \\color{blue}{27}) \\\\ &amp; &amp; &amp;= (-13)\\cdot \\color{blue}{56} + 27\\cdot \\color{blue}{27} \\\\ a=\\color{blue}{83},&amp;\\quad b=\\color{blue}{56}, &amp;\\qquad 1 &amp;= (-13)\\cdot \\color{blue}{56} + 27\\cdot (1\\cdot \\color{blue}{83} + (- 1)\\cdot \\color{blue}{56}) \\\\ &amp; &amp; &amp;= 27\\cdot \\color{blue}{83} + (-40)\\cdot \\color{blue}{56} \\\\ a=\\color{blue}{3791},&amp;\\quad b=\\color{blue}{83}, &amp;\\qquad 1 &amp;= 27\\cdot \\color{blue}{83} + (-40)\\cdot (1\\cdot \\color{blue}{3791} + (- 45)\\cdot \\color{blue}{83}) \\\\ &amp; &amp; &amp;= -40\\cdot \\color{blue}{3791} + 1827\\cdot \\color{blue}{83} \\\\ \\end{aligned} \\] We end up with the equation \\(-40\\cdot \\color{blue}{3791} + 1827\\cdot \\color{blue}{83}\\). Reducing this equation \\(\\bmod 379179\\) gives \\(1827\\cdot 83 \\equiv 1 \\pmod {3791}\\). Therefore, \\(83^{-1} \\equiv 1827 \\pmod {3791}\\). Exercise 6.9 Consider the linear equation \\(ax\\equiv b\\pmod n\\). When does it have a solution? Solve the equation \\(6x = 10 \\pmod {23}\\). Solution. To solve such exercises, it is enough to remember that the normal operations behave as one should expect. This means that we can consider the equation to be defined over the \\(\\mathbb{R}\\) and perform any operations “the correct way”, here \\(\\mod n\\). This means that here, we can solve this linear equation as \\(x = b \\cdot a^{-1} \\pmod {n}\\). This equation admits a solution iff \\(a\\) has an invert \\(\\pmod n\\). This happens iff \\({gcd}(a, n)=1\\). By the previous question, the solution either does not exist or is equal to \\(10\\cdot 6^{-1}\\pmod{23}\\). In this case \\(23\\) is a prime which means that the inverse exists (in the general case, one can compute the \\(gcd\\) and check if the result is 1 or not). Therefore, we need to compute the inverse of \\(6\\) \\(\\pmod{23}\\) and multiply it with 10 \\(\\pmod {23}\\). We first compute the inverse. We have \\[ \\begin{aligned} 23 &amp;= 3\\cdot 6 + 5 \\\\ 6 &amp;= 1\\cdot 5 + 1 \\\\ \\end{aligned} \\] Therefore we have: \\[ 1 = 1\\cdot 6 - 1\\cdot 5 = 1\\cdot 6 - 1(1\\cdot 23 - 3\\cdot 6) = -1 \\cdot 23 + 4 \\cdot 6 \\] Reducing the above equation \\(\\pmod {23}\\) we get \\(6^{-1}\\equiv 4 \\pmod{23}\\). Finally \\(4\\cdot 10 \\equiv 40 \\equiv 17\\pmod{23}\\). The solution is \\(x\\equiv 17\\pmod{23}\\). Exercise 6.10 Consider the following problem: given as input \\(N=p\\cdot q\\) where \\(p,q\\) are large primes, compute the value \\(\\phi(N)\\). Do you think this is a tractable problem. Solution. These kind of problems have two possible approaches: either it is tractable, which can be shown by presenting an efficient algorithm for it, or it is (probably)28 intractable, which can be shown by showing that an efficient algorithm for it implies an efficient algorithm for another (assumingly) hard problem. Unfortunately, aside intuition, there is no way to know which is the case, so it must be trial and error. To prove hardness, you should consider a related problem. In this case, the obvious related problem is factoring products of large primes. Recall this is assumed to be hard. We will assume the existence of an algorithm \\(\\mathcal{A}_\\phi\\) that solves this problem and use it to construct an algorithm \\(\\mathcal{A}_{\\text{fact}}\\) that solves the factoring problem. If we succeed, either our assumption is wrong (the existence of \\(\\mathcal{A}_\\phi\\)) is wrong, or factoring is easy. It is widely believed that this is not the case, so it must be the case that the problem is indeed hard. Our strategy for factoring is the following: assuming we know \\(\\phi(N)\\), we have two equations for two unknowns \\(p\\) and \\(q\\), specifically, \\[ N = p\\cdot q,\\qquad \\phi(N) = (p-1)(q-1) \\] We can set in the latter \\(q = N/p\\) and solve for \\(p\\). This will result in a quadratic equation which we can solve efficiently! Now, out of the two possible solutions we will choose the one that corresponds to an integer, prime number. Note that such a number must exist (we assume \\(N=p\\cdot q\\)) so we will indeed find it! And of course, knowing \\(p\\) means we can also compute \\(q=N/p\\). We present the result a bit more formally. Assume there exists an algorithm \\(\\mathcal{A}_\\phi\\) that takes input \\(N=p\\cdot q\\) where \\(p,q\\) are large primes, and outputs the value \\(\\phi(N)\\). We build an algorithm \\(\\mathcal{A}_{\\text{fact}}\\) that takes input \\(N=p\\cdot q\\) where \\(p,q\\) are large primes, and outputs \\(p\\) and \\(q\\). The algorithm \\(\\mathcal{A}_{\\text{fact}}(N)\\) does the following Compute \\(\\phi\\gets \\mathcal{A}_{\\phi}(N)\\). Define the equation \\[ (p-1)(N/p - 1) = \\phi \\Leftrightarrow(p-1)(N/p - 1) = \\phi \\] Solve the equation \\(p^2 + (\\phi - N - 1)p + N = 0\\). Let \\(p_1, p_2\\) be the solutions to the equation. Choose the solution \\(p_b\\) where \\(p_b\\) is a prime number. Set \\(p = p_b\\) and \\(q = N/p_b\\). Output \\((p,q)\\). The algorithm is correct: as we argued before, there exists a \\(p\\) that satisfies \\((p-1)(N/p - 1) = \\phi\\) and in particular one of the (at most two) solutions will necessarily be a prime integer number. This number must be one of the two divisors of \\(N\\). And clearly, the other divisor is \\(N/p\\). So we indeed compute the unique factorization of \\(N\\). Furthermore, the algorithm \\(\\mathcal{A}_{\\text{fact}}(N)\\) is efficient if \\(\\mathcal{A}_{\\phi}(N)\\) is efficient. Indeed all steps can be performed efficiently. We conclude that we have an efficient algorithm for factoring assuming an efficient algorithm for computing \\(\\phi\\). Therefore, finding \\(\\phi\\) is at least as hard as factoring. Exercise 6.11 Alice bought a set of 120 balls but it seems a lot of them are missing. When she arranges the balls in rows of length 13 there are 2 balls on the last row and when she arranges them in rows of length 11 there are 5 on the last row. How many balls are missing? Solution. Let’s call \\(x\\) the number of balls. The fact that 2 balls are on the last row in the 13 ball arrangement means that \\(x\\pmod {13} = 2\\) and equivalently for the other case \\(x\\pmod {11} = 5\\). Since \\(13,11\\) are prime numbers they are also coprime. We can apply the Chinese remainder theorem. We have that \\[ x = (11\\cdot 2 (11^{-1} \\mod 13)) + (13\\cdot 5(13^{-1}\\mod 11) ) \\mod (11\\cdot 13) \\] Since the numbers are small we can compute the inverses by trial and error. Noting that \\(13\\equiv 2 \\pmod {11}\\) we have \\[ 6\\cdot 11 \\equiv 66 \\equiv 5\\cdot 13 + 1\\equiv 1 \\pmod{13}, \\qquad 6\\cdot 2 \\equiv 11+1 \\equiv 1 \\pmod{11} \\] or equivalently \\[ 6 \\equiv 2^{-1} \\pmod{13}, \\qquad 6 \\equiv 2^{-1} \\pmod{11} \\] We therefore have \\[ \\begin{aligned} x &amp;= (11\\cdot 2 (11^{-1} \\mod 13)) + (13\\cdot 5(2^{-1}\\mod 11) ) \\mod (11\\cdot 13) \\\\ &amp;= (11\\cdot 2 \\cdot 6 + 13\\cdot 5\\cdot 6) \\mod (11\\cdot 13) \\\\ &amp;= (132 + 390) \\mod (143) \\\\ &amp;= 93 \\mod (143) \\\\ \\end{aligned} \\] The number of balls must be 93. Indeed expressing the \\(\\text{mod}\\) as an Euclidian division, we have \\[x = n\\cdot 143 + 93\\] but since \\(x\\leq 120 &lt; 143\\) it must be the case that \\(n=0\\) which means \\(x = 93\\) and therefore \\(27\\) balls are missing. For those with a background in complexity theory, primality is a problem in P and factorization is a problem in NP.↩︎ The only small caveat is that, when \\(a\\) is a multiple of \\(12\\), the remainder will be \\(0\\), not \\(12\\), although both of these represent the same position. So, to be precise, let us assume that our clock has a \\(0\\) instead of \\(12\\), so that it perfectly aligns with the remainders.↩︎ Note the wording. It is currently unknown how to prove that computational problems are hard or whether they actually are for that matter. What we can show is show relations between problems, i.e. if problem A is hard so is problem B.↩︎ "],["algebraic-structures.html", "7 Algebraic structures 7.1 Groups 7.2 Finite fields Solved exercises", " 7 Algebraic structures We complete our exposition of the mathematical background by discussing some common algebraic structures, which generalize what we have seen in the previous section. Informally, an algebraic structure is a set with one or more operations on the elements of the set. The properties of these operations determine the kind of structure that we have. In this section, we will learn about two of these: Groups. Finite fields. 7.1 Groups Groups are the simplest algebraic structure that we will study, since they only have one operation. Definition 7.1 A group is a pair \\((\\mathbb{G},\\circ)\\), where \\(\\mathbb{G}\\) is a set and \\(\\circ\\) is an operation on \\(\\mathbb{G}\\), that is, a function \\[\\begin{aligned} \\circ : \\phantom{a} &amp; \\mathbb{G}\\times \\mathbb{G}&amp; \\rightarrow &amp; \\phantom{a}\\mathbb{G}\\\\ &amp; (x,y) &amp; \\mapsto &amp; \\phantom{a} x\\circ y, \\end{aligned}\\] which additionally satisfies the following properties: Associative law: \\((x\\circ y)\\circ z = x\\circ(y\\circ z)\\) for all \\(x,y,z\\in \\mathbb{G}\\). Existence of identity: there exists \\(e\\in \\mathbb{G}\\) such that \\(e\\circ x=x\\circ e=x\\) for all \\(x\\in \\mathbb{G}\\). Such element \\(e\\) is called the identity element. Existence of inverse: for any \\(x\\in \\mathbb{G}\\), there exists \\(y\\in \\mathbb{G}\\) such that \\(x\\circ y = y\\circ x = e\\), where \\(e\\) is the identity element. Such \\(y\\) is called the inverse of \\(x\\). A group is said to be commutative (or abelian) if it satisfies the following additional property: Commutativity: \\(x\\circ y=y \\circ x\\) for all \\(x,y\\in \\mathbb{G}\\). The order of a group is the number of elements in \\(\\mathbb{G}\\), if the set is finite, and infinite otherwise, and is denoted by \\(ord(\\mathbb{G})\\). When there is no ambiguity about the operation, we will refer to the group \\(\\mathbb{G}\\), instead of \\((\\mathbb{G},\\circ)\\), for simplicity. Most of the time, the group operation will be (possibly modular) addition or multiplication, although these are not the only possible cases. Let us consider some examples, and see whether they are groups or not. \\((\\mathbb{Z}_{\\geq 0},+)\\), where \\(\\mathbb{Z}_{\\geq 0}\\) is the set of non-negative elements of \\(\\mathbb{Z}\\), and \\(+\\) is integer addition. Integer addition is associative, and there is an identity element \\(0\\). However, there is no \\(x\\in\\mathbb{Z}_{\\geq 0}\\) such that \\[1+x=0,\\] and therefore \\((\\mathbb{Z}_{\\geq 0},+)\\) is not a group. \\((\\mathbb{Z},+)\\) is a group of infinite order, since the operation is still associative, there is an identity element \\(0\\), and for any \\(x\\in\\mathbb{Z}\\), there exists \\(y=-x\\in\\mathbb{Z}\\) such that \\[x+y=y+x=0.\\] For similar reasons, the pair \\((\\mathbb{Z}_n,+)\\), for \\(n\\in\\mathbb{N}\\), is also a group, of order \\(n\\). For \\(n\\in\\mathbb{N}\\), the pair \\((\\mathbb{Z}_n^*,\\cdot)\\), where \\(\\cdot\\) is multiplication modulo \\(n\\), is a group of order \\(\\varphi(n)\\). The operation is clearly associative, \\(1\\) is an identity element for multiplication, and every element \\(x\\in\\mathbb{Z}_n^*\\) has an inverse \\(x^{-1}\\in\\mathbb{Z}_n^*\\), by definition of \\(\\mathbb{Z}_n^*\\). The pair \\((\\mathbb{Z}_2\\times\\mathbb{Z}_3,+)\\), where \\(+\\) means component-wise addition in their respective moduli, is a group of order \\(6\\). Indeed, it is easy to see that a product of two groups is a group. Moreover, since the operations are commutative, all of these groups are commutative. Exercise 7.1 Explain why none of the following is a group: \\((\\mathbb{Z},\\cdot)\\), where \\(\\cdot\\) is integer multiplication. \\((\\mathbb{Z}_n,\\cdot)\\), where \\(\\cdot\\) is multiplication modulo \\(n\\), for \\(n\\in \\mathbb{N}\\). Group notation. In the above examples, you can observe that the notation differs from case to case, depending on the nature of the operation. For example, given some element \\(x\\), we denote its inverse with respect to addition by \\(-x\\), and its inverse with respect to multiplication by \\(x^{-1}\\). Applying the operation to \\(n\\) copies of the same element \\(x\\) is represented by \\(nx\\) in additive notation, and by \\(x^n\\) in multiplicative notation. Even though not all group operations are addition or multiplication, it is common to adopt their notation for a generic group operation.29 In these notes, we will often use multiplicative notation for generic groups. That is, we denote the identity element by \\(1\\), the inverse of \\(x\\) by \\(x^{-1}\\), and the operation of \\(x\\) and \\(y\\) by \\(x\\cdot y\\) or simply \\(xy\\). Observe that, if a group \\(\\mathbb{G}\\) contains an element \\(g\\), then the fact that \\(g^2=gg\\) implies that \\(g^2\\in\\mathbb{G}\\) too. Similarly, \\(g^3=g^2g\\in \\mathbb{G}\\), and so on. It is easy to generalize this idea, and conclude that, if \\(g\\in \\mathbb{G}\\), then \\(g^n\\in \\mathbb{G}\\) for all \\(n\\in\\mathbb{N}\\). That is, a single element \\(g\\) generates many elements of a group. This leads us to the concept of cyclic groups, which are those that contain only the powers of a single element. Definition 7.2 A group \\(\\mathbb{G}\\) is said to be cyclic if there exists \\(g\\in \\mathbb{G}\\) such that \\[\\mathbb{G}=\\{g^n\\mid n\\in\\mathbb{Z}_{\\geq 0}\\}.\\] This group is also denoted by \\(\\langle g \\rangle\\), and is called the group generated (or spanned) by \\(g\\), and \\(g\\) is called a generator of \\(\\mathbb{G}\\). Note that, in a group with additive notation, we would write instead \\[\\mathbb{G}=\\{nx\\mid n\\in\\mathbb{Z}_{\\geq 0}\\}.\\] The name of these groups come from their often cyclic nature. We illustrate this with the example of the group \\(\\mathbb{Z}_5\\) with addition modulo \\(5\\). We claim that \\(\\mathbb{Z}_5\\) is generated by \\(2\\). Indeed, \\[\\begin{aligned} &amp; 2\\cdot 0\\bmod 5 = 0\\\\ &amp; 2\\cdot 1\\bmod 5 = 2\\\\ &amp; 2\\cdot 2\\bmod 5 = 4\\\\ &amp; 2\\cdot 3\\bmod 5 = 6 \\bmod 5 = 1\\\\ &amp; 2\\cdot 4\\bmod 5 = 8 \\bmod 5 = 3\\\\ &amp; 2\\cdot 5\\bmod 5 = 10 \\bmod 5 = 0\\\\ &amp; 2\\cdot 6\\bmod 5 = 12 \\bmod 5 = 2\\\\ &amp; 2\\cdot 7\\bmod 5 = 14 \\bmod 5 = 4\\\\ &amp; \\vdots \\end{aligned}\\] We make two observations: the first is that all the elements of \\(\\mathbb{Z}_5\\) are reached through powers of \\(2\\), thus we have proven that it is generated by \\(2\\). The second is that, after \\(2\\cdot 5\\), it is clear that the numbers start to repeat in a fixed order. That is, the powers of \\(2\\) cycle through \\(\\mathbb{Z}_5\\) in the order \\(0,2,4,1,3\\). Note that generators are not unique. For example, \\(\\mathbb{Z}_5\\) could be generated by \\(1\\) as well, although not by \\(0\\). Moreover, although we will not get into the technical details, we informally mention that any cyclic group of finite order \\(n\\) behaves like the additive group \\(\\mathbb{Z}_n\\), and any cyclic group of infinite order behaves like \\(\\mathbb{Z}\\). Proposition 7.1 For any \\(n\\in\\mathbb{N}\\), we have that \\((\\mathbb{Z}_n,+)\\), where \\(+\\) is addition modulo \\(n\\), is a cyclic group. Exercise 7.2 Decide whether the group \\(\\mathbb{Z}_2\\times\\mathbb{Z}_3\\), with component-wise modular addition, is a cyclic group. Groups might contain smaller groups inside, that are consistent with the same operation. Definition 7.3 Let \\(\\mathbb{G}\\) be a group. A subgroup \\(\\mathbb{H}\\) of \\(\\mathbb{G}\\) is a subset of \\(\\mathbb{G}\\) that contains the identity element and such that \\(\\mathbb{H}\\) forms a group with the operation of \\(\\mathbb{G}\\) restricted to \\(\\mathbb{H}\\). That is, for any \\(x,y\\in \\mathbb{H}\\), we have that \\(x^{-1}\\in\\mathbb{H}\\) and \\(xy\\in \\mathbb{H}\\). For example, the set of even integers is a subgroup of \\((\\mathbb{Z},+)\\), since the addition of even numbers is even. It is easy to see that subgroups are groups too. Exercise 7.3 Decide which of these subsets are subgroups of \\((\\mathbb{Z}_4,+)\\): \\[\\{0\\},\\quad\\qquad\\{0,2\\},\\quad\\qquad\\{1,3\\},\\quad\\qquad\\{0,1,3\\}.\\] It is easy to see that, for any \\(x\\in\\mathbb{G}\\), the group \\(\\langle x\\rangle\\) is a subgroup of \\(\\mathbb{G}\\). Note that this might not be the whole \\(\\mathbb{G}\\). This allows us to define the order of an element as follows. Definition 7.4 Let \\(\\mathbb{G}\\) be a group, and \\(x\\in\\mathbb{G}\\). We define the order of \\(x\\) as the order of \\(\\langle x\\rangle\\), and denote it by \\(ord(x)\\). The following Sage code defines the additive group \\((\\mathbb{Z}_9,+)\\), computes a generator, and then computes the order of each element, also explicitly providing the cycle that corresponds to that element. G = AdditiveAbelianGroup([9]) g = G.gens()[0] # The [0] is necessary because, technically, # .gens() returns a list. print(str(G)+&quot;, generator: &quot;+str(g)) for x in G: print(&quot;x = &quot;+str(x)+&quot;, ord(x) = &quot;+str(x.order())) for i in range(x.order()+1): print(i*x) If \\(\\mathbb{H}\\) is a subgroup of \\(\\mathbb{G}\\), it is clear that the order of \\(\\mathbb{H}\\) will be smaller than the order of \\(\\mathbb{G}\\), since the former is contained in the latter. But even more, it will necessarily be a divisor, which is something you may have observed after running the above code. Proposition 7.2 (Lagrange's theorem) Let \\(\\mathbb{G}\\) be a finite group. For any subgroup \\(\\mathbb{H}\\) of \\(\\mathbb{G}\\), \\(ord(\\mathbb{H})\\mid ord(\\mathbb{G})\\). For any \\(x\\in\\mathbb{G}\\), \\(ord(x)\\mid ord(\\mathbb{G})\\). The order of a group plays an important role in the group operation. Proposition 7.3 (Euler's theorem) Let \\(\\mathbb{G}\\) be a group of finite order. Then, for any \\(x\\in \\mathbb{G}\\), we have that \\[x^{ord(\\mathbb{G})}=1.\\] Exercise 7.4 Verify that Euler’s theorem holds for the group \\(\\mathbb{Z}_7^*\\), with the operation multiplication modulo \\(7\\). The exercise can also be solved with Sage, using the following code. G = Integers(7).unit_group() # Multiplicative group Z_7^*. for x in G: print(x^(G.order())) These propositions give us an easy way to check whether an element \\(x\\in\\mathbb{G}\\) is a generator. Recall that the order of \\(x\\) tells us how many elements there are in \\[\\langle x\\rangle = \\{x^n\\mid n\\in\\mathbb{Z}_{\\geq 0}\\},\\] that is, the number of steps in the cycle of powers of \\(x\\) before going back to \\(1\\) and repeating elements. If it were the case that \\(ord(x)=ord(\\mathbb{G})\\), this would mean that the cycle is as big as \\(\\mathbb{G}\\), and therefore it must be that \\(\\langle x\\rangle = \\mathbb{G}\\). On the other hand, if \\(ord(x)&lt;ord(\\mathbb{G})\\), this would mean that not all elements of \\(\\mathbb{G}\\) can be obtained as powers of \\(x\\), and thus \\(x\\) would not generate \\(\\mathbb{G}\\). Hence, we can check whether \\(x\\) generates \\(\\mathbb{G}\\) by computing \\(ord(x)\\) and comparing it with \\(ord(\\mathbb{G})\\). To do so, a naive approach would be to iteratively compute \\[x,x^2,x^3,x^4,x^5,\\dots\\] until some \\(n\\) is found such that \\[x^n=1,\\] and we would have that \\(n=ord(x)\\). However, note that this approach takes time linear in \\(ord(\\mathbb{G})\\). When \\(ord(\\mathbb{G})\\) is easy to factor, a more sensible approach is to use Lagrange’s theorem, which tells us that \\[ord(x)\\mid ord(\\mathbb{G}).\\] This allows us to drastically reduce the candidates to the powers \\[x^{\\frac{ord(\\mathbb{G})}{d}},\\] for all \\(d\\mid ord(\\mathbb{G})\\). The smallest exponent that produces a \\(1\\) will be the order of \\(x\\). In particular, if \\(ord(G)\\) is prime, then there are only two candidates: \\(x\\) and \\(x^{ord(\\mathbb{G})}\\). Hence in this case, if \\(x\\neq 1\\), necessarily \\(x\\) is a generator. That is, we have proven the following result. Proposition 7.4 Let \\(\\mathbb{G}\\) be a cyclic group of prime order. If \\(g\\in\\mathbb{G}\\) is not the identity element, then \\(g\\) generates \\(\\mathbb{G}\\). We conclude the section on groups by looking at some specific properties of the groups \\(\\mathbb{Z}_n\\) and \\(\\mathbb{Z}_n^*\\). Proposition 7.5 Let \\(p\\) be a prime number. The multiplicative group \\(\\mathbb{Z}_p^*\\) is a cyclic group of order \\(p-1\\), and it has \\(\\varphi(p-1)\\) generators. These generators are called primitive roots modulo \\(p\\). Moreover, a direct consequence of Euler’s theorem and the proposition above is known as Fermat’s little theorem.30 Proposition 7.6 (Fermat's little theorem) Let \\(p\\) be a prime number. Then, for any \\(x\\in\\mathbb{Z}\\), we have that \\[x^{p-1}\\equiv1\\pmod{p}.\\] 7.2 Finite fields A field is a more complex algebraic structure, since it is equipped with two operations, both of which work similarly to a group operation.31 Definition 7.5 A field is a triple \\((\\mathbb{F},\\circ,\\ast)\\), where \\(\\mathbb{F}\\) is a set and \\(\\circ\\) and \\(\\ast\\) are operations on \\(\\mathbb{F}\\), that is, functions \\[\\begin{aligned} \\circ : \\phantom{a} &amp; \\mathbb{F}\\times \\mathbb{F}&amp; \\rightarrow &amp; \\phantom{a}\\mathbb{F}&amp; \\qquad\\qquad &amp; \\ast :&amp; \\mathbb{F}\\times\\mathbb{F}&amp; \\rightarrow &amp; \\mathbb{F}\\\\ &amp; (x,y) &amp; \\mapsto &amp; \\phantom{a} x\\circ y, &amp; &amp; &amp; (x,y) &amp; \\mapsto &amp; x\\ast y, \\end{aligned}\\] which additionally satisfy the following properties: \\((\\mathbb{F},\\circ)\\) is a commutative group, with identity element denoted by \\(0\\). \\((\\mathbb{F}\\setminus\\{0\\},\\ast)\\) is a commutative group, where \\(\\mathbb{F}\\setminus\\{0\\}\\) is the set \\(\\mathbb{F}\\) except for the identity element of \\(\\circ\\). Distributive law: \\(x\\ast(y\\circ z)=(x\\ast y)\\circ(x\\ast z)\\) for all \\(x,y,z\\in\\mathbb{F}.\\) Since we have two operations at the same time, we will adopt the additive and multiplicative notation from groups to represent each of them, respectively. That is, we will think of the first operation of a field as a form of addition and the second as a form of multiplication. On input \\(x,y\\), we write the result of the first operation by \\(x+y\\), and the result of the second by \\(xy\\). The identity elements of each operation are denoted by \\(0\\) and \\(1\\), respectively. The inverse of \\(x\\) with respect to the first operation is denoted by \\(-x\\). The inverse of \\(x\\) with respect to the second operation is denoted by \\(x^{-1}\\). The following table summarizes the notation: Operation Operation on input \\(x,y\\) Identity element Inverse of \\(x\\) \\(+\\) \\(x+y\\) \\(0\\) \\(-x\\) \\(\\cdot\\) \\(xy\\) \\(1\\) \\(x^{-1}\\) We consider some examples: \\(\\mathbb{Q}\\), \\(\\mathbb{R}\\) and \\(\\mathbb{C}\\), with usual addition and multiplication, are fields. The properties are easy to check. Identity elements are \\(0\\) and \\(1\\). The additive inverse of \\(x\\) is \\(-x\\), and the multiplicative inverse of \\(x\\) is \\(1/x\\). Consider \\(\\mathbb{Z}_9\\), with addition and multiplication modulo \\(9\\). It satisfies most of the properties of a field, but we will see that some elements have no inverse with respect to the second operation. The following table gives the multiplicative inverses of each element: \\(x\\) \\(0\\) \\(1\\) \\(2\\) \\(3\\) \\(4\\) \\(5\\) \\(6\\) \\(7\\) \\(8\\) \\(x^{-1}\\) \\(-\\) \\(1\\) \\(5\\) \\(-\\) \\(7\\) \\(2\\) \\(-\\) \\(4\\) \\(8\\) Note that some elements are not invertible, in this case \\(0\\), \\(3\\) and \\(6\\). Therefore, \\(\\mathbb{Z}_9\\) is not a field. In Sage, the following code can be used to check whether an element of \\(\\mathbb{Z}_n\\) has a multiplicative inverse. R = Integers(n) # Z_n with addition and # multiplication modulo n. for x in R: print(x,x.is_unit()) In this course we will be interested in finite fields,32 so what are some examples of those? Could it be, for example, that \\(\\mathbb{Z}_n\\) is a field for some \\(n\\in\\mathbb{N}\\)? We already know that this will not be true for all \\(n\\in\\mathbb{N}\\), since we saw that some elements of \\(\\mathbb{Z}_9\\) do not have an inverse. Proposition 7.7 The \\(\\mathbb{Z}_n\\), with addition and multiplication modulo \\(n\\), is a field if and only if \\(n\\) is a prime number. The only finite fields that exist have a number of elements that is either a prime number or a power of a prime number. Proposition 7.8 Let \\(\\mathbb{F}\\) be a finite field. Then \\(\\mathbb{F}\\) has \\(p^k\\) elements, where \\(p\\) is a prime number and \\(k\\in\\mathbb{N}\\). We denote such a finite field by \\(\\mathbb{F}_{p^k}\\). If \\(k=1\\), the field is called a prime field, otherwise it is known as an extension field. We call \\(p\\) the characteristic of the field. Moreover, given a prime \\(p\\) and \\(k\\in\\mathbb{N}\\), there is essentially only one field with \\(p^k\\) elements, although this same field can have different representations. One might think that these extension fields \\(\\mathbb{F}_{p^k}\\) correspond to \\(\\mathbb{Z}_{p^k}\\), but actually this is not true. Again, we recall the example in which we showed that \\(\\mathbb{Z}_9=\\mathbb{Z}_{3^2}\\) is not a field, because some elements are not invertible. So, if \\(\\mathbb{F}_{p^k}\\) is not \\(\\mathbb{Z}_{p^k}\\), what is it? It turns out that we need some more involved tools to describe these. An element of the field \\(\\mathbb{F}_{p^k}\\) is not represented by an integer. Instead, we use a polynomial \\[r_{k-1}x^{k-1}+\\dots +r_1x+r_0,\\] for some coefficients \\(r_{k-1},\\dots,a_0\\in\\mathbb{F}_p\\). Note that the degree of the polynomial is \\(k-1\\) for representing an element of \\(\\mathbb{F}_{p^k}\\). Since there are \\(k\\) coefficients, and each of these has \\(p\\) possible values, we see that there are in total \\(p^k\\) possible polynomials, and thus so far it makes sense to try to associate these with the \\(p^k\\) elements of \\(\\mathbb{F}_{p^k}\\). But we still need to specify how the operations work. For addition, we use usual polynomial addition, that is, \\[\\begin{split} R(X)+S(X)&amp; =(r_{k-1}X^{k-1}+\\dots +r_1X+r_0)+(s_{k-1}X^{k-1}+\\dots +s_1X+s_0) = \\\\ &amp; =(r_{k-1}+s_{k-1})X^{k-1}+\\dots +(r_1+s_1)X+(r_0+s_0), \\end{split}\\] which is also a polynomial of degree at most \\(k-1\\), and it is easy to see that it verifies the conditions of a group operation. Note that the coefficients are in \\(\\mathbb{F}_p\\), so the additions of the coefficients are reduced modulo \\(p\\) if necessary. For multiplication, we consider multiplying the two polynomials. However, note that, in general, multiplying two polynomials \\(R(X)\\) and \\(S(X)\\) of degree \\(k-1\\) produces a polynomial \\(R(X)S(X)\\) of degree \\(2k-2\\), which is not in \\(\\mathbb{F}_{p^k}\\) anymore. Our trick then is analogous to what we did in \\(\\mathbb{F}_p=\\mathbb{Z}_p\\). In that case, we reduced the result modulo a prime number \\(p\\). Now, we will reduce the result modulo a certain polynomial \\(T(X)\\) of degree \\(k\\). This means that we compute the polynomial division of \\(R(X)S(X)\\) by \\(T(X)\\), and keep the remainder, so that the result has degree at most \\(k-1\\).33 Since this is analogous to the case of integers, we extend the notation \\[A(X)\\bmod{T(X)}\\] to denote the remainder of the polynomial division \\(A(X)\\) by \\(T(X)\\), and the notation \\[A(X)\\equiv B(X)\\pmod{T(X)}\\] to denote that \\[(A(X)-B(X))\\bmod{T(X)}=0,\\] that is, \\(A(X)-B(X)\\) is a multiple of \\(T(X)\\). But which polynomial \\(T(X)\\) should we use? Not every polynomial will yield a multiplication law of a field. We require an irreducible polynomial, that is, a polynomial that cannot be written as a product of two non-trivial polynomials with coefficients in \\(\\mathbb{F}_p\\). At first, this might seem like an obscure condition, but if you think about it, it is actually analogous to what happened for prime fields. In that case, we started from \\(\\mathbb{Z}_n\\), and concluded that it is only a field when \\(n\\) is a prime, which is precisely a number that cannot be factored in a non-trivial way. In extension fields, we are representing elements by polynomials instead of integers, and so we look for the analogue of a prime number in this sense of lack of factorization. This leads us to irreducible polynomials, although a formal proof of this statement is beyond the scope of this course. Proposition 7.9 Let \\(p\\) be a prime number and \\(k\\in\\mathbb{N}\\). Let \\(T(X)\\) be an irreducible polynomial of degree \\(k\\) with coefficients in \\(\\mathbb{F}_p\\). The set of polynomials of degree at most \\(k-1\\) and coefficients over \\(\\mathbb{F}_p\\), together with the operations polynomial addition and multiplication modulo \\(T(X)\\), is a field of \\(p^k\\) elements. The choice of irreducible polynomial \\(T(X)\\) determines the multiplication law, but any of the outcomes can be seen as different representations of the same finite field of size \\(p^k\\). Finally, inversion of elements in \\(\\mathbb{F}_{p^k}\\) can be handled with the extended Euclidean algorithm for polynomials, similarly to the way it was used to invert integers modulo \\(n\\). We describe this algorithm below. Definition 7.6 Given a polynomial \\[a_kX^k+a_{k-1}X^{k-1}+\\dots+a_1X+a_0,\\] we define its leading coefficient as the coefficient of the highest-degree monomial, in this case \\(a_k\\). A polynomial with leading coefficient \\(1\\) is called monic. Definition 7.7 Given two polynomials \\(A(X)\\) and \\(B(X)\\), we define their greatest common divisor as the highest-degree monic polynomial that divides both. Proposition 7.10 Let \\(p\\) be a prime number and let \\(k\\in\\mathbb{N}\\). Let \\(\\mathbb{F}_{p^k}\\) be the finite field of \\(p^k\\) elements, with irreducible polynomial \\(T(X)\\). Let \\(A(X),B(X)\\) be two elements of \\(\\mathbb{F}_{p^k}\\). Then, there exist \\(U(X),V(X)\\) in \\(\\mathbb{F}_{p^k}\\) such that \\[A(X)U(X)+B(X)V(X)=\\gcd(A(X),B(X)).\\] The Euclidean algorithm for polynomials is very similar to the analogous algorithm for integers, with an additional step to ensure that the output is a monic polynomial. Compute the polynomial division of \\(A(X)\\) by \\(B(X)\\), obtaining \\(Q(X),R(X)\\) such that the \\(0\\leq \\deg R&lt;\\deg B\\) and \\[A(X)=B(X)Q(X)+R(X).\\] If \\(R(X)=0\\), go to step 3. Otherwise, return to the previous step, replacing \\(A(X)\\) by \\(B(X)\\) and \\(B(X)\\) by \\(R(X)\\). Multiply \\(B(X)\\) by the inverse of its leading coefficient, and output the result. This will produce \\(\\gcd(A(X),B(X))\\), from which we can go back through the procedure to find the expression in the proposition above. We show an example in \\(\\mathbb{F}_{5^4}\\), with irreducible polynomial \\(T(X)=X^4+4X^2+4X+2\\). \\[A(X)=X^3+3X^2+4, \\qquad\\qquad B(X)=X^2+2X+2.\\] Note that coefficients are in \\(\\mathbb{F}_5\\), and thus all the operations performed on coefficients must be carried out modulo \\(5\\). We start by dividing \\(A(X)\\) by \\(B(X)\\), obtaining \\[X^3+3X^2+4=(X^2+2X+2)(X+1)+(X+2)\\] Since the remainder is not \\(0\\), we compute the next polynomial division, obtaining \\[X^2+2X+2=(X+2)X+2.\\] Finally, \\[(X+2)=2(3X+1)+0.\\] Since the last remainder is \\(0\\), we conclude that the greatest common divisor of \\(A(X)\\) and \\(B(X)\\) is \\(2\\) multiplied by the inverse of its leading coefficient, so in this case \\[\\gcd(A(X),B(X))=2\\cdot 2^{-1}=1.\\] Again, by undoing these steps, we can find the expression in Proposition 7.10: \\[2=(X^2+2X+2)-(X+2)X.\\] We also have that \\[X+2=(X^3+3X^2+4)-(X^2+2X+2)(X+1),\\] and thus, combining the two expressions, \\[\\begin{aligned} 2 &amp; = (X^2+2X+2)-\\left((X^3+3X^2+4)-(X^2+2X+2)(X-1)\\right)X = \\\\ &amp; = (X^2+2X+2)-(X^3+3X^2+4)X + (X^2+2X+2)(X^2+X) = \\\\ &amp; = (-X)(X^3+3X^2+4)+(X^2+X+1)(X^2+2X+2). \\end{aligned}\\] Multiplying both sides by \\(2^{-1}\\bmod{5}=3\\), we have that \\[\\begin{aligned} 1 &amp; \\equiv (-3X)(X^3+3X^2+4)+(3X^2+3X+3)(X^2+2X+2) \\equiv \\\\ &amp; \\equiv (2X)(X^2+3X^2+4)+(3X^2+3X+3)(X^2+2X+2). \\end{aligned}\\] Therefore, \\[U(X)=2X,\\qquad\\qquad V(X)=3X^2+3X+3.\\] As in the case of integers, we can compute the inverse of \\(A(X)\\) in \\(\\mathbb{F}_{p^k}\\) by running the Extended euclidean algorithm on \\(A(X)\\) and the irreducible polynomial \\(T(X)\\), obtaining some polynomials \\(U(X),V(X)\\) such that \\[1=A(X)U(X)+T(X)V(X),\\] thus \\[1\\equiv A(X)U(X)\\pmod{T(X)},\\] and we find that \\(U(X)\\) is the inverse of \\(A(X)\\) in \\(\\mathbb{F}_{p^k}\\). We show to define and manipulate finite fields in Sage. # Finite field of size 3^5. The optional argument modulus=p # allows to specify the irreducible polynomial p to be used. F = GF(3^5, &quot;X&quot;) F # Get the polynomial p(X)=X into the field. X = F(&quot;X&quot;) # Show the irreducible polynomial being used. F.modulus() # Define two elements of F and operate with them. a = F(X^4+X^2+2*X) b = F(2*X^2+2*X^3+1) # Addition print(&quot;a + b = &quot;+str(a+b)) # Multiplication print(&quot;a * b = &quot;+str(a*b)) # Inversion print(&quot;a^{-1} = &quot;+str(a^(-1))) Solved exercises Exercise 7.5 Consider the group \\((\\mathbb{Z}_7^*, \\cdot)\\). How many subgroups does it have? What are the generators of each? Consider \\((\\mathbb{Z}^*_{127}, \\cdot)\\). Does it have a subgroup of size \\(2\\)? If so find a generator of it. Solution. First, let’s use the Lagrange theorem to see what we should expect. The multiplicative group \\(\\mathbb{Z}_7^*\\) has \\(\\phi(7)=7-1=6\\) elements. Therefore its order is \\(ord(\\mathbb{Z}_7^*)=6\\) and all its subgroups should divide its order. This means it should have subgroups of orders \\(0,1,2,3,6\\). Let’s take each element and see the subgroup it generates. We denote with blue the smallest exponent that gives \\(1\\). Note that this exponent is the order of the group. \\[ \\begin{aligned} 1:\\ &amp;1^{\\color{blue}{1}} \\equiv 1 \\pmod 7 \\\\ 2:\\ &amp;2^{1} \\equiv 2 \\pmod 7 \\\\ &amp;2^{2} \\equiv 4 \\pmod 7 \\\\ &amp;2^{\\color{blue}{3}} \\equiv 8 \\equiv 1 \\pmod 7 \\\\ 3:\\ &amp;3^{1} \\equiv 3 \\pmod 7 \\\\ &amp;3^{2} \\equiv 9 \\equiv 2 \\pmod 7 \\\\ &amp;3^{3} \\equiv 2 \\cdot 3 \\equiv 6 \\equiv -1 \\pmod 7 \\\\ &amp;3^{\\color{blue}{6}} \\equiv 3^3\\cdot 3^3 \\equiv (-1)\\cdot (-1) \\equiv 1 \\pmod 7 \\\\ 4:\\ &amp;4^{1} \\equiv 4 \\pmod 7 \\\\ &amp;4^{2} \\equiv 16 \\equiv 2 \\pmod 7 \\\\ &amp;4^{\\color{blue}{3}} \\equiv 2\\cdot 4 \\equiv 8 \\equiv 1 \\pmod 7 \\\\ 5:\\ &amp;5^{1} \\equiv 5 \\pmod 7 \\\\ &amp;5^{2} \\equiv 25 \\equiv 4 \\pmod 7 \\\\ &amp;5^{3} \\equiv 4\\cdot 5 \\equiv 20 \\equiv -1 \\pmod 7 \\\\ &amp;5^{\\color{blue}{6}} \\equiv 5^{3}5^{3}\\equiv (-1) \\cdot (-1) \\equiv 1 \\pmod 7 \\\\ 6:\\ &amp;6^{1} \\equiv 5 \\equiv -1 \\pmod 7 \\\\ &amp;6^{\\color{blue}{6}} \\equiv (-1) \\cdot (-1)\\equiv 1 \\pmod 7 \\\\ \\end{aligned} \\] We next list the groups. We have: A subgroup of size \\(1\\). Its elements are \\(\\{1\\}\\) and it has one generator, \\(1\\). A subgroup of size \\(2\\). Its elements are \\(\\{1,6\\}\\) and it has one generator, \\(6\\). A subgroup of size \\(3\\). Its elements are \\(\\{1,2,4\\}\\) and it has two generators, \\(2,4\\). A subgroup of size \\(6\\) (note this is \\(\\mathbb{Z}^*_7\\)). Its elements are \\(\\{1,2,3,4,5,6\\}\\) and it has two generators, \\(3,5\\). The first question can be answered via the Lagrange theorem. The number \\(127\\) is a prime, so \\(\\phi(127)=126\\) and \\(2 \\mid 126\\). Therefore it indeed has a subgroup of size 2. To find a generator, instead of going over many possibilities we will use a trick. We can write \\(126\\equiv 127-1\\equiv -1 \\pmod{127}\\). But \\((-1)^2\\equiv 1\\pmod{127}\\), so \\(126\\) is th generator we look for. Exercise 7.6 Compute \\(7^{321172} \\pmod{15}\\). Solution. We can use in theory solve this with fast exponentiation. However, this will require unnecessarily many exponentiations. We can do much better using group theory. Specifically, we utilize Euler’s theorem. We consider the group \\((\\mathbb{Z}_{15}^*, \\cdot)\\). Recall that the theorem states that for all \\(x\\in\\mathbb{Z}_{15}\\), \\(x^{|\\mathbb{Z}_n^*|}=1\\). We start by computing the size of the group. This is the group of invertible elements \\(\\mod 15\\) and we know that the number of these elements is \\[ \\phi(15) = \\phi(3\\cdot 5) = \\phi(3)\\cdot\\phi(5) = 2 \\cdot 4 = 8 \\] Next, we compute the Euclidian division of \\(321172\\) with \\(15\\). This gives \\[ 321172 = 40146\\cdot 8 + 4 \\] and we can now write \\[ 7^{321172} \\equiv 7^{40146\\cdot 8 + 4} \\equiv (7^8)^{40146}\\cdot 7^4 \\equiv 7^4\\pmod{15} \\] where the last equality holds since \\(7^8\\equiv 1\\pmod{15}\\). Now the operation is quite simple! We have \\[ \\begin{aligned} 7^2 &amp;\\equiv 49 \\equiv 4 \\pmod{15} \\\\ 7^4 &amp;\\equiv 7^2\\cdot 7^2 \\equiv 4\\cdot 4 \\equiv 1 \\pmod{15} \\\\ \\end{aligned} \\] Therefore \\(7^{321172} \\equiv 1 \\pmod{15}\\). Exercise 7.7 Consider the group \\((\\mathbb{Z}_{15}^*,\\cdot)\\). Which are its elements? Is the group cyclic? Solution. The group consists of the invertible elements \\(\\mod 15\\), that is the set \\(\\{x \\in \\{0,\\ldots, 14\\} | \\text{ s.t. } gcd(x,15) = 1\\}\\). Since 15 has two factors, 3 and 5, we look all the elements that are not divided by both. By inspection, the set is \\[ \\{1, 2, 4, 7, 8, 11, 13, 14\\} \\] The simplest way to solve this exercise is to compute the order of each elements and check if there is an element of order \\(|\\mathbb{Z}_{15}^*|=8\\). To make the computations easier, we will use the Lagrange theorem, which states that for all \\(g\\in\\mathbb{G}\\), \\(ord(g)|ord(\\mathbb{G})\\). In the case \\(\\mathbb{G}=\\mathbb{Z}_{15}^*\\), the order of the group is 8. Therefore, each element can have order \\(1, 2, 4\\) or \\(8\\). Let’s now compute the order for each: \\[ \\begin{aligned} 1:\\ &amp;1^{\\color{blue}{1}} \\equiv 1 \\pmod {15} \\\\ 2:\\ &amp;2^{1} \\equiv 2 \\pmod {15} \\\\ &amp;2^{2} \\equiv 4 \\pmod {15} \\\\ &amp;2^{\\color{blue}{4}} \\equiv {4}\\cdot 4 \\equiv 16 \\equiv 1\\pmod {15} \\\\ 4:\\ &amp;4^{1} \\equiv 4 \\pmod {15} \\\\ &amp;4^{\\color{blue}{2}} \\equiv 4\\cdot 4 \\equiv 16 \\equiv 1 \\pmod {15} \\\\ 7:\\ &amp;7^{1} \\equiv 7 \\pmod {15} \\\\ &amp;7^{2} \\equiv 49 \\equiv 4 \\pmod {15} \\\\ &amp;7^{\\color{blue}{4}} \\equiv 4\\cdot 4 \\equiv 16\\cdot 1 \\pmod {15} \\\\ 8:\\ &amp;8^{1} \\equiv 8 \\pmod {15} \\\\ &amp;8^{2} \\equiv 64 \\equiv 4 \\pmod {15} \\\\ &amp;8^{\\color{blue}{4}} \\equiv 4\\cdot 4 \\equiv 16\\equiv 1 \\pmod {15} \\\\ 11:\\ &amp;11^{1} \\equiv 11 \\equiv -4 \\pmod {15} \\\\ &amp;11^{2} \\equiv (-4)\\cdot (-4) \\equiv 16 \\equiv 1 \\pmod {15} \\\\ 13:\\ &amp;13^{1} \\equiv 13 \\equiv -2 \\pmod {15} \\\\ &amp;13^{2} \\equiv (-2)\\cdot (-2) \\equiv 4\\pmod {15} \\\\ &amp;13^{\\color{blue}{4}} \\equiv 13^{2}\\cdot 13^{2} \\equiv 4\\cdot 4 \\equiv 16 \\equiv 1\\pmod {15} \\\\ 14:\\ &amp;14^{1} \\equiv 14 \\equiv -1 \\pmod {15} \\\\ &amp;14^{\\color{blue}{2}} \\equiv (-1)\\cdot (-1) \\equiv 1\\pmod {15} \\\\ \\end{aligned} \\] Therefore, no element has order 8 and the group \\(\\mathbb{Z}_{15}^*\\) is not cyclic. Exercise 7.8 What is the largest subgroup of \\(\\mathbb{Z}_{73}^*\\) that has a prime order? Find a generator of it. Let \\(p,q\\) be prime numbers. When does \\(\\mathbb{Z}_p^*\\) has a subgroup of size \\(q\\)?. For what types of primes \\(p\\) are these subgroups maximal in size? Consider \\(p,q\\) as above, that is, \\(\\mathbb{Z}_p*\\) has a subgroup \\(\\mathbb{H}\\) of size \\(q\\). How many generators does \\(\\mathbb{H}\\)? Solution. We know that \\(\\mathbb{Z}_{73}^*\\) has \\(\\phi(73)=72\\) elements. Also, by the Lagrange theorem, we know that the order of every subgroup must divide the order of the group. In this case, we have that \\(72= 2\\cdot 2\\cdot 2 \\cdot 3 \\cdot 3\\). The only possibilities for a subgroup of prime order are \\(2\\) and \\(3\\). Therefore, the biggest prime order subgroup of \\(\\mathbb{Z}_{73}^*\\) has three elements. Let’s find the group and a generator. We need to find any group element \\(x\\) such that \\(x^2\\neq 1 \\mod 73\\) and \\(x^2= 1 \\mod 73\\). We do this through trial and error. \\[ \\begin{aligned} 2:\\ &amp;2^{3}\\equiv 8 \\not\\equiv 1 \\pmod {73} \\\\ 3:\\ &amp;3^{3}\\equiv 27 \\not\\equiv 1 \\pmod {73} \\\\ 4:\\ &amp;4^{3}\\equiv 64 \\not\\equiv 1 \\pmod {73} \\\\ 5:\\ &amp;5^{3}\\equiv 125 \\equiv 52 \\not\\equiv 1 \\pmod {73} \\\\ &amp;\\qquad \\vdots \\\\ 8:\\ &amp;8^{3}\\equiv 64\\cdot 8 \\equiv -9\\cdot 8 \\equiv -72 \\equiv 1 \\pmod {73} \\\\ \\end{aligned} \\] Our candidate is 8. We need to check that its order is indeed 3. This is true because \\(8^2\\equiv 64\\not\\equiv 1 \\pmod{73}\\). So \\(8\\) is the generator of a size 3 subgroup. The group elements are \\(\\{1, 8, 64\\}\\). We know that \\(\\mathbb{Z}_p^*\\) has order \\(\\phi(p)=p-1\\). Also, by the Lagrange theorem we know that the order of any subgroup \\(\\mathbb{H}\\) must divide the order of the group. Therefore, \\(\\mathbb{Z}_p^*\\) has a subgroup of size \\(q\\) if and only if \\(q | p-1\\) or equivalently \\(p= k \\cdot q + 1\\). The subgroup’s size gets smaller as \\(k\\) grows. Therefore, the size is maximal when \\(k=2\\) (note that \\(k=1\\) implies \\(p=q+1\\) but no pair of primes -apart \\((2,3)\\)- can satisfy this condition), that is for \\(p,q\\) that satisfy \\(p=2q+1\\). A prime number \\(q\\) for which \\(2q+1\\) is also a prime is called a Sophie_Germain. The order of the subgroup \\(\\mathbb{H}\\) is \\(q\\) and \\(q\\) is a prime number. This means that it is only divisible by \\(1\\) and by \\(q\\). The only element of a group that can have order one is the identity element. Therefore, all elements of \\(\\mathbb{H}\\) apart from the identity have order \\(q\\) and are generators of \\(\\mathbb{H}\\). We conclude that \\(\\mathbb{H}\\) has \\(q-1\\) generators. Remark. In the above exercise, the existence of primes \\(p\\) and \\(q\\) can be confusing. Let’s clarify a bit. In both the groups \\(\\mathbb{Z}_p^*\\) and \\(\\mathbb{H}\\leq \\mathbb{Z}_p^*\\), the group operation is multiplication \\(\\mod p\\). Think of it this way: in the world of \\(\\mathbb{Z}_p^*\\), only operations \\(\\mod p\\) are applicable. When we are in the subgroup \\(\\mathbb{H}\\), we are still on the same world, we just take a subset of \\(q\\) elements of this world but the operations we do on them do not change! Also note that all elements of the subgroup \\(\\mathbb{H}\\) belong in the set \\(\\{1,\\ldots, p-1\\}\\) and not \\(\\{1,\\ldots, q-1\\}\\)! Exercise 7.9 (*) Show that for any group \\((\\mathbb{G}, \\circ)\\) and any \\(a,b\\in\\mathbb{G}\\), the equation \\(a = b\\circ x\\) has a unique solution. Consider the group \\((\\mathbb{Z}_n, +)\\). Construct an encryption scheme with perfectly secrecy using only addition \\(\\mod n\\). What happens if you use the same key twice? Generalize the above construction to work with any group \\(\\mathbb{G}\\) assuming that the group operation and the computation of an inverse element can be efficiently computed. Solution. First, we show there exists a solution. Since \\(\\mathbb{G}\\) is a group, \\(b\\) has an inverse element, i.e. there exists an element \\(c\\) s.t. \\(b\\circ c=e\\) where \\(e\\) denotes the identity element. We claim \\(x = c\\circ a\\) is a solution. Indeed we have \\[ b\\circ(c\\circ a) = (b\\circ c)\\circ a = e\\circ a = a \\] The first equality holds by the associative lay, the second by the existence of the inverse and the third by the identity property. Now, assume there are two solution, \\(x_1, x_2\\) we have \\(a=bx_1=b\\circ x_2\\) which means that \\(b\\circ x_1 = b\\circ x_2\\). Multiplying both sides on the left with \\(c\\) we get \\[ c\\circ(b\\circ x_1) = c\\circ(b\\circ x_1) \\Leftrightarrow (c\\circ b)\\circ x_1 = (c\\circ b)\\circ x_2 \\Leftrightarrow e\\circ x_1 = e\\circ x_2 \\Leftrightarrow x_1 = x_2 \\] therefore the solution must be unique. We will encrypt elements by adding \\(\\mod n\\) and decrypt by subtracting (i.e. inverting and adding) \\(\\mod n\\). Concretely, the message space is \\(\\mathbb{Z}_n\\) and the key is a uniformly distributed element of \\(\\mathsf{k}\\gets\\mathbb{Z}_p\\). To encrypt a message \\(m\\in\\mathbb{Z}_n\\) we compute \\(\\mathsf{c}:= \\mathsf{m}+\\mathsf{k}\\mod n\\) and to decrypt we compute \\(\\mathsf{m}:= \\mathsf{c}+(-\\mathsf{k})\\mod n\\). We next show that the correctness and perfect secrecy properties hold. Correctness: We have \\[ \\mathsf{Dec}_\\mathsf{k}(\\mathsf{Enc}_\\mathsf{k}(\\mathsf{m})) = \\mathsf{Dec}_\\mathsf{k}(\\mathsf{m}+\\mathsf{k} ) = \\mathsf{m}+\\mathsf{k} + (-\\mathsf{k}) = \\mathsf{m} \\] Perfect secrecy: We need to show that for any fixed \\(\\mathsf{c}\\), \\(\\mathsf{m}_0\\) and \\(\\mathsf{m}_1\\), the probability (over the choice of the key) that \\(\\mathsf{c}\\) is an encryption of \\(\\mathsf{m}_0\\) is equal to the probability (over the choice of the key) that \\(\\mathsf{c}\\) is an encryption of \\(\\mathsf{m}_1\\). Consider any pair \\(\\mathsf{c}, \\mathsf{m}\\) and the equation \\(\\mathsf{c} = \\mathsf{m} + x\\). By the previous question, this has a unique solution. Equivalently there exists a unique \\(\\mathsf{k}\\) s.t. \\(\\mathsf{c} = \\mathsf{Enc}_{\\mathsf{k}}(\\mathsf{m})\\). Since the key is chosen at random, we conclude that for all \\(\\mathsf{m},\\mathsf{c}\\), \\(\\Pr_{\\mathsf{k}}[\\mathsf{c} = \\mathsf{Enc}_{\\mathsf{k}}(\\mathsf{m})]=1/|\\mathbb{Z_n}| = 1/n\\) and therefore \\[ \\Pr_{\\mathsf{k}}[\\mathsf{c} = \\mathsf{Enc}_{\\mathsf{k}}(\\mathsf{m_0})] = 1/n = \\Pr_{\\mathsf{k}}[\\mathsf{c} = \\mathsf{Enc}_{\\mathsf{k}}(\\mathsf{m_1})] \\] which concludes the proof. Assume we use the same key twice, that is we see two ciphertexts \\(\\mathsf{c}_1, \\mathsf{c}_2\\) corresponding to some messages \\(\\mathsf{m}_1, \\mathsf{m}_2\\). This means that for some \\(\\mathsf{k}\\) we have \\[ \\mathsf{c}_1 = \\mathsf{m}_1 + \\mathsf{k},\\qquad \\mathsf{c}_2 = \\mathsf{m}_2 + \\mathsf{k} \\] Subtracting the two relations we get \\[ \\mathsf{c}_1 - \\mathsf{c}_2 = (\\mathsf{m}_1 + \\mathsf{k})- (\\mathsf{m}_2 + \\mathsf{k})= \\mathsf{m}_1 - \\mathsf{m}_2 \\] therefore an attacker can learn the \\(\\mathsf{m}_1 - \\mathsf{m}_2\\) by only looking the ciphertexts. This question can be answered by making a simple -albeit crucial- observation. In the previous question we did not use any property specific to addition \\(\\mod n\\)! We relied solely on the properties of the group. This means that after replacing \\(\\mathbb{Z}_n\\) with any group \\(\\mathbb{G}\\) and addition mod \\(n\\) with the group operations, we can follow the same arguing! Concretely we sample a random group element \\(\\mathsf{k}\\in\\mathbb{G}\\) as the encryption key34 and define encryption and decryption as \\[ \\mathsf{Enc}_{\\mathsf{k}}(\\mathsf{m}) = \\mathsf{m}\\circ\\mathsf{k},\\qquad \\mathsf{Dec}_{\\mathsf{k}}(\\mathsf{c}) = \\mathsf{c}\\circ\\mathsf{k^{-1}},\\qquad \\] where \\(\\mathsf{k}^{-1}\\) denotes the inverse of \\(\\mathsf{k}\\). Correctness and perfect secrecy hold by the same argument we used for \\((\\mathbb{Z}_n, +)\\). Finally, using the same key for two messages leaks the information \\(\\mathsf{m}_1 \\circ \\mathsf{m}_2^{-1}\\) Remark. Consider the original one-time-pad defined in the group \\((\\mathbb{Z}_2^k, +)\\) where \\(\\mathbb{Z}_2^k = \\mathbb{Z}_2\\times\\mathbb{Z}_2\\times\\ldots\\times\\mathbb{Z}_2\\). Noting that addition modulo 2 is the xor operation, we can in fact see that this is an instantiation of the above. Also noting that the inverse of \\(x\\in\\mathbb{Z}_2^k\\) is itself, we get that \\(\\mathsf{m}_1 \\circ \\mathsf{m}_2^{-1} = \\mathsf{m}_1 \\oplus \\mathsf{m}_2\\). Exercise 7.10 Let \\(\\mathbb G=\\langle g\\rangle\\) be a cyclic group with generator \\(g\\). Describe an algorithm that on input \\(h\\in\\mathbb{G}\\) computes its inverse \\(h^{-1}\\). Is your algorithm efficient? Solution. Our goal is to find an element \\(h&#39;\\in\\mathbb{G}\\) such that \\(h\\cdot h&#39;=1\\) where \\(1\\) is the identity of \\(\\mathbb{G}\\). Consider the subgroup \\(\\langle h\\rangle\\). The element \\(h&#39;=h^{j}\\) should also belong in this subgroup (every element of a group has an inverse and since \\(h\\in\\langle h\\rangle\\), its inverse must also be on the group \\(\\langle h\\rangle\\)). We can write \\(h=h^1\\), \\(h&#39;=h^k\\) and by Euler’s theorem \\(1 = h^{\\text{ord}(h)}\\). Therefore, our equation becomes \\[ h^{1}\\cdot h^{k} = h^{\\text{ord}(h)} \\Leftrightarrow h^{k} = h^{\\text{ord}(h)-1} \\] which means that \\(k = \\text{ord}(h) - 1\\). Our algorithm does the following: Compute the value \\(\\text{ord}(h)\\). Compute the value \\(k = \\text{ord}(h)-1\\) Compute the value \\(h^{-1} = h^k\\) The last two steps can be implemented efficiently. For the second, it is evident and the third can be implemented similar to how modular fast exponentiation is implemented35. What about the first one? This is in fact unclear. In some groups there are efficient algorithms while in others the problem is assumed to be hard. Therefore the algorithm is efficient iff we apply it in a group \\(\\mathbb{G}\\) for which computing the order of group elements is efficient. A prime example of a different group operation, which appears very often in cryptography, is the group law of elliptic curves.↩︎ Not to be confused with Fermat’s last theorem, which states that there are no positive integer solutions for the equation \\(x^n+y^n=z^n\\) for any integer \\(n&gt;2\\). This theorem has become famous due to its history: it was originally claimed by Pierre de Fermat in the 17th century. He wrote it in the margin of a book, claiming to know a proof but lacking the space to write it down. Such proof was never found, and the first known proof of the theorem appeared more than three centuries later, in the 1990s, when Andrew Wiles finally solved the problem, after the contributions of a long lineage of mathematicians.↩︎ Appendix A contains a more detailed introduction to this section. It first defines another algebraic structure called a ring, which is somewhat between a group and a field, and then defines fields as a particular type of ring. This appendix is not part of the content for the exam, and the main body of these notes is designed to be self-contained, but nevertheless we advise you to read Appendix A before reading about fields, since it provides a more natural introduction to the topic.↩︎ Sometimes also called Galois fields, after the mathematician Évariste Galois, who notably proved that there is no algebraic formula for solving polynomial equations of degree higher than \\(4\\), and then died after a duel at the age of 20.↩︎ See Appendix D.4 for a refresher on how to perform polynomial division.↩︎ Technically, we need to also assume that there exists an efficient algorithm that samples (close to) uniformly distributed elements of \\(\\mathbb{G}\\).↩︎ One can write \\(k=\\sum_i b_i 2^{i}\\) where \\(b_i\\in\\{0,1\\}\\), compute all powers \\(h^{2^i}\\) and rewrite \\[ h^{k} = h^{\\sum_i b_i 2^{i}} = \\prod h^{b_i 2^{i}}. \\] Since the bit length of \\(k\\leq \\text{ord}(\\mathbb{G})\\) is at most \\(\\mathcal{O}(\\log\\text{ord}(\\mathbb G))\\), the exponentiation has complexity \\(\\mathcal{O}(\\log\\text{ord}(\\mathbb G))\\).↩︎ "],["public-key-encryption.html", "8 Public-key encryption 8.1 Public-key cryptography 8.2 The RSA encryption scheme 8.3 Security of RSA 8.4 Efficiency optimizations Solved exercises", " 8 Public-key encryption Equipped with the mathematical tools developed in the previous two chapters, we are now in a position to introduce the concept of public-key cryptography, and present some of the best-known constructions in this setting. More precisely, we will learn about: The paradigm of public-key cryptography. The RSA encryption scheme and its security. 8.1 Public-key cryptography In Section 1, we introduced a symmetric encryption scheme, in which two parties use a shared secret key to communicate privately. We still have not solved the problem of establishing this common key in a secure way. We will now introduce a new type of encryption scheme, which stems from the following idea: what if we don’t need a shared secret key to have secure communications? This idea was introduced by Diffie and Hellman in the renowned paper New directions in cryptography,36 which is considered to be the birth of modern cryptography. They introduced the notion of asymmetric or public-key encryption, which on a high level works as follows. Imagine that Alice wishes to send a message to Bob. Bob produces two keys \\(\\mathsf{pk}\\) and \\(\\mathsf{sk}\\), crafted in such a way that whatever is encrypted with \\(\\mathsf{pk}\\) can be decrypted only with \\(\\mathsf{sk}\\). That is, for any message \\(\\mathsf{m}\\), \\[\\mathsf{Dec}_{\\mathsf{sk}}\\left(\\mathsf{Enc}_{\\mathsf{pk}}(\\mathsf{m})\\right)=\\mathsf{m}.\\] Then Bob publishes \\(\\mathsf{pk}\\), so that anyone can know it, and keeps \\(\\mathsf{sk}\\) secret. For this reason, \\(\\mathsf{pk}\\) is known as Bob’s public key and \\(\\mathsf{sk}\\) is Bob’s secret key. Definition 8.1 An asymmetric (or public-key) encryption scheme is composed of three efficient algorithms: \\[(\\mathsf{KeyGen},\\mathsf{Enc},\\mathsf{Dec}).\\] The \\(\\mathsf{KeyGen}\\) algorithm chooses two keys \\(\\mathsf{pk},\\mathsf{sk}\\) of length \\(\\lambda\\), according to some probability distribution, and such that \\[\\mathsf{Dec}_{\\mathsf{sk}}\\left(\\mathsf{Enc}_{\\mathsf{pk}}(\\mathsf{m})\\right)=\\mathsf{m}.\\] The \\(\\mathsf{Enc}\\) algorithm uses the public key \\(\\mathsf{pk}\\) to encrypt a message \\(\\mathsf{m}\\), and outputs the encrypted message \\[\\mathsf{c}=\\mathsf{Enc}_{\\mathsf{pk}}(\\mathsf{m}).\\] The \\(\\mathsf{Dec}\\) algorithm uses the secret key \\(\\mathsf{sk}\\) to decrypt an encrypted message \\(\\mathsf{c}\\), recovering \\(\\mathsf{m}\\) as \\[\\mathsf{Dec}_{\\mathsf{sk}}(\\mathsf{c})=\\mathsf{m}.\\] In this context, \\(\\mathsf{m}\\) is called the plaintext, and \\(\\mathsf{c}\\) is said to be its corresponding ciphertext. The keys \\(\\mathsf{pk}\\) and \\(\\mathsf{sk}\\) are the public key and secret key, respectively. Note that, for such a construction to be secure, we need that the secret key cannot be efficiently computed from the public key. Otherwise, since the public key is known to everybody, in particular attackers, this could be exploited to recover the secret key and decrypt any message. Another difference with symmetric encryption is that the structure of the keys is different. In symmetric encryption, we had a key associated to two parties, Alice and Bob, which was used to send messages both ways. But, in the explanation above, we just described how Alice sends messages to Bob, but not the other way around. Note that if Bob tried to use \\(\\mathsf{sk}\\) to encrypt, with the hope that Alice decrypts with \\(\\mathsf{pk}\\), then anybody would be able to decrypt, since \\(\\mathsf{pk}\\) is public. Therefore, Alice needs another pair of keys, one public that is used for everybody else to encrypt messages to Alice, and one secret, that is used by Alice to decrypt messages addressed to her. This might seem like a downgrade, since before we needed only one key and now we have four in total. We emphasize, however, that none of the secret keys need to be shared, and the public ones can be shared through an insecure channel. Moreover, we actually have less keys in the asymmetric case when many parties are involved, as is highlighted by the following exercise. Exercise 8.1 Suppose that we have \\(n\\) parties, and each of them wishes to communicate with all the others. Compute how many keys are needed if they use: A symmetric encryption scheme. A public-key encryption scheme. 8.2 The RSA encryption scheme Although Diffie and Hellman introduced the idea of public-key encryption in 1976, it was not until 1978 that Rivest, Shamir and Adleman published the first public-key encryption scheme, which became known as the RSA encryption scheme.37 The scheme works as follows. \\(\\mathsf{KeyGen}\\): on input a security parameter \\(\\lambda\\), choose two uniformly random prime numbers \\(p,q\\) of bitlength \\(\\lambda/2\\), and let \\(N=pq\\). We will work in \\(\\mathbb{Z}_N\\), and call \\(N\\) an RSA modulus. Choose \\(e\\in\\mathbb{Z}_N\\), and compute \\[d\\equiv e^{-1}\\pmod{\\varphi(N)}.\\] Output the public key \\[\\mathsf{pk}=(N,e),\\] and the secret key \\[\\mathsf{sk}=d.\\] Note that it is crucial that \\(p,q\\) are uniformly random, whereas \\(e\\) can be a fixed parameter.38 \\(\\mathsf{Enc}\\): given a message \\(\\mathsf{m}\\in\\mathbb{Z}_N\\), and the receiver’s public key \\((N,e)\\), output a ciphertext \\[\\mathsf{m}^{e}\\bmod{N}.\\] \\(\\mathsf{Dec}\\): given a ciphertext \\(\\mathsf{c}\\) and the secret key \\(d\\), output \\[\\mathsf{c}^{d}\\bmod{N}.\\] There are a few things to consider here. The first is, why does this even work? That is, how can we be sure that the original message is recovered after encryption and decryption. We observe that, given a message \\(\\mathsf{m}\\), we have that \\[\\mathsf{Dec}_{\\mathsf{sk}}(\\mathsf{Enc}_{\\mathsf{pk}}(\\mathsf{m}))=\\mathsf{Dec}_{\\mathsf{sk}}(\\mathsf{m}^e\\bmod{N})=\\left(\\mathsf{m}^{e}\\right)^d\\bmod{N}=\\mathsf{m}^{ed}\\bmod{N}.\\] Now, we use that \\[d\\equiv e^{-1}\\pmod{\\varphi(N)},\\] which means that there is an integer \\(k\\) such that \\[de=k\\varphi(N)+1.\\] Thus, by plugging this into the expression above, we have \\[\\mathsf{m}^{ed}\\bmod{N}=\\mathsf{m}^{k\\varphi(N)+1}\\bmod{N}=\\left(\\left(\\mathsf{m}^{\\varphi(N)}\\right)^k\\cdot\\mathsf{m}\\right)\\bmod{N}.\\] Finally, we use Euler’s theorem (Proposition 7.3) and Proposition 7.5, which tell us that \\[\\mathsf{m}^{\\varphi(N)}\\bmod{N}=1,\\] and therefore \\[\\mathsf{m}^{ed}\\bmod{N}=\\mathsf{m}.\\] This proves that decryption indeed reverses encryption. A second consideration is: are the three algorithms involved efficient? It is easy to see that \\(\\mathsf{Enc}\\) and \\(\\mathsf{Dec}\\) are efficient, since they amount to one modular exponentiation each, which we have seen that is an efficient computation (Section 6.4). Let us analyze the \\(\\mathsf{KeyGen}\\) algorithm by breaking it into these steps: Sample prime numbers of size \\(\\lambda\\). Compute \\(N=pq\\). Compute \\(\\varphi(N)\\). Compute the inverse of \\(e\\) modulo \\(\\varphi(N)\\). Step (1) can be further broken into two parts: sample a random number of length \\(\\lambda\\), and recognize whether it is a prime or not. We know we can sample random numbers efficiently and, as discussed in Section 6.1 and Appendix B, there are efficient algorithms to determine the primality of a number.39 So the strategy is to sample random numbers until we find a prime. But how many tries do we need? The prime number theorem (Proposition 6.3) tells us that, for large numbers, the amount \\(\\pi(n)\\) of primes up to \\(n\\) is roughly \\(n/\\log n\\) which means that the probability of a random number being a prime is approximately \\(1/\\log n\\). This means that, on average, we will need \\(\\log n=O(\\lambda)\\) tries before finding a prime. Thus, the total cost of step (1) is \\(O(\\lambda^3)\\). Step (2) is simple arithmetic, which is efficient. Computing \\(\\phi(N)\\) in step (3) is easy when knowing the factorization of \\(N\\) since, if \\(N=pq\\), then \\[\\varphi(N)=(p-1)(q-1),\\] as a consequence of Proposition 6.8. Finally, step (4) can be performed efficiently using the Euclidean algorithm. The following Sage code is a very simple implementation of the three algorithms composing the RSA encryption scheme. # Set a security parameter sec_param = 1024 ### KEY GENERATION # Generate two prime numbers of length sec_param/2 p = random_prime(2^(sec_param/2-1),2^(sec_param/2)-1) q = random_prime(2^(sec_param/2-1),2^(sec_param/2)-1) # Set the RSA modulus: N = p*q # Compute Euler&#39;s phi function on N: phi = (p-1)*(q-1) # Define Z_N, so that all operations are # automatically reduced modulo N. Z = Integers(N) # Choose a public key: e = 2^16 + 1 # Compute the corresponding secret key: d = inverse_mod(e,phi) # Euclidean algorithm is used under the hood. ### ENCRYPTION - using pk = (N,e) # Choose a message to encrypt. m = 1766704380348666914344743843625136737766400008545151048420480921590988455650205660330488601346206061875826343297391617046317582074537509379708354843527043248265272066232991516996098399031098469466906571566159583240626991926476312991813498057350788070826660741984814907490494865792846706478975249596131279388 # Check that the message fits in Z_N. if (m &gt;= N): print(&quot;Message too large.&quot;) else: # Encrypt the message c = Z(m)^e # Z(m) is written instead of m so that Sage recognizes m # as an element of Z_N, and performs operations modulo N. print(&quot;c = &quot;+str(c)) ### DECRYPTION - using sk = d m = Z(c)^d print(&quot;m = &quot;+str(m)) Exercise 8.2 In the code above, can we replace the line about computing \\(\\varphi(N)\\) by phi = euler_phi(N)? 8.3 Security of RSA So we know that the scheme works and is efficient. It remains to discuss security. As mentioned above, the secret key should be hard to deduce from the public key, otherwise anyone would have access to it, and thus anyone would be able to decrypt. By looking again at the generation of the secret key in \\(\\mathsf{KeyGen}\\), we observe that it can be computed from \\(e\\) and \\(\\varphi(N)\\). The parameters \\(e\\) and \\(N\\) are public, so what prevents attackers from computing the secret key? The crucial point is that, in \\(\\mathsf{KeyGen}\\), we were able to compute \\(\\varphi(N)\\) from the factorization \\((p,q)\\) of \\(N\\), by computing \\[\\varphi(N)=(p-1)(q-1).\\] However, if we do not know the factors of \\(N\\), we cannot carry out this computation. Moreover, this works both ways: it can be shown that knowledge of \\(\\varphi(N)\\) allows to factor \\(N\\) efficiently. Thus, the security of RSA relies on the hardness of factorization. Definition 8.2 Let \\(p,q\\) be large prime numbers, and let \\(N=pq\\). The factorization problem consists of recovering \\(p,q\\), given \\(N\\). As mentioned in Section 6.1, there is no known algorithm for factoring a product of two large primes efficiently. Hardness of factorization is a necessary condition for security but, unfortunately, not a sufficient one. That is, an adversary could still in principle decrypt a ciphertext without the need of the secret key, with some other technique. This motivates the introduction of the following problem. Definition 8.3 Let \\(p,q\\) be large primes, and let \\(N=pq\\). Let \\(e\\in\\mathbb{Z}_N\\). The RSA problem consists of recovering \\(\\mathsf{m}\\in\\mathbb{Z}_N\\), given \\(N,e\\) and \\(\\mathsf{m}^e\\bmod{N}\\). Clearly if factorization is easy then the RSA problem is also easy, but the implication in the other direction is not known to be true or false so far. However, as is the case with the factorization problem, there have been extensive attacks against the RSA problem, and no better attack than factorization of \\(N\\) has been found. This provides a reasonable guarantee that the problem is indeed hard, even if we lack a formal proof. For security against current computational power, most organizations suggest a security parameter of at least \\(\\lambda=2048\\) (see https://www.keylength.com/). That is, an RSA modulus \\(N\\) of bitlength \\(2048\\) is believed to be secure against current factorization attempts. To date, the highest RSA modulus to be factored has bitlength \\(829\\), and took about \\(2700\\) core-years.40 Webcomic by xkcd (https://xkcd.com/538/). So is this enough to call the RSA scheme secure? Unfortunately, no, since it is vulnerable to other attacks that do not depend on recovering the secret key. We consider the following scenario: suppose that Alice is sending Bob a date of the year, in the format \\(DDMM\\), where \\(DD\\) is the day and \\(MM\\) is the month. An attacker knows this, and also has access to Bob’s public key \\((N,e)\\), since anyone can obtain public keys. The attacker eavesdrops the ciphertext \\(\\mathsf{c}\\) that Alice sends Bob, and then computes \\[(DDMM)^{e}\\bmod{N}\\] for each \\(DD\\in\\{1,\\dots,31\\}\\) and \\(MM\\in\\{1,\\dots,12\\}\\). The attacker compares the list of results with \\(\\mathsf{c}\\) until they find a match, which tells them the date that was encrypted in \\(\\mathsf{c}\\). This is known as a chosen-plaintext attack (CPA), since the adversary can obtain the encryptions of messages of their own choice. Below is some code for running this attack. As an attacker, we have access to the security parameter \\(\\lambda\\), the RSA modulus N, and the encryption exponent e. We intercept some ciphertext c, and run the attack by comparing c with the encryption of each possible message. ### Auxiliary function # Write day/month in the format DDMM def format(day,month): day = str(day) month = str(month) if len(day)&lt;2: day = &quot;0&quot; + day if len(month)&lt;2: month = &quot;0&quot; + month return int(day + month) ### CPA attack # Known data N = 5084923486342919837749158826454356403346569259981671106186333244915073155770076069992841087736392422153624652509603466938787643616193693073473157600021972806569653700645220307421997336878744077854611907151783228311349496598408945325528067737317894046858136344781889361465025184092329532181879347609645469411 Z = Integers(N) e = 2^16 + 1 # Intercepted ciphertext c = 105605073257617821289274662706975761114405451836859336022070629464826511380165185130471900685486223806100216503822026792593184983241872596132965357095523791203088732252839389155546759634409370409954409570109113362453081088475465706906936653890215874964553000419826116298403548222781967172784767448559073540 # Running the attack for month in range(1,13): for day in range(1,32): m = format(day,month) c_candidate = Z(m)^e if c_candidate == c: print(&quot;Recovered plaintext: &quot;+str(m)) The version of RSA described above is known as textbook RSA, because it is a version simplified for didactic purposes, but is not secure against chosen-plaintext attacks, and thus not secure for real-world use. How could such an attack happened? We boil it down to three facts: The receiver’s public key is known by the attacker, so the attacker can compute ciphertexts of messages of their choice. The set of possible messages is small, so it is efficient for the attacker to compute ciphertexts of every possible message. The encryption algorithm was deterministic, so the attacker can compare their list of ciphertexts with the intercepted ciphertext and find a match. Recall Principle 3 from the beginning of the course: there is no security without randomness. Fact (1) happens by design of public-key schemes. There is not much we can do about fact (2) either, since a good encryption scheme should allow users to communicate any data. Therefore, to fix RSA we need to do something about fact (3). The idea is to modify the message before running it through the RSA encryption algorithm, and add some randomness to it. There are many different proposals to achieve this, collectively known as padded RSA. We describe one successful variant known as RSA-OAEP.41 Unfortunately, it is not as simple as appending some random bits at the end of the message, and we need a more involved process. Let \\[G:\\{0,1\\}^{k_0}\\rightarrow\\{0,1\\}^{\\ell+k_1},\\qquad H:\\{0,1\\}^{\\ell+k_1}\\rightarrow \\{0,1\\}^{k_0}\\] be two hash functions, where \\(k_0,k_1\\) are such that \\(\\lambda=O(k_0)\\) and \\(\\lambda=O(k_2)\\), and \\(\\ell+k_0+k_1\\) is smaller than the bitlength of \\(N\\). We describe how to modify a message \\(\\mathsf{m}\\) of bitlength \\(\\ell\\). We introduce randomness by uniformly sampling a bitstring \\(r\\in\\{0,1\\}^{k_0}\\), and then compute: \\[s=\\mathsf{m}|\\mathbf 0^{k_1}\\oplus G(r),\\qquad t= r\\oplus H(s),\\] where \\(\\mathsf{m}|\\mathbf 0^{k_1}\\) is the bitstring \\(\\mathsf{m}\\), concatenated with the string of zeros of length \\(k_1\\). Then, we set the new message as \\(\\hat{\\mathsf{m}}=(s,t)\\), which is then run through textbook RSA. Observe that \\(s\\in\\{0,1\\}^{\\ell+k_1}\\) and \\(t\\in\\{0,1\\}^{k_0}\\), so the message \\(\\hat{\\mathsf{m}}\\) has the appropriate length. We summarize the construction in the following diagram. Exercise 8.3 Describe the decryption procedure that corresponds to RSA-OAEP. With these modifications, we can finally claim that the RSA cryptosystem is secure. Proposition 8.1 If the RSA problem is hard and \\(G\\) and \\(H\\) behave as ideal42 hash functions, then the RSA-OAEP encryption scheme is secure. Note that the textbook version of RSA is also malleable. This means that, given a ciphertext of some message, it is easy to produce a ciphertext of a related message. For example, given a ciphertext \\(\\mathsf{c}\\) for the message \\(\\mathsf{m}\\), i.e. \\[\\mathsf{c}=\\mathsf{m}^e\\bmod{N},\\] an adversary can compute \\[\\mathsf{c}&#39;=2^e\\mathsf{c}\\bmod{N}=(2\\mathsf{m})^e\\bmod{N},\\] which is a valid ciphertext for the message \\(2\\mathsf{m}\\bmod{N}\\). The OAEP transformation also makes the scheme non-malleable. 8.4 Efficiency optimizations As discussed above, the algorithms involved in RSA are all efficient, although not particularly fast. In this section, we look at some efficiency tricks to speed up the computations. A simple one is to choose the encryption exponent \\(e\\) so that its binary representation has many zeros. This has an impact on the computation of the exponentiation when the square-and-multiply algorithm (Section 6.4) is used. Recall that, for each bit, the algorithm consists of one squaring and, if the bit is \\(1\\), one multiplication, both operations modulo \\(n\\). By choosing an exponent like \\(e=2^{n}+1\\), with binary representation \\[[e]_2=1\\underbrace{0\\dots0}_{n-1}1,\\] we ensure that we skip most of the multiplications. In practice, often the exponent \\(2^{16}+1\\) is chosen. Another possible optimization is to make use of the Chinese remainder theorem (Proposition 6.9), and perform the operations in \\(\\mathbb{Z}_p\\) and \\(\\mathbb{Z}_q\\), and then reconstruct the plaintext in \\(\\mathbb{Z}_N\\), instead of working in \\(\\mathbb{Z}_N\\) directly. Since \\(p,q\\) are half the size of \\(N\\), exponentiations are cheaper here, and overall the procedure is roughly four times faster. More precisely, let \\(\\mathsf{c}\\in\\mathbb{Z}_N\\) be a ciphertext, and let \\(d\\in\\mathbb{Z}_N\\) be the secret key. We compute \\[\\begin{aligned} &amp; \\mathsf{c}_p = \\mathsf{c}\\bmod{p} &amp; \\mathsf{c}_q = \\mathsf{c}\\bmod{q},\\\\ &amp; d_p = d\\bmod{\\varphi(p)},\\qquad &amp; d_q = d\\bmod{\\varphi(q)}, \\end{aligned}\\] and use these to decrypt in \\(\\mathbb{Z}_p\\) and \\(\\mathbb{Z}_q\\): \\[\\mathsf{m}_p = \\mathsf{c}_p^{d_p} \\bmod{p},\\qquad \\mathsf{m}_q = \\mathsf{c}_q^{d_q} \\bmod{q}.\\] Now, we have \\(\\mathsf{m}_p\\) and \\(\\mathsf{m}_q\\), and the Chinese remainder theorem tells us that there is a unique \\(\\mathsf{m}\\in\\mathbb{Z}_N\\) such that \\[\\begin{aligned} &amp; \\mathsf{m}\\equiv \\mathsf{m}_p \\pmod{p}, &amp; \\mathsf{m}\\equiv \\mathsf{m}_q \\pmod{q}, \\end{aligned}\\] and the second part of the theorem gives us a formula to explicitly recover such \\(\\mathsf{m}\\). Solved exercises Exercise 8.4 Sample a textbook RSA key for \\(\\lambda = 5\\). Use your key to encrypt a message. Decrypt the encrypted message. Solution. We simply need to follow the \\(\\mathsf{KeyGen}\\) algorithm. First we to sample 5-bit prime numbers, that is a pair of prime numbers in the range \\([(10000)_2,(11111)_2] = [16, 31]\\). Let us pick \\(p = 17, q = 31\\). We therefore have \\(N = 527\\), \\(\\phi(N) = 16\\cdot 30 = 480\\). Let us also use \\(e=7\\) for the public exponent. We compute the secret key \\(d=e^{-1}\\mod \\phi(N) = 7^{-1}\\mod 480\\) via the Euclidian algorithm. We have: \\[ \\begin{aligned} 480 &amp;= 68\\cdot 7 + 4 \\\\ 7 &amp;= 1\\cdot 4 + 3 \\\\ 4 &amp;= 1\\cdot 3 + 1 \\\\ \\end{aligned} \\] Therefore we have: \\[ \\begin{aligned} 1 &amp;= 4 - 1\\cdot 3 = 4 - 1 \\cdot (7 - 1\\cdot 4) = -1\\cdot 7 + 2\\cdot 4 \\\\ &amp;= - 1\\cdot 7 + 2\\cdot(480 - 68\\cdot 7) \\\\ &amp;= 2\\cdot 480 - 137\\cdot 7 \\end{aligned} \\] Therefore \\(d \\equiv -137\\equiv 343\\pmod 480\\). Our key pair is \\[ \\mathsf{pk} = (527, 7),\\qquad \\mathsf{sk}=343 \\] Let’s encrypt \\(m=42\\). We need to compute \\(c\\equiv 42^7\\pmod{527}\\). We will use fast exponentiation for this. We have \\(7 = 2^2 + 2^1 + 2^0\\) and therefore \\(c = 42^{2^2}\\cdot 42^{2^1}\\cdot 42^{2^0}\\). We have: \\[ \\begin{aligned} 42^2 &amp;\\equiv 1764\\equiv 183\\pmod{527}\\\\ 42^4 &amp;\\equiv 42^2\\cdot 42^2\\equiv 183\\cdot 183\\equiv 33489\\equiv 288\\pmod{527} \\end{aligned} \\] and finally \\[ c \\equiv 42^{2^2}\\cdot 42^{2^1}\\cdot 42^{2^0} \\equiv 288\\cdot 183\\cdot 42 \\equiv 52704\\cdot 42\\equiv 4 \\equiv 168\\pmod{527} \\] The decryption is exactly like encryption as far as calculations are concerned. We simply use the secret exponent instead of the public one. We have \\[ m \\equiv 168^{343}\\equiv\\ldots\\equiv 42 \\] Exercise 8.5 We show that the textbook RSA is malleable. Show how one can compute an encryption of the message \\(m&#39;=k\\cdot m\\) given only \\(k\\), the public key \\((N,e)\\) and an encryption of of \\(m\\). Solution. Let \\(c_m\\) be an encryption of \\(m\\). We can compute an encryption of \\(k\\) as \\(c_k = k^e\\mod N\\) and compute a new ciphertext \\(c=c_m\\cdot c_k\\mod N\\). We claim that \\(c\\) is the encryption of \\(m&#39;\\) under the public key \\((N,e)\\). Indeed, trying to decrypt it we have: \\[ c^d \\equiv (c_m\\cdot c_k)^{d} \\equiv c_m^{d}\\cdot c_k^{d} \\equiv m^{e\\cdot d}\\cdot k^{e\\cdot d} \\equiv m\\cdot k \\equiv m&#39;\\mod N \\] Exercise 8.6 Consider a variation of the textbook RSA where we use twin primes, namely primes \\(p,q\\) such that \\(p=q+2\\). Show that this variation is not secure? Generalize the above attack for pairs of prime \\(p,q\\) that are “close” to each other, i.e \\(|p-q|\\) is a small integer. Solution. The variation is not secure. The modulus \\(N\\) can be written as \\(N=p(p+2)=p^2 + 2p\\). Adding one on both sides we get \\[ N+1 = p^2 + 2p + 1 = (p+1)^2 \\] Now we can compute (over the integers) the value \\(s = \\sqrt{N+1}\\) and we get \\(s = p+1\\). Therefore \\(p=s-1\\) and \\(q = s+1\\). Assume w.l.o.g. that \\(p&gt;q\\) and let \\(p-q = 2k\\) be the “distance” of the two primes. Note that this number is even since the two primes are odd numbers. We can now rewrite the two primes as \\[ p = \\frac{p+q}{2} + k, \\qquad q = \\frac{p+q}{2} - k \\] and we can write the modulus \\(N\\) as \\[ N = \\left(\\frac{p+q}{2} + k\\right)\\left(\\frac{p+q}{2} - k\\right) =\\left(\\frac{p+q}{2}\\right)^2 - k^2 \\] and therefore \\(N + k^2 = \\left(\\frac{p+q}{2}\\right)^2\\). We can now test for every value of \\(k\\) (here we use the fact that this is a small integer) whether the value \\(N+k^2\\) is a perfect square, or in other words whether \\(\\sqrt{N+k^2}\\in\\mathbb{N}\\). Assume this is the case for some \\(k^*\\). We have: \\[ s = \\sqrt{N+{k^*}^2} = \\frac{p+q}{2} \\] We can now set \\(p = s + k^*\\), \\(q = s-k^*\\). We claim this is the correct factorization of the modulus \\(N\\). Indeed we have \\[ p\\cdot q = (s + k^*)(s - k^*) = s^2 -k\\cdot s + k\\cdot s - k^* = s^2 - k^* = (N + {k^*}^2) - {k^*}^2 = N \\] Exercise 8.7 Alice wants to create a different RSA key to securely communicate with each of her friends. However, her computer is a bit old and looking for large primes takes a while. Alice decides to use the same prime \\(p\\) in all her moduli, that is, she create the RSA moduli \\(N_i = p\\cdot q_i\\) for \\(i\\in \\{1\\ldots m\\}\\). This reduces the key generation time to about a half. Is Alice’s choice secure? Solution. No, this is not secure at all. Specifically, we consider an attacker, Eve, who has access to all the public moduli \\(N_i\\) (recall these are part of the public key and therefore we can assume Eve has access to them). Eve does the following simple attack: she takes any pair of these, say \\(N_1, N_2\\) and she computes \\(gcd(N_1, N_2)\\). Since \\(N_i = p\\cdot q_i\\) and \\(q_i\\)s are prime numbers, the only common divisor is \\(p\\), which is efficiently computed (recall the Euclidian algorithm is polynomial time). Having \\(p\\), she can compute for each \\(i\\) the value \\(q_i = N_i/p\\). Exercise 8.8 (*) Define an RSA group to be the group \\(\\mathbb{Z}_N^*\\) where \\(N\\) is an RSA modulus, that is \\(N=p\\cdot q\\) for \\(p,q\\) \\(\\lambda\\)-bit prime numbers. Argue that the computational problem of finding the order of an RSA group is hard. Can you identify any non-trivial (i.e. not \\(\\mathbb{Z}_N^*\\) or \\(\\{1\\}\\)) subgroup of an RSA group? Solution. The problem boils down to a simple observation: the order of the group \\(\\mathbb{Z}_N^*\\) is by definition \\(\\phi(N)\\). As we show above, knowing this value means that you can compute the factorization of \\(N\\) which is assumed to be a computationally hard problem. By the Lagrange theorem, we know that any subgroup’s order must divide the order of the group, i.e. \\(\\phi(N)\\). We don’t know the order of the group, however, we know its shape is \\(\\phi(N)=(p-1)(q-1)\\) for some primes \\(p,q\\). This means that this value is an even value, which in turn means that \\(2|\\phi(N)\\) and so there must exist a group of size \\(2\\). In addition, such a group is easy to identify, it simply contains \\(-1\\equiv N-1\\pmod{N}\\) and \\(1\\). Diffie, W., &amp; Hellman, M. (1976). New directions in cryptography. IEEE transactions on Information Theory, 22(6), 644-654.↩︎ Rivest, R. L., Shamir, A., &amp; Adleman, L. (1978). A method for obtaining digital signatures and public-key cryptosystems. Communications of the ACM, 21(2), 120-126.↩︎ Initially, it was suggested to use \\(e=3\\) for efficiency, although this opened the gates to some attacks. Nowadays, the most common option is \\(e=2^{16}+1\\).↩︎ In practice, most of the time we use the Miller–Rabin algorithm, which runs in time \\(O(\\lambda^2)\\), although it can produce false positives with a very small probability. A completely fail-safe alternative is the AKS algorithm, which is still efficient but much slower. More detail can be found in Appendix B.↩︎ https://lists.gforge.inria.fr/pipermail/cado-nfs-discuss/2020-February/001166.html↩︎ OAEP stands for optimal asymmetric encryption padding.↩︎ Essentially, this means that the hash functions output uniformly random elements of their respective codomains. This is not true for actual hash functions, but it does not make a difference in practice.↩︎ "],["discrete-logarithm-cryptosystems.html", "9 Discrete logarithm cryptosystems 9.1 The discrete logarithm problem 9.2 The Diffie-Hellman key exchange 9.3 The ElGamal encryption scheme 9.4 Formal security for public key encryption Solved exercises", " 9 Discrete logarithm cryptosystems In the previous lesson, we saw that the security of the RSA encryption scheme relies on the hardness of factoring products of two large primes. However, this is not the only “hard” problem in which we can base our security. In this section, we will introduce The discrete logarithm problem. The Diffie–Hellman key exchange protocol. The ElGamal encryption scheme. 9.1 The discrete logarithm problem In this section, we will present another computational problem that is believed to be hard. To do so, we first introduce the notion of discrete logarithm. Let \\(\\mathbb{G}\\) be a cyclic group, with a generator \\(g\\), written with multiplicative notation. Remember that this means that \\[\\mathbb{G}=\\langle g \\rangle = \\{g^n\\mid n\\in\\mathbb{Z}_{\\geq 0}\\},\\] that is, any element of \\(\\mathbb{G}\\) can be seen as a power of the generator \\(g\\). We can also look at this from the other side: given any element \\(h\\in\\mathbb{G}\\), there exists \\(n\\in\\mathbb{Z}_{\\geq 0}\\) such that \\[g^n=h.\\] This leads to the following definition. Definition 9.1 Let \\(\\mathbb{G}\\) be a cyclic group with generator \\(g\\), written multiplicatively. Given \\(h\\in\\mathbb{G}\\), we define the discrete logarithm (DLog) of \\(h\\) with respect to \\(g\\) as the value \\(n\\in\\mathbb{Z}_{\\geq 0}\\) such that \\(g^n=h\\), and we denote it by \\[\\log_gh=n.\\] When the generator is fixed and there is no ambiguity, we might simply write \\(\\log h\\). The name of the discrete logarithm comes from its similarity to logarithm as the inverse operation to exponentiation over the real numbers. That is, If \\(a,b\\in\\mathbb{R}\\), and \\(c=a^b\\), then we have that \\(\\log_ac=b\\). In Sage, given a group element h and a generator g, the discrete logarithm of h with respect to g can be computed with log(h,g). Definition 9.2 Let \\(\\mathbb{G}\\) be a cyclic group with generator \\(g\\). The discrete logarithm problem relative to \\(\\mathbb{G}\\) consists of, given \\(\\mathbb{G},g\\) and a uniformly random \\(h\\in\\mathbb{G}\\), computing \\(\\log_gh\\). Similarly to the factorization problem, the discrete logarithm problem is believed to be hard (i.e. computationally infeasible) for some well-chosen groups. Again, we do not have a formal proof of the problem being hard, but it has been studied for decades, and no general algorithm faster than exponential-time has been found. An important detail is the “well-chosen” part in the previous paragraph. That is, there exist some groups for which faster algorithms are known. Some cases can even be solved in time polynomial in the size of \\(p\\). Consider, for example, \\((\\mathbb{Z}_p,+)\\) for some large prime \\(p\\), and a generator \\(g\\). For this additive group, the discrete logarithm problem becomes, given \\(\\mathbb{Z}_p,g\\) and a uniformly random \\(h\\in\\mathbb{Z}_p\\), to find \\(x\\) such that \\[gx\\equiv h\\pmod{p}.\\] But this can easily be solved as \\[x\\equiv g^{-1}h\\pmod{p},\\] where the inverse can be computed efficiently using the extended Euclidean algorithm. Therefore, these groups are not suitable for cryptographic purposes, if we want to rely on the discrete logarithm problem being hard. A better candidate are the multiplicative groups \\((\\mathbb{Z}_n^*,\\cdot)\\) or, more precisely, a large subgroup of prime order. Without getting into much detail, the restriction to the prime-order subgroup is due to the Pohlig–Hellman attack,43 which tells us that the discrete logarithm problem in a composite-order group is as hard as the problem in the largest prime-order subgroup. If \\(n\\) is prime, then the order of \\(\\mathbb{Z}_n^*\\) is \\(n-1\\), and thus we want to ensure that the largest prime-order subgroup is as large as possible relative to \\(n\\). This motivates the introduction of safe primes, which are prime numbers \\(p\\) such that \\(q=(p-1)/2\\) is also a prime. This ensures that \\(\\mathbb{Z}_p^*\\) has order \\(2q\\), and the subgroup of order \\(q\\) can be used. To convince ourselves that looking for safe primes is efficient enough, let us try to find them with Sage, using the following code. sec_param=128 while True: p = randint(2^(sec_param-1),2^(sec_param)-1) if p in Primes(): q = (p-1)/2 if q in Primes(): print(&quot;p = &quot;+str(p)) print(&quot;q = &quot;+str(q)) break Running on the free version of CoCalc, these are the approximate times to find a safe prime, for different choices of the security parameter \\(\\lambda\\) Running time \\(128\\) Less than a second \\(256\\) A few seconds \\(512\\) About five minutes \\(1024\\) Two hours This will be much faster with serious computing power (and a refined search algorithm), and nevertheless observe that this will be something that we only need to run once, since the prime can be reused without compromising the hardness of the problem. Exercise 9.1 If we want \\(q\\) as close to \\(p\\) as possible, why don’t we look for primes \\(p\\) such that \\(q=p-1\\) is also prime? Although, strictly speaking, no efficient algorithm is known, some algorithms that are better than exponential have been found for these groups, so this is not an ideal candidate either. The current record of broken discrete logarithm is in a group \\(\\mathbb{Z}_p^*\\) for a prime \\(p\\) of bitlength \\(795\\), which took around \\(3100\\) core-years.44 Currently, the best choice is groups of points of elliptic curves. Elliptic curves are an advanced topic in mathematics, and lie at the intersection of algebraic geometry and number theory. We will not cover them in these notes, but it suffices to say that one can define a group law in the set of points of one such curve, and some of these curves are believed to have very hard discrete logarithms, much harder than the groups \\(\\mathbb{Z}_n^*\\) of the same size. In contrast with the discrete logarithm records in \\(\\mathbb{Z}_n^*\\), the largest known discrete logarithm solved corresponds to an elliptic curve of order \\(n\\), for \\(n\\) a \\(114\\)-bit integer. It took the researchers 13 days of parallel computation on \\(256\\) NVIDIA Tesla V100 GPUs.45 9.2 The Diffie-Hellman key exchange In the first half of the course, we discussed symmetric cryptography. For two parties Alice and Bob to communicate using a symmetric encryption scheme, they need to first establish a shared secret key, in a process called key agreement or key establishment. This is a non-trivial task. Maybe they could meet in person and agree on a key, or maybe they could both trust a third party to handle the key agreement for them. But clearly this is not ideal, and depending on the context maybe not possible at all. One way to solve this issue is to use an asymmetric encryption scheme as a key encapsulation mechanism: Alice chooses a random symmetric key \\(k\\), and uses a public-key encryption scheme, like RSA, to encrypt the key and send it to Bob. Then Bob decrypts the message and learns \\(k\\). They can do this because the public-key encryption scheme does not require sharing secret keys in advance. From this point onwards, Alice and Bob can use the key \\(k\\) to communicate using a symmetric encryption scheme, like AES. One might ask why not use public-key encryption all the time, since it requires no shared keys. While this poses no security problem, asymmetric encryption schemes tend to be much less efficient that symmetric ones, so it is simply faster to establish a key using asymmetric encryption, and later use symmetric encryption. An alternative approach to key encapsulation mechanisms is to use the following. Definition 9.3 A key exchange protocol is a procedure between two parties, at the end of which both parties know a common key. The first and best-known key exchange protocol is the Diffie–Hellman key exchange protocol, introduced in 1976,46 and it works as follows. The only common input is a security parameter \\(\\lambda\\). On input \\(\\lambda\\), Alice chooses a uniformly random prime \\(p\\) of bitlength \\(\\lambda\\), and determines a cyclic group \\(\\mathbb{G}\\) of order \\(p\\), and a generator \\(g\\) of \\(\\mathbb{G}\\). Alice sends \\((\\mathbb{G},g)\\) to Bob.47 Alice chooses a uniformly random \\(a\\in\\mathbb{Z}_p\\), computes \\(A=g^a\\), and sends \\(A\\) to Bob. Similarly, Bob chooses a uniformly random \\(b\\in\\mathbb{Z}_p\\), computes \\(B=g^b\\), and sends \\(B\\) to Alice. Now, Alice computes the key as \\(k=B^a\\), and Bob computes the same key as \\(k=A^b\\). Since \\(k\\) is technically a group element, it is processed in some established way to obtain a bitstring, which is the actual key that will be used for symmetric encryption purposes. The parameters \\((\\mathbb{G},g)\\) can be reused without compromising security, so steps (1) and (2) do not need to be run again every time that Alice and Bob want to share a key, but it is very important that the values \\(a,b\\) are always fresh. Before discussing security, let us convince ourselves that indeed Alice and Bob end up with the same key. Observe that \\[B^a = \\left(g^b\\right)^a = g^{ab} = \\left(g^a\\right)^b = A^b.\\] Therefore, the value computed by each party is actually the same. Notice how \\(k\\) was never sent from one side to the other. However, some partial information related to it was sent, so one might be worried that an eavesdropper might learn some information about the key from the intercepted values, \\(A\\) and \\(B\\). We define the exact level of security that we want to attain. Definition 9.4 A key exchange protocol is secure in the presence of an eavesdropper if no efficient adversary that observes the protocol (that is, an adversary that sees \\(\\mathbb{G},g,A,B\\)) can tell the real key from a uniformly random bitstring of the same size. Notice that this is a very strong definition, which implies that not a single bit of the key is leaked. So why is the Diffie–Hellman key exchange protocol secure? Consider the following scenario. The adversary eavesdrops \\(A,B\\), and finds \\(a\\) by solving the discrete logarithm of \\(A\\) with respect to \\(g\\). Then the adversary could simply compute \\(B^a\\) and learn the shared key. Therefore, to prevent such an attack, we must ensure that the discrete logarithm problem is hard in \\(\\mathbb{G}\\). However, formally this is not enough, because conceivably an adversary could extract the key directly from \\(A,B\\), without the need to solve any discrete logarithm. This motivates the introduction of the following related problem. Definition 9.5 Let \\(\\mathbb{G}\\) be a cyclic group of primer order \\(p\\), with generator \\(g\\). Let \\(a,b\\) be uniformly random elements of \\(\\mathbb{Z}_p\\), and let \\[A=g^a,\\qquad B=g^b.\\] The computational Diffie–Hellman (CDH) problem relative to \\(\\mathbb{G}\\) consists of, given \\(\\mathbb{G},g,A,B\\), computing \\(g^{ab}\\). If this problem is hard, then clearly an adversary will not be able to compute a Diffie–Hellman key from eavesdropping communications. However, our definition of security requires something stronger: that such a key cannot be distinguished from random strings. Definition 9.6 Let \\(\\mathbb{G}\\) be a cyclic group of primer order \\(p\\), with generator \\(g\\). Let \\(a,b\\) be uniformly random elements of \\(\\mathbb{Z}_p\\), and let \\[A=g^a,\\qquad B=g^b.\\] With probability \\(1/2\\), let \\(C=g^{ab}\\), otherwise let \\(C\\) be a uniformly random element of \\(\\mathbb{G}\\). The decisional Diffie–Hellman (DDH) problem relative to \\(\\mathbb{G}\\) consists of, given \\(\\mathbb{G},g,A,B,C\\), decide whether \\(C=g^{ab}\\) or \\(C\\) is something else. Observe that, unlike any other computational problem that we have considered, the DDH problem can easily be solved with probability \\(1/2\\), by guessing at random. Thus, for decisional problems, we say that the problem is hard if there is no efficient algorithm that can do significantly better than that. For example, an efficient algorithm that succeeds in solving the DDH problem with probability \\(2/3\\) would be considered a breach of the problem. Exercise 9.2 Prove that: If we can break the DLog problem, then we can break the CDH problem. If we can break the CDH problem, then we can break the DDH problem. As is was the case with the factorization and RSA problems, there is no known algorithm that can solve CDH or DDH any faster than the DLog problem. Nevertheless, there is no formal proof that solving either of these allows us to solve DLog, so the problems are not known to be equivalent, hence why we require them for security of the Diffie–Hellman key exchange protocol. Proposition 9.1 If the DDH problem is hard relative to a group \\(\\mathbb{G}\\), then the Diffie–Hellman key exchange, using the group \\(\\mathbb{G}\\), is secure in the presence of an eavesdropper. 9.3 The ElGamal encryption scheme We introduce the ElGamal encryption scheme,48 Unlike the RSA cryptosystem, the ElGamal encryption scheme works in a cyclic group of prime order. An advantage of this scheme is that it provides CPA security by default, without the need of padding, so no hash function is required. \\(\\mathsf{KeyGen}\\): on input a security parameter \\(\\lambda\\), choose a cyclic group \\(\\mathbb{G}\\) of prime order \\(p\\), and a generator \\(g\\) of \\(\\mathbb{G}\\). We will write \\(\\mathbb{G}\\) with multiplicative notation. Sample a uniformly random \\(x\\in\\mathbb{Z}_p\\), and set \\(h=g^x\\). Output the public key \\[\\mathsf{pk}=(\\mathbb{G}, g, h),\\] and the secret key \\[\\mathsf{sk}=x.\\] \\(\\mathsf{Enc}\\): given a message \\(\\mathsf{m}\\in\\mathbb{Z}_p\\), with \\(\\mathsf{m}\\) small, and the receiver’s public key \\((\\mathbb{G}, g, h)\\), choose a uniformly random \\(r\\in\\mathbb{Z}_p\\) and output the ciphertext \\[\\mathsf{c}=(\\mathsf{c}_1,\\mathsf{c}_2)=(g^r,g^{\\mathsf{m}}h^r).\\] \\(\\mathsf{Dec}\\): given a ciphertext \\(\\mathsf{c}\\) and the secret key \\(x\\), output \\[\\log_g\\frac{\\mathsf{c}_2}{\\mathsf{c}_1^x}.\\] It is easy to see that decryption recovers the original message encrypted. Indeed, observe that \\[g^{\\mathsf{m}}h^r=g^{\\mathsf{m}}\\left(g^x\\right)^r=g^{{\\mathsf{m}}+xr},\\] and thus \\[\\log_g\\frac{\\mathsf{c}_2}{\\mathsf{c}_1^x}=\\log_g\\frac{g^{{\\mathsf{m}}+xr}}{\\left(g^r\\right)^x}=\\log_g g^{\\mathsf{m}}={\\mathsf{m}}.\\] One thing that might seem counter-intuitive is the fact that we are supposed to find the discrete logarithm of \\(g^m\\) to recover the message. But, at the same time, we will require the DLog problem to be hard for security. The key point is that \\(\\mathsf{m}\\) is small relative to \\(p\\), so that the DLog of \\(g^m\\) can be solved efficiently. Observe that this does not contradict the discrete logarithm problem, which states that the discrete logarithm should be hard to compute for uniformly random elements of \\(\\mathbb{Z}_p\\), but it is fine if it can be solved for small values. Note. This version of the ElGamal encryption scheme is known as lifted ElGamal, because the message \\(\\mathsf{m}\\) is an element in \\(\\mathbb{Z}_p\\) that is lifted to the exponent to operate with the group element \\(g^{\\mathsf{m}}\\). It is also possible to use a group element as message directly, in which case encryption is performed as \\[\\mathsf{c}=(\\mathsf{c}_1,\\mathsf{c}_2)=(g^r,\\mathsf{m}h^r)\\] and decryption consists of computing \\[\\frac{\\mathsf{c}_2}{\\mathsf{c}_1^x}.\\] Intuitively, the security of the scheme relies on the DLog problem being hard, because an adversary that can compute discrete logarithms would be able to recover \\(x\\) and run the decryption algorithm. Moreover, observe that the scheme is randomized, that is, the same plaintext can produce different ciphertexts, depending on each encryption’s randomness \\(r\\). Formally, again we require the DDH problem to be hard to ensure security. Proposition 9.2 If the DDH problem is hard for a group \\(\\mathbb{G}\\), then the ElGamal encryption scheme in \\(\\mathbb{G}\\) is secure. Below is an implementation of the ElGamal encryption scheme in \\(\\mathbb{Z}_p^*\\). The value of \\(p\\) used is a safe prime of bitlength \\(1024\\) found used the algorithm above. ### KEY GENERATION # Choose parameters p = 98477271628635149697160687227938079584387801057656524674547805684845362792314056005063953177189645361017363320475498530632115997158699647766751945380661872143643706009196552179178021780647235396351787889600626935912984928121265808769679480550797947557845891911467438040517702514768796757174157093600219716843 q = (p-1)/2 # Check that p, q are primes p in Primes(), q in Primes() # Set the group as Z_n^*, choose a generator g and check that g has order q G = Integers(p) g = G(3) g.multiplicative_order() == q # Compute and output the keys. x = randint(0,q) h = g^x pk = (p,g,h) sk = x ### ENCRYPTION # Choose a small message (so that its DLog can be computed) m = 27593 if (m&gt;=q): print(&quot;Message too large.&quot;) else: # Sample randomness r = randint(0,q) # Compute the ciphertext c = (g^r, g^m*h^r) print(c) ### DECRYPTION w = c[1]/(c[0]^x) # Solve the discrete logarithm of w with respect to g by brute force for m in range(q): if g^m == w: print(m) break 9.4 Formal security for public key encryption In section 4 we studied the formal frameworks for analyzing security of encryption schemes. The most important models where the IND-CPA experiment, where the adversary can ask of encryptions of arbitrary plaintexts and is then challenged to decide whether a ciphertext \\(c\\) is an encryption of \\(m_0\\) or \\(m_1\\) (which the adversary chooses), the IND-CCA experiment which is the same with the addition that the adversary can also ask for decryptions of arbitrary ciphertexts, excluding the challenge ciphertext. Let us first reflect on the models in the public key setting. The crucial point here is that in this setting anyone can produce ciphertexts of any message on its own. Indeed, no secret is involved in the encryption process; one just need to use the public key! Therefore, IND-CPA is the minimum requirements. Indeed, an adversary can produce on its own encryptions of its choice, which is equivalent to asking for encryptions of messages to the challenger. To put it in other words: a public key encryption scheme that is insecure when given the ability to get encryptions of known messages is insecure in general! Next, we ask what security guarantees the presented public key encryptions schemes guarantee. We can already partially answer that with the attacks we saw in section 4. The plain version of RSA is deterministic and therefore it is not IND-CPA (therefore not IND-CCA) secure. The ElGamal encryption scheme is malleable and therefore it is not IND-CCA secure. Let’s focus on the second point. As we will see in detail in exercise 9.9, a pairwise multiplication of El Gamal ciphertexts \\(c_1,c_2\\) encrypting messages \\(m_1,m_2\\) respectively, yields an ElGamal encryption of the message \\(m = m_1\\cdot m_2\\) (\\(m=m_1+m_2\\) for the lifted ElGamal case). This means that we can change a ciphertext in a way that the corresponding change in the underlying plaintext is predictable: simply produce an encryption of \\(c_2=\\textsf{Enc}_{pk}(m_2)\\) and multiply it with another ciphertext \\(c_1\\) corresponding to some message \\(m_1\\)! Therefore, ElGamal is not IND-CCA secure. While we will not prove the next claims, we present the security notions that are satisfied. Note that these are conditioned on assumptions! If the DDH assumption holds, ElGamal is IND-CPA secure. If the RSA assumption holds and the hash function \\(H\\) is ideal (meaning it behaves as a random function), then the RSA-OAEP instantiated with \\(H\\) is CCA-secure. Solved exercises Exercise 9.3 You want to establish a secret key with Alice using Diffie-Hellman key exchange. You both have as input the security parameter \\(\\lambda = 5\\). For the group, you use a prime-order subgroup of \\(\\mathbb{Z}^*_p\\) where \\(p\\) is a safe prime. Send the first message to Alice, that is send a description of a group \\(\\mathbb{G}\\) of prime order \\(q\\) for some \\(q\\) with length 5 bits. Alice uses the randomness \\(10\\) for the key exchange. What message do you receive? Pick a random value and construct the message you should send to Alice. Compute the common key. How would Alice compute the common key with the information she has? Solution. Let’s first find a prime \\(q\\) that that is 5-bit long, i.e. \\(16\\leq q &lt;32\\). Since we work with safe primes, it must also hold that \\(p=2q + 1\\) is also a prime. The set of primes in this range is \\(\\{17, 19, 23, 29\\}\\). We need to check which is a safe prime. \\(17\\cdot 2 + 1 = 35\\) which is not a prime. \\(19\\cdot 2 + 1 = 39\\) which is not a prime. \\(23\\cdot 2 + 1 = 47\\) which is a prime. Bingo! Our safe prime is \\(47\\) so will work over \\(\\mathbb{Z}_{47}^*\\). This is not our group though! We need a subgroup of order \\(23\\). Let’s find an element of \\(\\mathbb{Z}_{47}^*\\) that generates a group of order \\(23\\). Recall by the Lagrange theorem that the order of a subgroup divides the order of the group. Here, \\(|\\mathbb{Z}_{47}| = 46 = 2\\cdot 23\\), so any element will have order of \\(1, 2, 23\\), or \\(46\\). Let’s try 2. \\[ \\begin{aligned} 2^2 \\equiv 4 \\not\\equiv 1 \\pmod {47}, \\qquad 2^{23} \\equiv \\ldots\\equiv 1 \\pmod {47} \\end{aligned} \\] Bingo! We can send to Alice \\[ \\mathbb{G} = \\text{subgroup } \\langle 2 \\rangle \\text{ of } \\mathbb{Z}_{47}^2 \\text{}, \\qquad g = 2 \\] Since she chose the random value \\(10\\), Alice computes \\[A \\equiv 2^{10}\\equiv 1024 \\equiv 37\\pmod{47}\\] Therefore, she sends \\(37\\). Let’s randomly choose \\(5\\) for our randomness. We compute \\(B \\equiv 2^5\\equiv 32\\pmod{47}\\). We therefore send \\(B=32\\) to Alice. We finally compute the shared key as \\(K \\equiv A^5 \\equiv 37^5\\equiv 16\\pmod{47}\\) (recall we don’t know the randomness Alice used). The common key is \\(K\\equiv 16\\pmod{47}\\). Alice does not know the randomness we used. She uses her own randomness to compute the common key. That is, she computes \\(K\\equiv B^{10}\\equiv 32^{10} \\equiv 16\\pmod{47}\\). Exercise 9.4 Alice and Bob want to communicate secretly in the classroom. They don’t have electronic devices but they are fast in calculations, so they plan to use Diffie Hellman key exchange and do the needed computations on paper. They are far from each other, however. They ask Eve to be the middleman and pass papers from one to the other. How can Eve learn their discussion? Solution. Eve can perform a man-in-the-middle attack. When she receives a message from Alice, she does not transmit it but rather she writes her own message to Bob and pretends this comes from Alice. She does the same on the Bob’s side. More concretely, when Alice -who initializes the protocol- sends \\(\\mathbb{G}, g\\), she passes the message to Bob normally. When Bob sends his value \\(B\\) \\((=g^b)\\) as part of the protocol, she does not transmit it. She keeps it for herself and creates a new message \\(B&#39;=g^{b&#39;}\\), that she sends to Alice. When Alice replies with \\(A\\) \\((=g^a)\\) she does the same: she keeps it and transmits a message \\(A&#39;=g^{a&#39;}\\) to Bob. Now Eve creates a Diffie-Hellman key to communicate with Alice, namely \\(K_{AE} = A^{b&#39;}\\) and another key to communicate with Bob, namely \\(K_{EB} = B^{a&#39;}\\). Alice also computes the share key \\(K_{AE}\\) thinking it is a shared key with Bob, and Bob the key \\(K_{EB}\\) thinking it is the shared key with Alice! Now, when Alice passes a message \\(m\\) encrypted with the common key \\(K_{AE}\\), Eve uses the key \\(K_{AE}\\) to decrypt it and then it reencrypts it with \\(K_{EB}\\) to give to Bob. It does the same for messages from Bob’s side. Below is a schematic of the man-in-the-middle attack. Figure 9.1: Man-in-the-Middle Attack on Diffie-Hellman Exercise 9.5 (*) Let \\(p\\) be a safe prime and let \\(\\mathbb{G}\\) be a subgroup of \\(\\mathbb{Z}_p\\) of prime order \\(q\\) with generator \\(g\\). Let \\(h\\in\\mathbb{G}\\). Show that if you know \\(\\alpha, \\beta\\neq 0\\) such that \\(g^\\alpha h^\\beta = 1\\), you can find \\(\\log_g h\\). Let \\(h\\) be a fixed, random element of \\(\\mathbb{G}\\). Define the hash function \\(H: \\mathbb{Z}_q\\times\\mathbb{Z}_q \\rightarrow \\mathbb{Z}_p^{*}\\) that maps \\[ (\\alpha,\\beta) \\mapsto g^\\alpha h^\\beta \\] Show that \\(H\\) is a collision-resistant hash function, assuming that the discrete logarithm problem is hard. Solution. The idea is to manipulate the equation \\(g^\\alpha h^\\beta = 1\\) in a way such that we end up with an equation of the form \\(h=g^x\\). The crucial thing to observe, is that when working with the exponents, we can work \\(\\mod q\\). This is the case since by Euler’s theorem \\(h^q=1\\) for any \\(h\\). This means that the exponents “circle around” with frequency \\(q\\). Let’s start with the equation \\(g^\\alpha h^\\beta = 1\\). First we want to move \\(h\\) on the other side. Compute the (additive) inverse of \\(\\beta\\) \\(\\mod q\\). This value is equal to \\(q-\\beta\\equiv -\\beta\\mod q\\). Multiply both sides by \\(h^{-\\beta}\\). We get \\[ g^\\alpha h^\\beta h^{-\\beta} = 1 \\Leftrightarrow g^\\alpha h^0 = h^{-\\beta}\\Leftrightarrow g^\\alpha = h^{-\\beta} \\] We need to get rid of value \\(-\\beta\\) on the right hand side. To do this, we need to to find an element \\(c\\) such that \\(c\\cdot(-\\beta)\\equiv 1\\mod q\\). Such a number is the multiplicative inverse \\(\\mod q\\) of \\(-\\beta\\). Note that since \\(\\beta\\neq 0\\) this number exists. We can compute this number via the extended Euclidian algorithm. Raising both sides of the equation to \\(c\\) we get: \\[ g^{\\alpha\\cdot c} = h^{-\\beta\\cdot c} \\Leftrightarrow g^{\\alpha\\cdot c} = h^{1} \\Leftrightarrow g^{\\alpha\\cdot c} = h \\] Therefore, the value \\(\\log_g h = \\alpha \\cdot c\\), where \\(c = -\\beta^{-1}\\mod q\\). First, let’s see that \\(H\\) is a hash function. The input is two elements of \\(\\mathbb{Z}_q\\). Since \\(q\\) is an \\(\\lambda\\) prime, the input size is \\(2\\cdot \\lambda\\) bits. The output is an element of \\(\\mathbb{Z}_p^*\\) with \\(p=2q+1\\), so it needs \\(\\lambda+1\\) bits to be represented which is smaller than \\(2\\cdot \\lambda\\). Therefore \\(H\\) is indeed a hash function. We will next show it is also collision-resistant. The idea is the following. Assume we find two pairs \\((\\alpha_1, \\beta_1)\\neq (\\alpha_2, \\beta_2)\\) with the same hash, that is \\[ g^{\\alpha_1}h^{\\beta_1} = c = g^{\\alpha_2}h^{\\beta_2} \\] Multiplying both sides with \\(g^{-\\alpha_2}h^{-\\beta_2}\\) (we compute these values \\(\\mod q\\)!) \\[ \\begin{aligned} g^{\\alpha_1}h^{\\beta_1}g^{-\\alpha_2}h^{-\\beta_2} = c = g^{\\alpha_2}h^{\\beta_2}g^{-\\alpha_2}h^{-\\beta_2} &amp;\\Leftrightarrow \\\\ g^{\\alpha_1-\\alpha_2}h^{\\beta_1-\\beta_2} = g^{\\alpha_1-\\alpha_1}h^{\\beta_1-\\beta_1} &amp;\\Leftrightarrow \\\\ g^{\\alpha_1-\\alpha_2}h^{\\beta_1-\\beta_2} = g^{0}h^{0} &amp;\\Leftrightarrow \\\\ g^{\\alpha_1-\\alpha_2}h^{\\beta_1-\\beta_2} = 1 \\end{aligned} \\] Note that either both \\(\\alpha_1=\\alpha_2\\) and \\(\\beta_1=\\beta_2\\) or both are different. Since we assume we found a collision, it must be the second case. By the previous question, setting \\(\\alpha = \\alpha_1 -\\alpha_2\\) and \\(\\beta = \\beta_1 -\\beta_2\\neq 0\\) we have \\(g^{\\alpha}h^{\\beta}=1\\) and therefore we can compute \\(\\log_{g}h\\). Therefore, the function \\(H\\) is collision-resistant or the discrete logarithm problem in \\(\\mathbb{G}\\) is not hard. Exercise 9.6 Let \\(\\mathbb{G}\\) be the subgroup of order \\(q=11\\) of \\(\\mathbb{Z}_{23}^*\\) and \\(g = 2\\) be a generator of \\(\\mathbb{G}\\). Sample a key pair for the ElGamal encryption scheme using \\(\\mathbb{G}\\), \\(g\\) as part of the public key. Encrypt the message \\(m = 5\\in\\mathbb{Z}_q\\) and decrypt the result using lifted ElGamal. Encrypt the message \\(m = 8\\in\\mathbb{G}\\) and decrypt the result using ElGamal. Solution. We have already fixed the group and the generator so it remains to sample \\(h\\). We first sample a random exponent in \\(\\mathbb{Z}_q\\). Let \\(x=9\\) be the exponent. We next compute \\(h=g^x\\equiv 2^{9}\\equiv 6\\pmod{23}\\). Note how the secret key is an element in \\(\\mathbb{Z}_q\\) and the public key an element of \\(\\mathbb{G}\\). First we sample randomness for encryption. Let the dice roll \\(r=3\\) (\\(\\in\\mathbb{Z}_q\\)!). Recall that the encryption in lifted is ElGamal is done by \\[ \\mathsf{Enc}_{\\mathsf{pk}}(m;\\ r) = \\left(g^r, g^mh^r\\right) \\] Substituting the values we have \\[ \\mathsf{Enc}_{{(\\mathbb{G},2,6)}}(5;\\ 3) = \\left(2^3, 2^56^3\\right) = \\left(8, 43\\cdot 216\\right) = \\left(8, 9\\cdot 9\\right) = \\left(8, 12\\right) \\in(\\mathbb{G}, \\mathbb{G}) \\] To decrypt we first compute the element \\(g^m\\) using the secret key \\(x\\). Let \\((c_1, c_2)\\) be the ciphertext \\((8,12)\\). We have \\[ g^m = \\frac{c_2}{c_1^x} = \\frac{12}{8^9} \\] First we compute \\(c_1^x = 8^9 \\equiv 9 \\pmod{23}\\). We now need to compute its multiplicative inverse \\(\\pmod{23}\\). Recall this is done using the Extended Euclidian Algorithm. We have \\(9^{-1}\\equiv 18 \\pmod{23}\\) and finally \\(g^m = 12\\cdot 18 \\equiv 9 \\pmod{23}\\). Finally, recall that since the discrete logarithm problem is computationally hard in such groups, we need to perform a bruteforce to retrieve the actual message. We have \\[ \\begin{aligned} 2^1 &amp;\\equiv 2 \\not\\equiv 9\\pmod{23} \\\\ 2^2 &amp;\\equiv 4 \\not\\equiv 9\\pmod{23} \\\\ 2^3 &amp;\\equiv 8 \\not\\equiv 9\\pmod{23} \\\\ 2^4 &amp;\\equiv 16 \\not\\equiv 9\\pmod{23} \\\\ 2^5 &amp;\\equiv 32 \\equiv 9\\pmod{23} \\end{aligned} \\] So \\(m = 5\\) (in \\(\\mathbb{Z}_q^*\\)). As in the previous question, we first we sample randomness for encryption. Let the dice roll \\(r=2\\) (\\(\\in\\mathbb{Z}_q\\)!) this time. In the simple ElGamal the encryption is done by computing \\[ \\mathsf{Enc}_{\\mathsf{pk}}(m;\\ r) = \\left(g^r, m\\cdot h^r\\right) \\] Substituting the values we have \\[ \\mathsf{Enc}_{{(\\mathbb{G},2,6)}}(8;\\ 2) = \\left(2^2, 8\\cdot 6^2\\right) = \\left(4, 8\\cdot 36\\right) = \\left(4, 12\\right) \\] In this version of ElGamal we can directly decrypt since we don’t have to compute a discrete logarithm. Let \\((c_1, c_2)\\) be the ciphertext \\((4,12)\\). We have \\[ m = \\frac{c_2}{c_1^x} = \\frac{12}{4^9} \\] First we compute \\(c_1^x = 4^9 \\equiv 13 \\pmod{23}\\). Using again the Extended Euclidian Algorithm we compute its multiplicative inverse \\(\\pmod{23}\\). We have \\(13^{-1}\\equiv 16 \\pmod{23}\\) and finally \\(m = 12\\cdot 16 \\equiv 8\\pmod{23}\\). So \\(m = 8\\) (in \\(\\mathbb{Z}_q^*\\)). Exercise 9.7 Let \\(\\mathbb{G}\\) be the subgroup of prime order \\(q\\) of \\(\\mathbb{Z}_{p}^*\\) for some prime \\(p\\). Show that decryption of an ElGamal is possible without the secret key if one knows the randomness used to encrypt the message. A bug in an ElGamal implementation causes the randomness to be sampled from a smaller space that \\(\\mathbb{Z}_q\\). You observe the traffic and notice two ciphertexts \\((c_1, c_2), (c_1&#39;, c_2&#39;)\\) with \\(c_1 = c_1&#39;\\). What can you learn about the corresponding plaintexts? In the previous case, can you also learn something about the secret key \\(x\\)? Solution. Let \\((c_1, c_2) = (g^r, mh^r)\\) be a ciphertext. Knowing \\(r\\), we can compute \\[ c_2 h^{-r} = mh^{r}h^{-r} = mh^{0} = m^1 = m \\] Similarly, we can compute this way \\(g^m\\) in lifted ElGamal and bruteforce to find \\(m\\). Since \\(c_1 = c_2\\), we know they must correspond to \\(g^r\\) for the same \\(r\\). Therefore, the second part of the ciphertext would be of the form \\[ c_2 = mh^r, \\qquad c_2&#39; = m&#39;h^r \\] and note that \\(r\\) is the same in the above. We will eliminate \\(r\\) from the pair of the equations. To do show, we compute \\(c_2\\cdot {c_2&#39;}^{-1}\\). This equals to \\[ c_2\\cdot {c_2&#39;}^{-1} = mh^r\\cdot {(m&#39;h^r)}^{-1} = mh^r\\cdot {m&#39;}^{-1}{h^r}^{-1} = m {m&#39;}^{-1} \\] so we can retrieve the value \\(m/m&#39;\\). In the case of lifted ElGamal the same operation would give us \\[ g^m\\cdot (g^{m&#39;})^{-1} = g^m\\cdot g^{-m&#39;}= g^{m-m&#39;} \\] By bruteforcing, we can learn \\(m-m&#39;\\). No. To see this, note that every time you encrypt a message with a public key, you already know the randomness. If this value helped learn something about the secret key, ElGamal would be completely insecure. Exercise 9.8 There is a very close connection between ElGamal and Diffie-Hellman key exchange. Describe the former in terms of the latter. Solution. In exercise 7.9 we show that given some group \\((\\mathbb{G}, \\cdot)\\), the ciphertext \\(c = K\\cdot m\\), where \\(m, k \\in \\mathbb{G}\\) and \\(k\\) is uniformly distributed has perfect secrecy. We can view ElGamal as an instance of a Diffie Hellman key exchange where both parties agree to a \\(k\\) and use it to encrypt a single message \\(k\\cdot m\\) (or \\(k\\cdot m^g\\) in the case of lifted ElGamal). More concretely fix a group \\(\\langle g\\rangle = \\mathbb{G}\\) of order \\(q\\) where DDH is hard. Alice, who wants to receive messages runs the second step of DH key exchange, that is she selects a random \\(x\\in \\mathbb{Z_q}\\) and computes \\(A = g^x\\). She does not use it to communicate with a single person. Instead, she publicly announces it along with the message “Complete the DH key exchange to communicate with me.”. Her ElGamal key pair is \\((x,A)\\) Bob runs the third part of the protocol, i.e. he selects a random \\(r\\) and computes the element \\(B=g^r\\). It also establishes the common key \\(k=A^r\\) \\((=g^{xr})\\) and uses it to encrypt in OTP style the message \\(m\\), that is it computes \\(c_2 = k\\cdot m\\). It sends the DH protocol message \\(B\\) and the encrypted message \\(c_2\\). Alice computes the common key \\(k = B^x\\) \\((=g^{rx})\\) and decrypts the message by computing \\(m=c_2\\cdot k^{-1}\\). We make a couple of notes: Note that this explains in a different way why using the same randomness is not good. Doing so means the emerging shared key \\(k\\) is the same and as we saw in exercise 7.9 this should never happen. ElGamal using this general one-time-pad in \\(\\mathbb{G}\\) which is perfectly secret but ElGamal does not have this property. Why is that? The reason is that the communicating parties, in order to avoid to meet and exchange a secret key before communicate, share publicly their Diffie-Hellman key exchange values \\(g^x\\) and \\(g^r\\). These fully define the common key \\(k\\) (there exists a unique key for each pair). We therefore must rely on a computational assumption, namely, that there is no efficient way to learn something about \\(k\\) given these values. Exercise 9.9 Show that ElGamal is homomorphic with respect to multiplication, that is \\[ \\mathsf{Enc}_{\\mathsf{pk}}(m_1; r_1)\\cdot \\mathsf{Enc}_{\\mathsf{pk}}(m_2; r_2) = \\mathsf{Enc}_{\\mathsf{pk}}(m_1\\cdot m_2; r_1+r_2) \\] Show that lifted ElGamal is homomorphic with respect to addition, that is \\[ \\mathsf{Enc}_{\\mathsf{pk}}(m_1; r_1)\\cdot \\mathsf{Enc}_{\\mathsf{pk}}(m_2; r_2) = \\mathsf{Enc}_{\\mathsf{pk}}(m_1+ m_2; r_1+r_2) \\] For both versions of ElGamal so that it is possible to reencrypt a message, that is, given only a ciphertext \\((c_1, c_2)\\), we can produce a different ciphertext the decrypts to the same message. Solution. This can be done by straightforward calculations. We have \\[ \\begin{aligned} \\mathsf{Enc}_{\\mathsf{pk}}(m_1; r_1)\\cdot \\mathsf{Enc}_{\\mathsf{pk}}(m_2; r_2) &amp;= (g^{r_1}, m_1\\cdot h^{r_1}) \\cdot (g^{r_2}, m_2\\cdot h^{r_2}) \\\\ &amp; = (g^{r_1}\\cdot g^{r_2}, (m_1\\cdot h^{r_1})\\cdot (m_2\\cdot h^{r_2})) \\\\ &amp; = (g^{r_1+r_2}, (m_1\\cdot m_2)\\cdot h^{r_1+r_2}) \\\\ &amp; = \\mathsf{Enc}_{\\mathsf{pk}}(m_1\\cdot m_2; r_1+r_2) \\end{aligned} \\] Similarly as in the previous case we have \\[ \\begin{aligned} \\mathsf{Enc}_{\\mathsf{pk}}(m_1; r_1)\\cdot \\mathsf{Enc}_{\\mathsf{pk}}(m_2; r_2) &amp;= (g^{r_1}, g^{m_1}\\cdot h^{r_1}) \\cdot (g^{r_2}, g^{m_2}\\cdot h^{r_2}) \\\\ &amp; = (g^{r_1}\\cdot g^{r_2}, (g^{m_1}\\cdot h^{r_1})\\cdot (g^{m_2}\\cdot h^{r_2})) \\\\ &amp; = (g^{r_1+r_2}, g^{m_1+ m_2}\\cdot h^{r_1+r_2}) \\\\ &amp; = \\mathsf{Enc}_{\\mathsf{pk}}(m_1+ m_2; r_1+r_2) \\end{aligned} \\] We make use of the homomorphic property each construction has. Let \\((c_1, c_2) = \\mathsf{Enc}_{\\mathsf{pk}}(m_1; r_1)\\) In the plain ElGamal we multiply the ciphertext with a fresh encryption of \\(1\\), that is we compute \\[ (c_1, c_2) \\cdot \\mathsf{Enc}_{\\mathsf{pk}}(1; r_2) = \\mathsf{Enc}_{\\mathsf{pk}}(1; r_2)\\cdot \\mathsf{Enc}_{\\mathsf{pk}}(1; r_2) = \\cdot \\mathsf{Enc}_{\\mathsf{pk}}(m_1; r_1+r_2) \\] In the lifted ElGamal we multiply the ciphertext with a fresh encryption of \\(0\\), that is we compute \\[ (c_1, c_2) \\cdot \\mathsf{Enc}_{\\mathsf{pk}}(0; r_2) = \\mathsf{Enc}_{\\mathsf{pk}}(0; r_2)\\cdot \\mathsf{Enc}_{\\mathsf{pk}}(0; r_2) = \\cdot \\mathsf{Enc}_{\\mathsf{pk}}(m_1; r_1+r_2) \\] Exercise 9.10 Alice is a tallying member in an election for a referendum in a university. To vote for yes, a voter sends an encryption of \\(g^{1}\\) using Lifted ElGamal to Alice and to vote no, it sends an encryption of \\(g^{2}\\). Alice then decrypts the message by first computing \\(M=g^{b}\\) and then checking if \\(M=g^1\\) or \\(M=g^2\\). She does not try to find the actual value if \\(M\\) is neither since it is not a valid ballot. She then replies to the voter “Thank you for participation” and tallies her vote. In case the vote is invalid, she sends a message clarifying the voting procedure, specifically she sends “Please send an encryption of \\(1\\) for yes and an encryption of \\(2\\) for no”. Eve does not care about the outcome of the referendum, but she is curious what Bob has chosen. She monitors the network and notices that Bob sends an encrypted message \\((c_1, c_2)\\) to Alice. Can she learn the choice of Bob? Assume that Bob votes correctly, that is it sends either an encryption of 0 or an encryption of 1. Solution. Eve can use the homomorphic property of ElGamal and the fact that Alice reveals some info about every ciphertext, specifically whether it is an encryption of \\(1\\) or \\(2\\) or not. She does the following: first, she computes a fresh encryption of \\(1\\), that is she computes \\((c_1&#39;, c_2&#39;) = (g^{r&#39;}, g^1h^{r&#39;})\\). She then computes a fresh ciphertext \\[ (c^*_1, c^*_2) = (c_1, c_2) \\cdot (c_1&#39;, c_2&#39;) \\] and sends it to Alice. She then waits for Alice’s response. If the response is “Thank you for participation” she concludes that Bob voted yes and if it is “Please send an encryption of \\(1\\) for yes and an encryption of \\(2\\) for no” she concludes he voted no. Indeed, let Bob’s ciphertext correspond to \\(\\mathsf{Enc}_{\\mathsf{pk}}(m_1; r_1)\\). By the homomorphic property of ElGamal, the message Eve sends corresponds to \\[ (c^*_1, c^*_2) = (c_1, c_2) \\cdot (c_1&#39;, c_2&#39;) = \\mathsf{Enc}_{\\mathsf{pk}}(m_1; r_1)\\cdot\\mathsf{Enc}_{\\mathsf{pk}}(1; r_2) = \\mathsf{Enc}_{\\mathsf{pk}}(m_1+1; r_1+r_2) \\] We have two cases: If Alice accepts the vote, it must be the case that \\(m_1+1\\in\\{1,2\\}\\). Since \\(m_1\\) is either \\(1\\) or \\(2\\), this means that \\(m_1=1\\) which corresponds to yes. If Alice rejects the vote, it must be the case that \\(m_1+1\\not\\in\\{1,2\\}\\). Since \\(m_1\\) is either \\(1\\) or \\(2\\), this means that \\(m_1=2\\) which corresponds to no. We also make a note here. Eve could just send the ciphertext \\((c_1, c_2\\cdot g^{1})\\) instead of encrypting \\(1\\) and do the same procedure. However, this could alert Alice since she would have received two ciphertexts with the same randomness (which can happen with only negligible probability). Encrypting the ciphertext \\(1\\) adds extra randomness \\(r_2\\) which makes impossible for Alice to make any correlation with Bob’s ciphertext. Pohlig, S., &amp; Hellman, M. (1978). An improved algorithm for computing logarithms over GF(p) and its cryptographic significance (corresp.). IEEE Transactions on information Theory, 24(1), 106-110.↩︎ Boudot, F., Gaudry, P., Guillevic, A., Heninger, N., Thomé, E., &amp; Zimmermann, P. (2020, August). Comparing the difficulty of factorization and discrete logarithm: a 240-digit experiment. In Annual International Cryptology Conference (pp. 62-91). Springer, Cham.↩︎ https://github.com/JeanLucPons/Kangaroo.↩︎ Diffie, W., &amp; Hellman, M. (1976). New directions in cryptography. IEEE transactions on Information Theory, 22(6), 644-654.↩︎ Formally, some description of the group is published as part of the key. For example, if it has been agreed that the group is some \\((\\mathbb{Z}_p^*,\\cdot)\\), then it is enough to publish \\(p\\).↩︎ ElGamal, T. (1985). A public key cryptosystem and a signature scheme based on discrete logarithms. IEEE transactions on information theory, 31(4), 469-472.↩︎ "],["digital-signatures.html", "10 Digital signatures 10.1 Signature schemes 10.2 RSA signatures 10.3 Signing large messages Solved exercises", " 10 Digital signatures In this section, we take a detour from encryption to look at signature schemes, which also follow the public-key paradigm but fulfill a different purpose altogether. More precisely, we will learn about: What are signature schemes and why do we care. An example of signature schemes based on RSA. How to sign large messages. 10.1 Signature schemes Our goal in this section will not be the same as before. No longer we are interested in obscuring the message being sent. Instead, consider the following example. Suppose that Bob receives a message from Alice. How can Bob know that the message was not intercepted in transit by an adversary, and modified before being forwarded to Bob? Moreover, what if the message does not come from Alice at all? To prevent these two problems, we want to somehow ensure: Authenticity: the message comes from the claimed sender. Integrity: the message was not modified in transit. Exercise 10.1 Think of some real-world examples in which failing to provide each of these properties might allow some malicious behaviour. The solution to both is to append a signature to the message. Definition 10.1 A signature scheme is a triple of algorithms \\[(\\mathsf{KeyGen},\\mathsf{Sign},\\mathsf{Verify}).\\] The \\(\\mathsf{KeyGen}\\) algorithm produces a pair of keys \\(\\mathsf{pk}\\) and \\(\\mathsf{sk}\\) of length \\(\\lambda\\) according to some probability distribution. The key \\(\\mathsf{pk}\\) is called the public key and the key \\(\\mathsf{sk}\\) is called thesecret key. The \\(\\mathsf{Sign}\\) algorithm uses the secret key \\(\\mathsf{sk}\\) to produce a signature \\(\\sigma\\) of a message \\(\\mathsf{m}\\) that depends on the message and the key. The \\(\\mathsf{Verify}\\) algorithm uses the public key \\(\\mathsf{pk}\\), the message \\(\\mathsf{m}\\) and the signature \\(\\sigma\\), and outputs \\(1\\) or \\(0\\) depending on whether \\(\\sigma\\) is a valid signature for \\(\\mathsf{m}\\) nor not. Now, instead of sending just \\(\\mathsf{m}\\), they proceed as follows. Alice runs the \\(\\mathsf{KeyGen}\\) algorithm to produce a pair of keys \\((\\mathsf{pk},\\mathsf{sk})\\), and publishes \\(\\mathsf{pk}\\). When Alice wants to sign the message \\(\\mathsf{m}\\), she computes \\[\\sigma=\\mathsf{Sign}_{\\mathsf{sk}}(\\mathsf{m}),\\] and sends \\((\\mathsf{m},\\sigma)\\) to Bob. Upon receiving the message, Bob runs the \\(\\mathsf{Verify}\\) algorithm with \\(\\mathsf{k}\\), \\(\\mathsf{m}\\) and \\(\\sigma\\). The idea is that, with a good signature scheme, no adversary should be able to sign any message without knowledge of the secret key \\(\\mathsf{sk}\\). Therefore: Authenticity is guaranteed, since nobody except for Alice could have signed the message. Integrity is also guaranteed, since an attacker intercepting \\((\\mathsf{m},\\sigma)\\) does not know how to compute a signature for a message different from \\(\\mathsf{m}\\). Exercise 10.2 One might think that using an encryption scheme already solves these issues. However, that is not true. Take, for example, the one-time pad. Show how an attacker against integrity can modify the ciphertext so that it decrypts to a different plaintext. Is there any real-world situations in which this could be troublesome? Note that in encryption the message was first processed with the public key in the \\(\\mathsf{Enc}\\) algorithm and then with the secret key in the \\(\\mathsf{Dec}\\) algorithm. However, signatures first use the secret key in the \\(\\mathsf{Sign}\\) algorithm, and then the public key is used to verify. That is because, in encryption, we wanted that: Anyone can encrypt a message addressed to some party. Only this party can decrypt the messages addressed to them. However, the situation is reversed in signatures, as we want that: Only one party can sign messages on their own behalf. Everyone can verify that the message was indeed signed by this party. Since in principle signatures are not encrypted, an eavesdropper could intercept a pair \\((\\mathsf{m},\\sigma)\\) of message and signature, and resend it later, impersonating the sender. We can prevent this from becoming an effective attack by time-stamping the message, that is, signing a version of the message that includes the time of sending, or an “expiration date”, beyond which the signature is not considered valid. Thus, we will informally say that a signature scheme is secure if no efficient adversary is able to produce, without the secret key, a pair of message and a valid signature for it such that the message is not one of those already eavesdropped by the adversary. 10.2 RSA signatures Interestingly, one can run the RSA encryption scheme “backwards” to obtain a signature scheme. That is, we use the decryption algorithm to sign, and the encryption algorithm to verify. \\(\\mathsf{KeyGen}\\): on input a security parameter \\(\\lambda\\), choose two uniformly random prime numbers \\(p,q\\) of bitlength \\(\\lambda/2\\), and let \\(N=pq\\). Choose \\(e\\in\\mathbb{Z}_N\\), and compute \\[d\\equiv e^{-1}\\pmod{\\varphi(N)}.\\] Output the public key \\[\\mathsf{pk}=(N,e),\\] and the secret key \\[\\mathsf{sk}=d.\\] \\(\\mathsf{Sign}\\): given a message \\(\\mathsf{m}\\in\\mathbb{Z}_N\\), and the sender’s secret key key \\(d\\), output a signature \\[\\mathsf{m}^{d}\\bmod{N}.\\] \\(\\mathsf{Verify}\\): given a message \\(\\mathsf{m}\\), a signature \\(\\sigma\\) and the public key \\((N,e)\\) of the sender, check whether \\[\\sigma^e\\equiv \\mathsf{m}\\pmod{N}.\\] If the equality holds, output \\(1\\) (accept the signature), otherwise output \\(0\\) (reject the signature). It is clear that genuine signatures are accepted, since \\[\\sigma^e\\equiv \\left(\\mathsf{m}^d\\right)^e\\equiv \\mathsf{m}^{ed}\\equiv \\mathsf{m}\\pmod{N},\\] similarly to how we argued in the case of RSA encryption, and the discussion about efficiency translates naturally. However, as it stands, the signature scheme is insecure. Recall that security means that an adversary cannot efficiently produce a pair of message and signature for a new message, given signatures of some messages. Let us assume that an adversary eavesdrops two message-signature pairs \\((\\mathsf{m}_0,\\sigma_0)\\) and \\((\\mathsf{m}_1,\\sigma_1)\\). Then, since \\[\\sigma_0^e\\equiv \\mathsf{m}_0,\\qquad \\text{ and }\\qquad \\sigma_1^e\\equiv \\mathsf{m}_1,\\] we have that \\[(\\sigma_0\\sigma_1)^e\\equiv \\mathsf{m}_0\\mathsf{m}_1.\\] Therefore, \\((\\sigma_0\\sigma_1) \\bmod{N}\\) is a valid signature for the new message \\((\\mathsf{m}_0\\mathsf{m}_1)\\bmod{N}\\). This attack is possible because of the malleability of RSA signatures. That is, given some messages and their signatures, it is easy to manipulate them to produce new signatures for new messages. In particular, in this case the product of two signatures yields the signature of the product of the messages. Thus, to prevent this attack, we will need to somehow sever this link between signatures of related messages. Moreover, there is another problem of a more practical nature. So far, we have achieved a way to sign a relatively short message, since \\(\\mathsf{m}\\in\\mathbb{Z}_N^*\\). We could make \\(N\\) larger to account for larger messages, but this would have a significant impact on efficiency, so we want to look for an alternative approach. In the next section, we will solve these two issues at once. 10.3 Signing large messages We now focus on the problem of producing a signature scheme for an arbitrarily large message, starting from a fixed-length one. More precisely, let us say that we have a signature scheme for messages of length \\(\\ell\\), and we have a message \\(\\mathsf{m}\\) of length \\(2\\ell\\). Our first idea could be to split the message in two halves \\(\\mathsf{m}_0,\\mathsf{m}_1\\), each of them of length \\(\\ell\\), and compute \\[\\sigma_0=\\mathsf{Sign}_{\\mathsf{sk}}(\\mathsf{m}_0),\\qquad\\sigma_1=\\mathsf{Sign}_{\\mathsf{sk}}(\\mathsf{m}_1),\\] and send \\((\\mathsf{m},\\sigma)\\), where \\(\\sigma=(\\sigma_0|\\sigma_1)\\). However, two problems arise, one theoretical and one practical: A secure signature scheme should ensure that an adversary cannot sign any new message. But the message \\((\\mathsf{m}_1|\\mathsf{m}_0)\\) is different from the message \\((\\mathsf{m}_0|\\mathsf{m}_1)\\), and it is easy to provide the signature \\((\\sigma_1|\\sigma_0)\\). Thus, our idea does not yield a secure signature scheme. Moreover, this becomes highly impractical as the message grows in size. Assume that we are sending a video file, which could be several gigabytes large. The signature is roughly as large as the message, and thus we are sending a lot of extra information just for authentication and integrity. Exercise 10.3 In an effort to defeat these issues, we consider simply signing the first block of length \\(\\ell\\) of the message, regardless of the size of the message. What is the issue with this? Surely we can do better. What if, somehow, we could compress the whole message in just one block of length \\(\\ell\\), and we sign that block? And don’t we have a tool that does exactly that? Indeed, we can use a hash function. Given a fixed-length signature scheme \\[(\\mathsf{KeyGen},\\mathsf{Sign}, \\mathsf{Verify})\\] and a public hash function \\(H\\), we build an arbitrary-length signature scheme as follows. The \\(\\mathsf{KeyGen}\\) algorithm stays the same. To sign an arbitrarily-large message \\(\\mathsf{m}\\), we compute: \\[\\sigma=\\mathsf{Sign}_{\\mathsf{sk}}(H(\\mathsf{m})).\\] That is, we reduce the problem to signing a fixed-length message \\(h\\), by hashing the original message. We make the following observations: The hash function must be public and deterministic, because both sender and recipient need to make use of it to compute \\(H(\\mathsf{m})\\). Although \\(H(\\mathsf{m})\\) can be computed by adversaries, they cannot sign it on their own because they lack the secret key \\(\\mathsf{sk}\\). Note that this also solves our security issue highlighted in the previous section, which stemmed from signatures of related messages being related. The intuition is that the outputs of a good hash function are “random-looking”, regardless of the inputs being related. This variant of RSA signatures is described below, and is known as the full-domain hash signature scheme. \\(\\mathsf{KeyGen}\\): on input a security parameter \\(\\lambda\\), choose two uniformly random prime numbers \\(p,q\\) of bitlength \\(\\lambda/2\\), and let \\(N=pq\\). Let \\(H:\\{0,1\\}^*\\rightarrow\\mathbb{Z}_N\\) be a hash function. Choose \\(e\\in\\mathbb{Z}_N\\), and compute \\[d\\equiv e^{-1}\\pmod{\\varphi(N)}.\\] Output the public key \\[\\mathsf{pk}=(N,e),\\] and the secret key \\[\\mathsf{sk}=d.\\] \\(\\mathsf{Sign}\\): given a message \\(\\mathsf{m}\\in\\{0,1\\}^*\\), and the sender’s secret key key \\(d\\), output a signature \\[H(\\mathsf{m})^{d}\\bmod{N}.\\] \\(\\mathsf{Verify}\\): given a message \\(\\mathsf{m}\\), a signature \\(\\sigma\\) and the public key \\((N,e)\\) of the sender, check whether \\[\\sigma^e\\equiv H(\\mathsf{m})\\pmod{N}.\\] If the equality holds, output \\(1\\) (accept the signature), otherwise output \\(0\\) (reject the signature). Proposition 10.1 If the RSA problem is hard and \\(H\\) behaves as an ideal49 hash function, then the full-domain hash signature scheme is secure. Solved exercises Exercise 10.4 Let \\(N = 187\\), \\(e = 7\\) be an RSA signature public key. Check if \\(\\sigma = 168\\) is a valid signature for \\(m = 42\\). Compute the secret signing key. Sign the message \\(9\\). Solution. We simply need to use the verification algorithm for RSA signature. That is, we need to check if \\(\\sigma^e \\equiv m\\pmod{N}\\). Applying the given values we have: \\[ \\begin{aligned} \\sigma^e &amp;\\equiv 168^7 \\equiv (-19)^7 \\equiv 15^{1+2+4} \\equiv (-19)^{1}\\cdot (-19)^{2}\\cdot (-19)^{4} \\equiv \\ldots \\equiv 42\\pmod{187} \\end{aligned} \\] Therefore the equation is satisfied and we \\(168\\) is a valid signature on \\(42\\). Due to security of RSA this cannot be done efficiently, but since here we deal with really small numbers we can indeed compute the secret key. After some trial and error, we can see that \\(N = 187 = 11\\cdot 17\\) and therefore \\(p=11\\) and \\(q=17\\). With these values we can compute \\(\\phi(N) = (p-1)(q-1) = 10\\cdot 16 = 160\\). Finally, to compute the secret key we need to compute the value \\(e^{-1}\\pmod{\\phi(N)} = 7^{-1}\\pmod{160}\\). Using the Extended Euclidian Algorithm we have that this value is \\(d\\equiv 23\\pmod{160}\\). Having the secret key \\(d\\) we can sign the message \\(9\\) by computing \\(m^d\\ equiv 9^{23}\\pmod{N}\\). We have \\[ 9^23 \\equiv 9^{16+4+2+1} \\equiv 9^{16}\\cdot 9^4\\cdot 9^2\\cdot 9^1 \\equiv \\ldots \\equiv 36 \\pmod{187} \\] and therefore \\(36\\) is the signature of \\(9\\) under the given public key. Exercise 10.5 Alice has an RSA public key \\(N=46339\\), \\(e=3\\). She wants to sign the message \\(7\\) and send it to Bob. To avoid the malleability issues of simple RSA, she uses full-domain hash RSA with \\(H(x) = x\\mod 100\\) as a hash function. Demonstrate that the choice of the hash function is not good by forging a signature assuming you have seen the message, signature pair \\(m = 85722, \\sigma = 20787\\) Solution. The choice is not good since the hash function is not collision resistant. Specifically, for every \\(x, y = x + k\\cdot 100\\), we have \\(H(x)=H(y)\\). Therefore, we can compute a signature for any message \\(m&#39; = 85722 + k\\cdot 100\\) for any \\(k\\). Let’s for example pick \\(m&#39; = 85722 - 42\\cdot 100 = 81522\\). We claim that \\(\\sigma = 20787\\). Indeed, we have \\(H(m&#39;)=m&#39;\\mod 100 = 22\\) and \\(\\sigma^e \\equiv 20787^{3} \\equiv 22\\pmod {46339}\\). Exercise 10.6 Recall from Section 8.4 that we can use the Chinese Remainder Theorem to speed up message decryption. Since the same computation is used to sign messages via RSA, we can use this optimization for computing RSA signatures as well. Specifically, the signer first represents the secret key \\(d\\) as \\[ d_p = d\\pmod {\\phi(p)}, \\qquad d_q = d\\pmod {\\phi(q)} \\] and signs the message \\(m\\) by computing \\[ \\sigma_p = m^{d_{p}}\\pmod p, \\qquad m^{d_{q}}\\pmod q \\] and using the Chineese remainder theorem to find the unique \\(\\sigma \\pmod{N}\\) such that \\[ \\sigma \\equiv \\sigma_p \\pmod p,\\qquad \\sigma \\equiv \\sigma_q \\pmod q \\] Let \\(N=7\\cdot 11 = 77\\) be an RSA modulus with \\(e=7\\) and \\(d\\equiv e^{-1}\\equiv 43\\pmod{\\phi(N)}\\). Sign the message \\(m=10\\) using the above procedure. Due to an implementation bug a false signature \\(\\sigma&#39;\\) on a message \\(m\\) is computed. Specifically, during computation the \\(\\sigma_p \\not\\equiv m^{d_{p}}\\pmod p\\). Later you obtain the valid signature \\(\\sigma\\). Explain how in this case the modulus \\(N\\) can be efficiently factored. Solution. We have \\(\\phi(N)=6\\cdot 10 = 60\\). We now compute the \\(p\\) and \\(q\\) parts of the key. We have: \\[ \\begin{aligned} d_p&amp;\\equiv d\\pmod{p}\\equiv 43\\pmod 7\\equiv 1\\pmod{7} \\\\ d_q&amp;\\equiv d\\pmod{q}\\equiv 43\\pmod {11}\\equiv 44-1\\equiv -1\\pmod{11}, \\end{aligned} \\] so \\((d_p, d_q) = (1\\mod 7,-1\\mod 11)\\). We next compute \\(\\sigma_p, \\sigma_q\\). We have \\[ \\begin{aligned} \\sigma_p&amp;\\equiv m^{d_p}\\pmod{p}\\equiv 10^1\\pmod 7\\equiv 3\\pmod{7} \\\\ \\sigma_q&amp;\\equiv m^{d_q}\\pmod{p}\\equiv 10^{-1}\\pmod {11}\\equiv 10\\equiv -1\\pmod{11} \\end{aligned} \\] Finally, we use the CRT to find \\(\\sigma\\). We need to compute the value \\[ \\sigma = (q\\sigma_p(q^{-1}\\mod p) + p\\sigma_q(p^{-1}\\mod q))\\pmod N \\] Applying the values we computed we have \\[ \\begin{aligned} \\sigma &amp;\\equiv 11\\cdot 3 (11^{-1}\\mod 7) + 7\\cdot (-1)\\cdot (7^{-1}\\mod 11)\\\\ &amp;\\equiv 11\\cdot 3 \\cdot 2 + 7\\cdot (-1)\\cdot 8\\\\ &amp;\\equiv 66 + (-56)\\\\ &amp;\\equiv 10\\pmod{77}\\\\ \\end{aligned} \\] We can verify that \\(\\sigma^{e}\\equiv 10^7 \\equiv 10 \\equiv m \\pmod{77}\\) so \\(\\sigma = 10\\) is a valid signature for \\(m=10\\). We have two signatures on \\(m\\), \\(\\sigma\\) which is a valid one, and \\(\\sigma&#39;\\) which is an invalid one. Additionally, we know that the error that happened on \\(\\sigma&#39;\\) was when computing the \\(\\mod p\\) part. For the valid signature \\(\\sigma\\) let \\[ \\sigma \\equiv \\sigma_p \\pmod p, \\qquad \\sigma \\equiv \\sigma_q \\pmod q, \\] For the invalid signature \\(\\sigma&#39;\\) we know that the value \\(\\sigma_p\\) was not correct but \\(\\sigma_q\\) was. Therefore \\[ \\sigma&#39; \\not\\equiv \\sigma_p \\pmod p, \\qquad \\sigma&#39; \\equiv \\sigma_q \\pmod q, \\] Now, consider the value \\(\\sigma - \\sigma&#39;\\). When in \\(\\mathbb{Z}_q\\), we have \\(\\sigma-\\sigma&#39; \\equiv 0 \\pmod q\\). When in \\(\\mathbb{Z}_p\\), however, we have \\(\\sigma-\\sigma&#39; \\not\\equiv 0 \\pmod p\\). This means that \\(q\\) divides \\(\\sigma - \\sigma&#39;\\) but \\(p\\) doesn’t. Since \\(N=p\\cdot q\\), this means the greatest common divisor of \\(N, \\sigma-\\sigma&#39;\\) is \\(q\\)! We can therefore efficiently compute this value through the Euclidian Algorithm to get \\(q\\) and then compute \\(p=N/q\\). Exercise 10.7 (*) In some cases, we might need to receive a signature on a private message. A good example for it is in voting. When casting a vote, you secretly select a choice and enclose it in an envelope in order to hide it. You then give the envelope to an official who makes some checks (you are eligible to vote, haven’t voted etc) and signs the envelope to mark it as a valid ballot. Essentially, the official “signs” your vote without knowing it. This is achieved through the physical means of the envelope. Blind signatures are the cryptographic analogue of the above. Alice, who wants to receive a signature on a message \\(m\\), first “blinds” the message and gives the blinded message to Bob. Bob -without learning anything about the message- signs the blinded message and gives the “blind” signature to Alice, who “unblinds” it to get a signature on \\(m\\). Alice can now convince anyone that Bob authorised her message while Bob does not know which message Alice has! RSA signatures can be used as blind signatures as follows: The key pair remains unchanged. The secret key is \\(d\\) and the public key \\((N, e)\\) as in simple RSA. Alice blinds a message \\(m\\) by picking a random \\(r\\in\\mathbb{Z}_n\\) and computing \\(\\tilde{m} = m\\cdot r^e\\pmod N\\). Bob signs the blinded message \\(\\tilde{m}\\) by simply computing an RSA signature on it, that is, it computes \\(\\tilde{\\sigma} = \\tilde{m}^d\\pmod N\\) and gives it to Alice. Alice unblinds the signature by computing \\(\\sigma = \\tilde{\\sigma}\\cdot r^{-1}\\). The signature on \\(m\\) is \\(\\sigma\\). She sends \\(m,\\sigma\\) to Charles. Charles runs the normal RSA verification to check the signature. That is, it checks whether \\(\\sigma^e\\equiv m\\pmod N\\). Let \\(N=77, e=7\\). Compute the blidned message for \\(m=7\\) by using randomness \\(19\\). Then compute the blind signature for this message and unblind it. Does it verify? Show that Charles will always accept the signature if Alice and Bob follow the above steps. Bob knows that Alice’s message is either “yes” or “no”, each encoded as a number \\(\\mod N\\). Argue that Bob cannot tell Alice’s vote by looking at \\(\\tilde{m}\\). Solution. To blind the message we need to compute \\(\\tilde{m} = m\\cdot r^e\\pmod N\\). Plugging in the numbers we have \\[ \\tilde{m} \\equiv 7\\cdot 19^{7} \\equiv \\ldots \\equiv 14\\pmod{77} \\] Next we need an RSA signature on this message. Since the numbers are small we can efficiently compute the secret key. We have \\(N=7\\cdot 11\\) and therefore \\(\\phi(N)=6\\cdot 10 = 60\\). The value \\(d\\) is the multiplicative inverse of \\(e=7\\) \\(\\mod 60\\). Running the Extended Euclidian Algorithm we get \\(d=43\\). We sign the message by computing \\(\\tilde{\\sigma} = \\tilde{m}^d\\pmod N\\). We have \\[ \\tilde{\\sigma} \\equiv \\tilde{m}^d \\equiv 14^{43} \\equiv \\ldots \\equiv 49 \\pmod{77} \\] To unblinded the signature we need to multiply it by \\(r^{-1}\\equiv 19^{-1}\\pmod{77}\\). Using the Extended Euclidian Algorithm we get \\(19^{-1}\\equiv 73\\equiv -4\\pmod{77}\\). Therefore \\[ {\\sigma} \\equiv \\tilde{\\sigma}\\cdot r^{-1} \\equiv 49\\cdot (-4) \\equiv \\ldots \\equiv 35 \\pmod{77} \\] Finally, let’s check if \\(\\sigma = 11\\) is a valid signature on \\(m=11\\). We have \\[ \\sigma^e \\equiv 35^7 \\equiv \\ldots\\equiv 7\\equiv m\\pmod{77} \\] and therefore the signature verifies. We need to verify that \\(\\sigma^e\\equiv m\\pmod{N}\\) where \\(\\sigma\\) is computed as above. We have \\[ \\begin{aligned} \\sigma &amp;\\equiv \\tilde{\\sigma}\\cdot r^{-1}\\\\ &amp;\\equiv \\tilde{m}^{d}\\cdot r^{-1} \\\\ &amp;\\equiv (m\\cdot r^e)^{d}\\cdot r^{-1}\\\\ &amp;\\equiv m^d\\cdot r^{ed}\\cdot r^{-1}\\\\ &amp;\\equiv m^d\\cdot r^{1}\\cdot r^{-1}\\\\ &amp;\\equiv m^d \\pmod{N}\\\\ \\end{aligned} \\] where in the above we used that \\(r^{ed} \\equiv r^{k\\phi(N)+1}\\equiv 1\\). Finally we have \\(\\sigma^{e} \\equiv m^{ed} \\equiv m \\pmod{N}\\) and the signature is valid. Let \\(m_y\\in \\mathbb{Z}_n\\) be the value corresponding to “yes” and \\(m_n\\in \\mathbb{Z}_n\\) be the value corresponding to “no”. What Bob actually sees is the value \\(\\tilde{m} \\equiv m\\cdot r^{e}\\pmod N\\) for \\(m\\) being either \\(m_y\\) or \\(m_n\\) and \\(r\\) be a random value. We will show that there exists a unique \\(r_y\\) to satisfy the equation for \\(m=x_y\\) and a unique \\(r_n\\) to satisfy the equation for \\(m=x_n\\). Therefore there are two possible explanations: Alice voted “yes” and chose the random value \\(r_y\\) or Alice voted “no” and chose the random value \\(r_n\\) and since \\(r\\) is random there is no good reason for Bob to believe one is more likely than the other. Let’s solve the above equation for \\(r\\): \\[ \\begin{aligned} \\tilde{m} \\equiv m\\cdot r^{e}\\pmod N &amp;\\Leftrightarrow \\tilde{m}\\cdot m^{-1} \\equiv r^{e}\\pmod N \\\\ &amp;\\Leftrightarrow (\\tilde{m}\\cdot m^{-1})^{d} \\equiv r^{ed}\\pmod N \\\\ &amp;\\Leftrightarrow (\\tilde{m}\\cdot m^{-1})^{d} \\equiv r\\pmod N \\\\ \\end{aligned} \\] So, either Alice selected “yes” and flipped the dice to \\(r_y = (\\tilde{m}\\cdot m_y^{-1})^d\\) or she selected “no” and flipped the dice to \\(r_n = (\\tilde{m}\\cdot m_n^{-1})^d\\). See Proposition 7.1.↩︎ "],["secret-sharing.html", "11 Secret sharing 11.1 Secret sharing 11.2 The Shamir secret sharing scheme 11.3 Threshold decryption in the ElGamal encryption scheme Solved exercises", " 11 Secret sharing In this section, we look at yet another cryptographic primitive, with a different functionality than encryption or signatures. We will learn about: Secret sharing. Polynomial interpolation and the Shamir secret sharing scheme. Threshold decryption in the ElGamal encryption scheme. 11.1 Secret sharing Sometimes, we want to ensure that some action requires more than one party to execute. One dramatic example, often depicted in popular media, is the two-man concept for the launch of nuclear weapons.50 The idea is that, to launch a nuclear missile, two operators must insert their respective keys in locks that are physically distant. This prevents a single operator from launching the missiles. The important observation here is that a single key is useless by itself, yet is it is absolutely necessary to proceed with the launch. In a sense, each of the keys is actually part of the key that controls the launch. In this section, we will look at the cryptographic version of that. Let us assume that we want to make some information \\(\\mathsf{m}\\) difficult to access unless \\(n\\) different parties collaborate, and let us denote the parties by the numbers \\(1\\) to \\(n\\). We call \\(\\mathsf{m}\\) the secret. Each party \\(i\\) will hold some information \\(s_i\\) called a share. The shares work in a way such that, by themselves, they are useless. However, if the parties reveal the shares to each other, then anyone with all the shares will be able to recover the secret. Moreover, we generalize this definition to admit situations in which not all shares are necessary to recover the secret. For example, we might want a situation in which we have \\(5\\) parties, and any \\(3\\) shares are enough to recover the secret, but any \\(2\\) shares reveal nothing about the secret. We will call this a \\(3\\)-out-of-\\(5\\) secret sharing scheme. We present a more formal definition below. Definition 11.1 Let \\(t,n\\in\\mathbb{N}\\). Av\\(t\\)-out-of-\\(n\\) secret sharing scheme is a pair of algorithms \\[(\\mathsf{Share},\\mathsf{Reconstruct}).\\] The \\(\\mathsf{Share}\\) algorithm takes the secret \\(\\mathsf{m}\\) and produces the set of shares \\(\\{s_1,\\dots,s_n\\}\\). This algorithm might be randomized. The \\(\\mathsf{Reconstruct}\\) algorithm takes as input a subset of the shares, and outputs a message. For the scheme to work correctly, we want that \\(\\mathsf{Reconstruct}\\) outputs \\(\\mathsf{m}\\) whenever it is fed with a subset of \\(\\{s_1,\\dots,s_n\\}\\) of size at least \\(t\\). According to this definition, the example at the beginning of the section would be a \\(2\\)-out-of-\\(2\\) secret sharing scheme. In general, \\(t\\)-out-of-\\(n\\) secret sharing schemes are called threshold secret sharing schemes, since we need to reach a threshold \\(t\\) of shares for the secret to be recoverable. More complex access structures, like subsets of shares of variable sizes, are possible, but we will not cover these in the course. Another aspect of secret sharing that we will not look at, but is worth mentioning, is the fact that whoever runs the \\(\\mathsf{Share}\\) algorithm knows the secret and all the shares, so this requires, in principle, that every other party trusts a sole party to generate the shares. Informally, we will say that a \\(t\\)-out-of-\\(n\\) secret sharing scheme is secure if any set of less than \\(t\\) shares reveals absolutely no information about the secret. Consider, for example, the following candidate for a \\(2\\)-out-of-\\(2\\) secret sharing scheme: \\(\\mathsf{Share}\\): given \\(\\mathsf{m}\\in\\{0,1\\}^\\lambda\\), split \\(\\mathsf{m}\\) in two halves and set each share as one of these halves. \\(\\mathsf{Reconstruct}\\): given two shares, concatenate them to recover the secret. It is clear that this will not be secure, because knowing one share gives away half of the bits of the secret. A \\(2\\)-out-of-\\(2\\) scheme that does work uses something that we used at the very beginning of the course: the one-time pad. \\(\\mathsf{Share}\\): given \\(\\mathsf{m}\\in\\{0,1\\}^\\lambda\\), choose a uniformly random \\(s_1\\in\\{0,1\\}^\\lambda\\) and set \\(s_2=s_1\\oplus\\mathsf{m}\\). \\(\\mathsf{Reconstruct}\\): given two shares \\(s_1\\) and \\(s_2\\), output \\(s_1\\oplus s_2\\). Clearly, this works because \\[s_1\\oplus s_2 = s_1\\oplus(s_1\\oplus\\mathsf{m})=\\mathsf{m},\\] since \\(s_1\\oplus s_1 = 0\\). The security relies on the perfect secrecy of the one-time pad: given one of the shares without the other, the message could be anything in \\(\\{0,1\\}^\\lambda\\). 11.2 The Shamir secret sharing scheme We introduce a \\(t\\)-out-of-\\(n\\) secret sharing scheme due to Shamir, who you might remember as the S in the RSA cryptosystem.51 The scheme relies on the idea of polynomial interpolation, which is explained in Appendix C. We know that a polynomial of degree \\(t-1\\) is uniquely determined by \\(t\\) pairs of point and image through the polynomial. Shamir’s idea is to encode the secret \\(\\mathsf{m}\\) into the polynomial, and give one evaluation to each party, so that any \\(t\\) parties can put together their shares and recover the polynomial by interpolation. We describe the scheme more precisely below. \\(\\mathsf{Share}\\): on input a security parameter \\(\\lambda\\), choose a prime number \\(q\\) of bitlength \\(\\lambda\\). Choose uniformly random coefficients \\(a_1,\\dots,a_{t-1}\\in\\mathbb{Z}_q\\), and define the following polynomial of degree \\(t-1\\): \\[f(X)=\\mathsf{m}+\\sum_{i=1}^{t-1}a_iX^i.\\] For \\(i=1,\\dots,n\\), set the share corresponding to user \\(i\\) as \\[s_i=(i,f(i)\\bmod{q}).\\] \\(\\mathsf{Reconstruct}\\): given \\(t\\) different shares, compute the Lagrange interpolation polynomial \\(f(X)\\) corresponding to them (with operations modulo \\(p\\)), and output \\(f(0)\\). The scheme works because all the shares come from the same polynomial of degree \\(t-1\\), and thus all interpolations from \\(t\\) shares yield the same polynomial \\(p(X)\\), which is necessarily the one used to create the shares in the first place. Given that polynomial, it is clear that \\(f(0)=\\mathsf{m}\\). The security comes from the fact that any \\(t-1\\) shares reveal nothing about the evaluation at \\(0\\). To see this, assume that we have \\(t-1\\) shares \\((x_i,y_i)\\), for \\(i=1,\\dots,t-1\\). Then, for any possible \\(\\mathsf{m}\\), there is a unique polynomial \\(f_\\mathsf{m}(X)\\) of degree \\(t-1\\) such that \\[f_\\mathsf{m}(0)=\\mathsf{m},\\qquad \\text{and} \\qquad f(x_i)=y_i \\quad \\text{ for all }i=1,\\dots,t-1.\\] That is, with only \\(t-1\\) shares, the secret \\(\\mathsf{m}\\) could still be anything! Proposition 11.1 The Shamir secret sharing scheme is a secure threshold secret sharing scheme. We also note that an explicit formula for the secret \\(\\mathsf{m}=p(0)\\) in terms of the interpolation points can be deduced directly from the interpolation polynomial formula. Indeed, since \\[f(X)=\\sum_{i=0}^ty_i\\ell_i(X),\\qquad \\text{where} \\qquad \\ell_i(X)=\\prod_{j\\neq i}\\frac{X-j}{i-j},\\] we have that \\[\\mathsf{m}=f(0)=\\sum_{i=0}^ty_i\\prod_{j\\neq i}\\frac{-j}{i-j}.\\] Thus, we actually do not need to interpolate the whole polynomial, as we only require the evaluation at \\(0\\). 11.3 Threshold decryption in the ElGamal encryption scheme In a normal public-key encryption scheme, the secret key, which is used for decryption, is stored in a single place. This creates a single point of failure, as if this place becomes compromised, then the attacker will be able to decrypt any messages in the past encrypted with the corresponding public key. One way to prevent this is to use secret sharing to split the secret key, and give the shares to different parties. A certain number of parties will be required to decrypt. This is known as threshold decryption. The crucial point is that they do not reconstruct the key, so no single party knows the whole key at any point in the process. Instead, the decryption is achieved by a combination of partial decryptions, in which each party uses their share to partially decrypt, and then these partial decryptions are combined to recover the original message. Below, we show how to combine the ElGamal encryption scheme with the Shamir secret sharing scheme to achieve \\(t\\)-out-of-\\(n\\) threshold decryption. \\(\\mathsf{KeyGen}\\): first we run the usual ElGamal key generation. On input a security parameter \\(\\lambda\\), choose a cyclic group \\(\\mathbb{G}\\) of prime order \\(q\\), and a generator \\(g\\) of \\(\\mathbb{G}\\). We will write \\(\\mathbb{G}\\) with multiplicative notation. Sample a uniformly random \\(x\\in\\mathbb{Z}_q\\), and set \\(h=g^x\\). Set the public key \\[\\mathsf{pk}=(\\mathbb{G}, g, h),\\] and the secret key \\[\\mathsf{sk}=x.\\] Then, we use Shamir secret sharing to split the secret key \\(\\mathsf{sk}\\), producing \\(n\\) shares \\(\\mathsf{sk}_i=(i,y_i)\\), for \\(i=1,\\dots,n\\). Output \\((\\mathsf{pk},\\mathsf{sk}_1,\\dots,\\mathsf{sk}_n)\\). \\(\\mathsf{Enc}\\): given a message \\(\\mathsf{m}\\in\\mathbb{Z}_q\\), with \\(\\mathsf{m}\\) small, and the receiver’s public key \\((\\mathbb{G}, g, h)\\), choose a uniformly random \\(r\\in\\mathbb{Z}_q\\) and output the ciphertext \\[\\mathsf{c}=(\\mathsf{c}_1,\\mathsf{c}_2)=(g^r,g^mh^r).\\] \\(\\mathsf{Dec}\\): each party \\(i\\), given a ciphertext \\(\\mathsf{c}=(\\mathsf{c}_1,\\mathsf{c}_2)\\) and their secret key share \\(\\mathsf{sk}_i=(i,y_i)\\), computes the partial decryption \\(\\mathsf{c}_1^{y_i}\\), and sends it to one agreed party that will take care of reconstructing the message. This designated party, once \\(t\\) partial decryptions have arrived, computes \\[\\alpha=\\prod_{i=1}^t\\left(\\mathsf{c}_1^{y_i}\\right)^{w_i},\\qquad \\text{where} \\qquad w_i=\\ell_i(0)=\\prod_{j\\neq i}\\frac{-j}{i-j}.\\] Finally, the message is recovered by computing \\[\\log_g\\frac{\\mathsf{c}_2}{\\alpha}.\\] To see that this works, we just need to ensure that \\(\\alpha=\\mathsf{c}_1^x\\) since, if that is the case, then decryption is just as in the usual ElGamal encryption scheme. Equivalently, we need to prove that \\[x=\\sum_{i=1}^ty_iw_i.\\] The expression on the right is exactly the one given at the end of Section 10.2, which tells us that indeed this formula recovers the secret \\(x\\). Note that each party kept their share \\(y_i\\) during the process, never revealing it. The only information being sent is \\(\\mathsf{c}_1^{y_i}\\), but we expect \\(y_i\\) to be hard to recover from here, due to the hardness of the discrete logarithm problem. Remark. Recall that in the case of safe-prime groups, two fields are involved: a field \\(\\mathbb{Z}_p\\) such that \\(\\mathbb{G}\\) is a subgroup of the unit group \\(\\mathbb{Z}_p^*\\), a field \\(\\mathbb{Z}_q\\) with \\(q = (p-1)/2\\) such that \\(\\text{ord}(\\mathbb{G}) = q\\). The message and secret key (which is our shared secret in the threshold case) live in \\(\\mathbb{Z}_q\\). Therefore, when working on the exponents, we do operations \\(\\mod q\\) and otherwise (i.e. when we work on \\(\\mathbb{G}\\)) we do operations \\(\\mod p\\). Solved exercises Exercise 11.1 You want to share the secret \\(m=11\\) over \\(\\mathbb{Z}_{23}\\). Share the secret to 5 parties in a way such that at least 4 of them are needed to recover the secret. Show how parties 1, 2, 4 and 5 would reconstruct the secret. Solution. We first need to run the \\(\\textsf{Share}\\) algorithm. The total number of parties are \\(n=5\\) and the desired threshold is \\(t=4\\). The first step is to pick a random polynomial \\(f(X)\\) of degree \\(d = t-1=3\\) such that \\(f(0)=m=11\\). We select the random polynomial \\(f(X) = 4X^3 + 10X^2 + 2X + 11\\). Note that the coefficients are elements of \\(\\mathbb{Z}_{23}\\). We next evaluate it at \\(5\\) distinct points. We have: \\[ \\begin{aligned} f(1) &amp;= 4\\cdot 1^3 + 10\\cdot 1^2 + 2\\cdot 1^1 + 11 = 4 \\pmod{23} \\\\ f(2) &amp;= 4\\cdot 2^3 + 10\\cdot 2^2 + 2\\cdot 2^1 + 11 = 18 = -5 \\pmod{23} \\\\ f(3) &amp;= 4\\cdot 3^3 + 10\\cdot 3^2 + 2\\cdot 3^1 + 11 = 8 \\pmod{23} \\\\ f(4) &amp;= 4\\cdot 4^3 + 10\\cdot 4^2 + 2\\cdot 4^1 + 11 = 21 = -2 \\pmod{23} \\\\ f(5) &amp;= 4\\cdot 5^3 + 10\\cdot 5^2 + 2\\cdot 5^1 + 11 = 12 \\pmod{23} \\\\ \\end{aligned} \\] We share the values \\[ (1, 4),\\quad (2,-5), \\quad (3,8), \\quad (4,-2), \\quad (5,12) \\] The first element of each pair, identifying the point on which we evaluate the polynomial, is public while the second, i.e. the evaluation itself is private. Let’s see how parties 1, 2, 4 and 5 can use the shares to recover the secret \\(m\\). First we compute the Lagrange polynomials corresponding to the three parties. Note that these can be computed without knowing any secret values since they only depend on the values where we evaluate the polynomial. In these case, the values that correspond to these parties are \\(1,2,4, 5\\) respectively. We don’t need to compute the actual polynomial \\(f(X)\\), knowing its value on \\(0\\) is enough. These means that could compute \\(\\ell_i(0)\\) instead of \\(\\ell_i(X)\\). Nevertheless, we compute \\(\\ell_1(X)\\) for demonstrative reasons. \\[ \\begin{aligned} \\ell_{1}(X) &amp;= \\prod_{j\\in\\{1,2,4,5\\}, j\\neq 1} \\frac{X-j}{i-j} = \\frac{X-2}{1-2}\\frac{X-4}{1-4}\\frac{X-5}{1-5}\\\\ &amp;= \\frac{X-2}{-1}\\frac{X-4}{-3}\\frac{X-5}{-4} = \\frac{(X^2-6X+8)(X-5)}{(-12)}\\\\ &amp;= \\frac{X^3-6X^2+8X-5X^2+30X-40}{-12}\\\\ &amp;= \\frac{X^3-11X^2+38X-40}{-12}\\\\ &amp;= \\frac{X^3-11X^2+15X-17}{-12}\\\\ &amp;= (-12)^{-1}X^3-(-12)^{-1}11X^2+(-12)^{-1}15X-(-12)^{-1}17\\\\ &amp;= (-2)\\cdot X^3-(-2)\\cdot 11X^2+(-2)\\cdot 15X-(-2)\\cdot 17\\\\ &amp;= -2 X^3+22\\cdot X^2-30X+34\\\\ &amp;= -2 X^3- X^2-7X+11\\\\ \\end{aligned} \\] In the above we used the fact that \\(8^{-1}\\equiv 3\\pmod{23}\\). Recall that the \\(i\\)-th Lagrange polynomial has the property that it evaluates to \\(0\\) in the whole set it is defined over except the point \\(i\\), it evaluates to \\(1\\) for \\(X=1\\) (always \\(\\mod 23\\)!). Let’s verify this. We have: \\[ \\begin{aligned} \\ell_1(1) &amp;= -2\\cdot 1^3 -1 \\cdot 1^2 -7\\cdot 1^1 + 11 = 1 \\\\ \\ell_1(2) &amp;= -2\\cdot 2^3 -1 \\cdot 2^2 -7\\cdot 2^1 + 11 = -16 - 4 -14+11 = -23 = 0\\\\ \\ell_1(4) &amp;= -2\\cdot 4^3 -1 \\cdot 4^2 -7\\cdot 4^1 + 11 = -128 - 16 -28+11 = -161 = 0\\\\ \\ell_1(4) &amp;= -2\\cdot 5^3 -1 \\cdot 5^2 -7\\cdot 5^1 + 11 = -250 - 25 -35+11 = -299 = 0\\\\ \\end{aligned} \\] Note that we only need the value \\(\\ell_1(0)=0\\) for the reconstruction. In the following we only compute the values \\(\\ell_i(0)\\) to significantly reduce calculations. \\[ \\begin{aligned} \\ell_{2}(0) &amp;= \\prod_{j\\in\\{1,2,4,5\\}, j\\neq 2} \\frac{0-j}{i-j} = \\frac{-1}{2-1}\\frac{-4}{2-4}\\frac{-5}{2-5} = -20\\cdot 6^{-1} = -20\\cdot 4 = -80 = 12 \\\\ \\ell_{4}(0) &amp;= \\prod_{j\\in\\{1,2,4,5\\}, j\\neq 4} \\frac{0-j}{i-j} = \\frac{-1}{4-1}\\frac{-2}{4-2}\\frac{-5}{4-5} = -10\\cdot (-6^{-1}) = -10\\cdot (-4) = 40 = 17\\\\ \\ell_{5}(0) &amp;= \\prod_{j\\in\\{1,2,4,5\\}, j\\neq 5} \\frac{0-j}{i-j} = \\frac{-1}{5-1}\\frac{-2}{5-2}\\frac{-4}{5-4} = -8\\cdot 12^{-1} = -8\\cdot 2 = -16 = 7 \\\\ \\end{aligned} \\] Once again note that the above values are independent of the secret polynomial and can be computed by the public values. The secret polynomial is can be written as \\[ f(X) = f(1)\\ell_1(X) + f(2)\\ell_2(X) + f(4)\\ell_4(X) + f(5)\\ell_5(X) \\] and the secret is the value \\(f(0)\\). Substituting \\(X\\) for \\(0\\) in the above we have \\[ \\begin{aligned} f(0) &amp;= f(1)\\ell_1(0) + f(2)\\ell_2(0) + f(4)\\ell_4(0) + f(5)\\ell_5(0) \\\\ &amp;= 11f(1) + 12f(2) + 17f(4) + 7f(5) \\\\ \\end{aligned} \\] It remains to plug in the above equation the secret shares corresponding to the points \\(1,2,4,5\\). We have \\[ \\begin{aligned} f(0) &amp;= 11\\cdot 4 - 12\\cdot 5 - 17\\cdot 2 + 7\\cdot 12 = 34 = 11 \\end{aligned} \\] and thus we reconstructed the secret \\(m=11\\). Exercise 11.2 Let \\(p=47\\) and consider the subgroup \\(\\mathbb{G} = \\langle 2\\rangle\\) of \\(\\mathbb{Z}_{47}^*\\) with order \\(q=23\\). Sample an ElGamal key pair and distribute the secret key to 5 parties such that any pair can decrypt a message. Use the public values \\(1,2,3,4,5\\). Encrypt the message \\(6\\in\\mathbb{G}\\) using the public key. Show how the parties \\(3,5\\) would decrypt the message. Solution. Recall that an ElGamal secret key is an element of \\(\\mathbb{Z}_q\\). Lets pick \\(x=5\\) as the secret key. The corresponding public key is \\(h=2^5 = 32\\in\\mathbb{G}\\). Let’s now secret share the secret key \\(x\\in\\mathbb{Z}_q\\). We have \\(n=5\\) parties and a threshold \\(t=2\\). Therefore, we need to sample a random polynomial \\(f(X)\\) over \\(\\mathbb{Z}_q\\) of degree \\(t-1=1\\) s.t. \\(f(0)=5\\). Let’s pick the random polynomial \\(f(X) = 10X + 5\\). We next evaluate it to create the shares. Note that since we distribute an element in \\(\\mathbb{Z}_q\\). \\[ \\begin{aligned} f(1) &amp;= 10\\cdot 1 + 5 = 15\\\\ f(2) &amp;= 10\\cdot 2 + 5 = 25 = 3\\\\ f(3) &amp;= 10\\cdot 3 + 5 = 35 = 12\\\\ f(4) &amp;= 10\\cdot 4 + 5 = 45 = 22 = -1\\\\ f(5) &amp;= 10\\cdot 5 + 5 = 55 = 32 = 9\\\\ \\end{aligned} \\] We distribute the shares \\[ (1, 15),\\quad (2, 3),\\quad (3, 12),\\quad (4, -1),\\quad (5, -9) \\] We do the normal ElGamal encryption (note that secret sharing has only to do with the secret key). Let’s use \\(r=3\\). We have: \\[ (c_1, c_2) = (g^r, m\\cdot h^r) = (2^3, 6\\cdot 32^3)= (8, 7) \\in\\mathbb{G}^2 \\] We now need to decrypt \\((c_1, c_2)=(8, 7)\\). Recall that decryption is done by computing \\(c_2/c_1^{x}\\). The part that depends on the secret key is \\(c_1^{x}\\). Noting that \\(x=f(0)\\) and that \\(f(X)= f(3)\\cdot \\ell_3(0)+f(5)\\cdot\\ell_5(0)\\) we can rewrite: \\[ \\begin{aligned} c_1^x &amp;= c_1^{f(X)} = c_1^{f(3)\\cdot \\ell_3(0)+f(5)\\cdot\\ell_5(0)}=c_1^{f(3)\\cdot \\ell_3(0)}c_1^{f(5)\\cdot \\ell_5(0)} \\end{aligned} \\] Let’s first compute the public values \\(\\ell(0), \\ell(0)\\). Note that we do the operations \\(\\mod q\\). We have \\[ \\begin{aligned} \\ell_{3}(0) &amp;= \\prod_{j\\in\\{3,5\\}, j\\neq 3} \\frac{0-j}{i-j} = \\frac{-5}{3-5} = \\frac{-5}{-2} = 5\\cdot 12 = 14\\\\ \\ell_{5}(0) &amp;= \\prod_{j\\in\\{3,5\\}, j\\neq 5} \\frac{0-j}{i-j} = \\frac{-3}{2} = (-3)\\cdot 12 = 10 \\end{aligned} \\] Now the first party partially decrypts by computing \\[ \\alpha_1 = c_1^{\\ell_{3}(0)f(3)}=8^{14\\cdot 12} = 8^7 = 12 \\] and the second party \\[ \\alpha_2 = c_1^{\\ell_{5}(0)f(5)}=8^{10\\cdot 9} = 8^{21} = 36 = -11 \\] Note in the above operations that the exponents are computed \\(\\mod q\\) and the exponentiations \\(\\mod p\\). We can now compute the value \\(\\alpha = \\alpha_1\\cdot \\alpha_2 = 12\\cdot (-11) = 9\\pmod p\\). Finally, the final decryption is \\[ m = c_2\\cdot \\alpha^{-1} = 7\\cdot (9^{-1}) = 7\\cdot 21 = 6 \\] Remark. It can get confusing what type of computation we are doing where. Always try to notice in which “world” each element lives. In ElGamal/DDH, the secret key and the randomness (and the message in the case of lifted ElGamal) live in \\(\\mathbb{Z}_q\\) and the rest in \\(\\mathbb{G}\\) which is a subgroup of \\(\\mathbb{Z}_p\\). Therefore, when doing operations “in the exponent” we work \\(\\mod q\\) and when doing operations “in the base” we do \\(\\mod q\\). Be extra careful when inverting elements. Note that in the previous exercise we both invert \\(\\mod q\\) when computing the Lagrange coefficients as well as \\(\\mod p\\) when decrypting the message. Exercise 11.3 (*) We have seen how we can share a secret amongst \\(n\\) parties s.t. at least \\(t\\) of them are needed to recover the secret. This is called the threshold access structure: any subset of the parties can recover the secret as long as their number is at least \\(t\\). We can, however, consider more complex access structures. Build secret sharing schemes for the following cases and argue that they are correct: Alice wants to share a secret in \\(\\mathbb{F}\\) between her \\(n\\) friends such that (1) no subset of less than \\(t\\) parties, and (2) no subset that does not contain Bob, can recover the secret. Alice wants to share a secret between her \\(n_1\\) friends and \\(n_2\\) colleagues (the sets are disjoint) such that at least \\(t_1\\) friends and \\(t_2\\) colleagues are required to recover the secret. Solution. Let \\(m\\) be the secret. First Alice chooses \\(m_1\\gets \\mathbb{F}\\) at random and computes \\(m_2=m-m_1\\). She then does the following: she gives \\(m_1\\) to Bob, and she secret-shares \\(m_2\\) using \\((t-1, n-1)\\)-Shamir secret sharing to the rest. Now, assume that any set of parties without Bob tries to recover the secret. No matter what they do, they have no information about \\(m_1\\) (note that nothing relevant to \\(m_1\\) is shared to anyone apart from Bob). Therefore they also have no information about \\(m\\). Also, assume that any subset with less than \\(t\\) parties tries to recover the secret. We have two cases: - if the subset contains Bob, there are less than \\(t-1\\) parties that hold shares for \\(m_2\\). Therefore, by security of Shamir secret sharing, they can get no information about \\(m_2\\) (note that Bob holds no information regarding \\(m_2\\)) and hence for the secret \\(m\\). - If the subset does not contain Bob, we have already argued that they cannot hope to recover the secret regardless of how many they are. We work similarly in this case. Let \\(m\\) be the secret. First Alice chooses \\(m_1\\gets \\mathbb{F}\\) at random and computes \\(m_2=m-m_1\\). She then does the following: she secret-shares \\(m_1\\) to her friends using \\((t_1, n_1)\\)-Shamir secret sharing. she secret-shares \\(m_2\\) to her colleagues using \\((t_2, n_2)\\)-Shamir secret sharing. Now assume any subset that contains less than \\(t_1\\) friends tries to recover the secret. Since the colleuages have no information about \\(m_1\\), the less than \\(t_1\\) friends will fail to gain any information about \\(m_1\\) and therefore for \\(m\\). An identical argument works for any subset that contains less than \\(t_2\\) friends. https://en.wikipedia.org/wiki/Two-man_rule↩︎ Shamir, A. (1979). How to share a secret. Communications of the ACM, 22(11), 612-613.↩︎ "],["zero-knowledge-proofs.html", "12 Zero knowledge Proofs 12.1 The Schnorr Protocol Solved exercises", " 12 Zero knowledge Proofs Consider the following problem: Alice claims she knows the ElGamal public key \\(x\\) corresponding to \\(h = g^x\\) and Bob does not believe her. How can Alice convince Bob? There is a simple solution: Alice can just send \\(x\\). Then Bob can verify that \\(h = g^x\\) and be convinced beyond doubt. But Alice does not want to compromise her secret key. She needs it to remain private. Another solution would be to ask Bob to encrypt any message of his choice and send the ciphertext. Alice can then decrypt the ciphertext -if she indeed knows the secret key- and send the plaintext to Bob. If the message is the one Bob encrypted he believes her. While the latter is clearly a better solution, it is not ideal: what if Bob intercepted the ciphertext, whose contents are supposed to be secret? Recall that ElGamal is not IND-CCA secure! We need an even more sophisticated solution. Before continuing, let us observe something about the requirements we have in regards to this problem. Contrary to the cryptographic primitives we have seen so far (e.g. encryption schemes, hash functions, digital signatures) we now need a protocol that protects Alice and Bob! In particular, we need to protect Bob from a possibly lying Alice, falsely claiming to know the secret \\(x\\), protect Alice from a curious Bob, who might try to learn information about the secret \\(x\\) by making Alice “convince” him that she knows the secret. Our first task is to precisely capture these requirements in a definition. We will keep the definition informal, since defining accurately notions such as “Alice knows \\(x\\)” or “Bob does not learn \\(y\\)” can be tricky and technical to formally capture and are out of the scope of this course. For our needs, it is enough to understand such notions with their intuitive meaning. First, let’s define the relation \\(\\mathcal{R}_\\text{dlog}\\) capturing the statement “\\(x\\) is the discrete log of \\(h\\) w.r.t. \\(g\\): \\[ \\mathcal{R}_\\text{dlog} = \\{((G, g, h), x) \\mid \\text{ $G$ is a group with generator $g$ and } h = g^x \\} \\] Let’s next give our informal definition of a . We define it for an arbitrary relation \\(\\mathcal{R}\\) and not only for \\(\\mathcal{R}_\\text{dlog}\\). We refer to the values \\((G, g, h)\\) which are known by both parties as the statement and to \\(x\\) which is known only to Alice as the witness. Definition 12.1 Let \\(\\mathcal{R}\\) be an NP relation containing statement/witness pairs \\((x, w)\\). A zero knowledge proof protocol is a protocol run between a Prover \\(\\mathcal{P}\\) (Alice) and a Verifier \\(\\mathcal{V}\\) (Bob) where \\(\\mathcal{P}\\) has input \\(x, w\\), \\(\\mathcal{V}\\) has input \\(w\\) and both have input a security parameter \\(\\lambda\\). \\(\\mathcal{P}\\) and \\(\\mathcal{V}\\) interact in a prescribed way for a number of rounds and at the end \\(\\mathcal{V}\\) outputs “accept” or “reject” indicating whether it is convinced that \\(\\mathcal{P}\\) knows \\(w\\) s.t. \\((x,w)\\in\\mathcal{R}\\). The protocol satisfies the following properties: Completeness: For every \\((x,w)\\in \\mathcal{R}\\), when \\(\\mathcal{P}\\) and \\(\\mathcal{V}\\) interact with inputs \\((x,w)\\) and \\(x\\) respectively, then \\(\\mathcal{V}\\) always outputs “accept”. Soundness: For every polynomial time \\(\\mathcal{P}^*\\) that does not know a witness \\(w\\) s.t. \\((x,w)\\in \\mathcal{R}\\) the probability that \\(\\mathcal{P}^*\\) and \\(\\mathcal{V}\\) interact and \\(\\mathcal{V}\\) outputs “accept” is negligible in \\(\\lambda\\). Zero Knowledge: After the interaction, \\(\\mathcal{V}\\) learns nothing except the validity of the statement, that is, that \\(\\mathcal{P}\\) knows a witness \\(w\\) such that \\((x,w)\\in \\mathcal{R}\\). Let’s also give some more intuition about the latter property. How do we even argue about it? The solution is quite elegant! We require that \\(\\mathcal{V}\\) can simulate interactions that look exactly the same with an interaction with the real \\(\\mathcal{P}\\). If \\(\\mathcal{V}\\) can do this on its own, then what can it learn from such an interaction? Nothing that it could not deduce on its own! This might sound a bit tricky at this point but it will become clear later with an example. 12.1 The Schnorr Protocol Let’s now introduce our first zero knowledge proof for \\(\\mathcal{R}_\\text{dlog}\\) called the Schnorr protocol! We are on the setting of Safe-Prime groups (the protocol generalizes to any group where the discrete logarithm problem is hard), so we describe the group as \\(\\mathbb{G} = (p, q, g)\\) where \\(g\\) is a generator of a subgroup of \\(\\mathbb{Z}_p^*\\) of order \\(q\\). We have our prover and verifier with common input \\(h\\) and \\(\\mathcal{P}\\) claims that it knows some \\(x\\) such that \\(h=g^x\\) in \\(\\mathbb{G}\\). We set the security parameter as \\(\\lambda = \\log q\\), i.e. the number of bits to represent an element of the group. Let’s next describe the prescribed interaction between \\(\\mathcal{P}, \\mathcal{V}\\). \\(\\mathcal{P}((p, q, g, h), x)\\) and \\(\\mathcal{V}(p, q, g, h)\\) interact as follows: \\(\\mathcal{P}\\) samples a random \\(s\\in\\mathbb{Z}_q\\) and computes \\(r=g^s\\). It sends \\(r\\) to \\(\\mathcal{V}\\). \\(\\mathcal{V}\\) samples a random \\(c\\in\\mathbb{Z}_q\\) and sends it to \\(\\mathcal{P}\\). \\(\\mathcal{P}\\) computes \\(z = s + cx\\) and sends \\(z\\) to \\(\\mathcal{V}\\). Finally, \\(\\mathcal{V}\\) is ready to make its decision. It checks the equation \\(g^z=r\\cdot h^c\\) and it outputs iff it is satisfied. Let’s now try to see if the three properties hold. Let’s start with completeness. For this, we assume that \\(x\\) is indeed the discrete logarithm of \\(h\\). Let’s see what the final equation gives: \\[ g^z = g^{s+cx} = g^{s}g^{cx}= g^{s}(g^{x})^x = r\\cdot h^c \\] so indeed the verifier always accepts in this case! For soundness we must make a more complicated argument. We will show that if the prover can answer the third message in a way that makes the verifier accept, it must (except with negligible probability) know the discrete logarithm \\(x\\). Concretely, assume that \\(\\mathcal{P}\\) sends any message \\(r=g^s\\) in the first round. Now assume it can make \\(\\mathcal{V}\\) accept for two different challenges \\(c_1\\neq c_2\\), that is, it can produce for both values \\(z_1, z_2\\) such that \\[ g^{z_1} = r\\cdot h^{c_1}, \\qquad g^{z_2} = r\\cdot h^{c_2} \\] Let’ divide the two equations. We get: \\[ g^{z_1-z_2} = h^{c_1-c_2} \\Leftrightarrow g^{(z_1-z_2)(c_1-c_2)^{-1}} = h \\] Recall that when we do operations on the exponents, we work \\(\\mod q\\) so in the above \\((c_1-c_2)^{-1}\\) is the multiplicative inverse of \\((c_1-c_2) \\mod q\\) (why does it exist?). But then this means that the discrete logarithm of \\(h\\) w.r.t. \\(g\\) is \\(x = (z_1-z_2)(c_1-c_2)^{-1}\\). Note that the prover knows the values \\(z_1, z_2, c_1, c_2\\) so it can indeed compute the witness \\(x\\). This can mean only two things: (1) either the prover can answer only for one challenge \\(c\\) in the protocol execution or (2) it actually knows the witness. And if the prover only knows how to convince the verifier for a single value \\(c\\), this means that it succeeds only if the verifier picks this specific value, which happens with probability \\(1/q\\) which is negligible! Finally, let’s discuss zero knowledge. Let’s first look into the transcript of the protocol, that is the “discussion” between \\(\\mathcal{P}\\) and \\(\\mathcal{V}\\). This consists of \\(3\\) elements: \\(\\mathsf{tr} = (r\\in\\mathbb{G}, c\\in\\mathbb{Z}_q, z\\in\\mathbb{Z}_q)\\). We are interested in their distribution: they consist of a uniformly distributed triplet that satisfies the equation \\(g^z = r\\cdot h^c\\). Can we pick such a triplet on our own without interacting with \\(\\mathcal{P}\\) and without knowing the secret? The answer is positive! We can do this as follows: pick any random \\(z, c\\in\\mathbb{Z}_q\\) and set \\(r = g^z\\cdot h^{-c}\\)! Note that this looks exactly like the transcript. This is enough to say that \\(\\mathcal{V}\\) does not learn anything from the interaction: anything it could hope to learn by \\(\\mathcal{P}\\) it can save the trouble of actually “talking” with \\(\\mathcal{P}\\) and just produce a valid triplet on its own. Since it follows the same distribution, it would learn the same things from it! There is a flaw in the above argument, however. We assumed that \\(\\mathsf{tr} = (r\\in\\mathbb{G}, c\\in\\mathbb{Z}_q, z\\in\\mathbb{Z}_q)\\) consists of a uniformly distributed triplet that satisfies the equation \\(g^z = r\\cdot h^c\\). But this is not necessarily true! In particular, \\(\\mathcal{V}\\) might try to cheat by not following the protocol! The value \\(c\\) it sends must be a uniformly distributed element of \\(\\mathbb{Z}_q\\) but what if Bob picks one that is not? It turns out that we cannot prove zero knowledge in this case. We only achieve a weaker notion: honest verifier zero knowledge which states that \\(\\mathcal{V}\\) learns nothing about the validity of the statement if it behaves honestly (in our case sending a uniformly challenge \\(c\\)). It turns out that this property is enough for practical application since there are ways to turn protocols with this property to protocols that achieve full zero knowledge. 12.1.1 Simulation vs Soundness There seems that something paradoxical is happening in the Schnorr protocol. We showed that we can simulate the interaction between \\(\\mathcal{P}\\) and \\(\\mathcal{V}\\) and argued that this is the reason \\(\\mathcal{V}\\) learns nothing about the witness. Why should \\(\\mathcal{V}\\) interact with \\(\\mathcal{P}\\) in the first place? Can’t he just “interact with itself”? The answer is no. In particular, \\(\\mathcal{V}\\) learns something by the interaction: that \\(\\mathcal{P}\\) indeed knows the witness! But how does it happen? What is the difference between a real and a simulated interaction? The answer lies in the order of the messages. In an actual interaction, \\(\\mathcal{P}\\) must send \\(r\\in\\mathbb{G}\\) before it sees \\(c\\)! If the latter is picked after \\(r\\) only someone that knows the secret can compute a valid \\(z\\) so indeed \\(\\mathcal{V}\\) is convinced about \\(\\mathcal{P}\\)’s claim! In other words, consider that Bob interacts with Alice and is convinced that she knows \\(x\\) s.t. \\(h=g^x\\). It then takes the transcript \\((r, c, z)\\) and shows it to Charlie. Bob claims that Alice knows \\(x\\). Should Charlie believe him? The answer is no! Charlie cannot know if this was a real interaction (where \\(c\\) was picked after \\(r\\)) or if it was one that Bob simulated. The only way Charlie can be convinced is if it interacts with Alice itself (or somehow be convinced that \\(c\\) was sampled after \\(r\\)). Solved exercises Exercise 12.1 Consider the safe prime group \\((p, q, g)\\) and the DH tuple \\((g, A = g^a, B = g^b, C = g^{ab})\\). Rewrite the DH tuple in terms of the dlog problem, i.e. by expressing appropriate discrete logarithm relations between the elements \\((g, A, B, C)\\). Write an (honest verifier) zero knowledge proof protocol for showing that the tuple \\((g, A, B, C)\\) is a DH tuple. Hint: run two Shcnorr protocol executions in parallel. Solution. We can express the tuple in terms of discrete logarithm equality, namely \\((g, A, B, C)\\) is a DH tuple if and only if the discrete logarithm of \\(B\\) w.r.t. \\(g\\) and the discrete logarithm of \\(C\\) w.r.t. \\(A\\) are equal. Formally, we define the DH relation as: \\[ \\mathcal{R}_\\text{dh} = \\{((G, g, A, B, C), x) \\mid \\text{ $G$ is a group with generator $g$ and } B = g^x \\text{ and } C = A^x\\} \\] We need a zero knowledge proof for \\(\\mathcal{R}_\\text{dh}\\). Our strategy is to run two parallel execution of the Schnorr protocol: one for \\(B = g^x\\) and one for \\(C = A^x\\). \\(\\mathcal{P}((p, q, g, A, B, C), x)\\) and \\(\\mathcal{V}(p, q, g, A, B, C)\\) interact as follows: \\(\\mathcal{P}\\) samples a random \\(s\\in\\mathbb{Z}_q\\) and computes \\(r_g=g^s\\) and \\(r_A = A^s\\). It sends \\(r_g, r_A\\) to \\(\\mathcal{V}\\). \\(\\mathcal{V}\\) samples a random \\(c\\in\\mathbb{Z}_q\\) and sends it to \\(\\mathcal{P}\\). \\(\\mathcal{P}\\) computes \\(z = s + cx\\) and sends \\(z\\) to \\(\\mathcal{V}\\). Finally, \\(\\mathcal{V}\\) is ready to make its decision. It checks the equations \\[ g^z=r_g\\cdot B^c, \\qquad A^z=r_A\\cdot C^c \\] and it outputs “accept” iff it is satisfied. Let’s see that our properties are satisfied. For completeness we have: \\[ g^z= g^{s + cx} = g^s\\cdot (g^x)^c = r_g\\cdot B^c \\] \\[ A^z= A^{s + cx} = A^s\\cdot (A^x)^c = r_A\\cdot C^c \\] so the honest prover always succeeds in convincing the verifier! Let’s move to soundness. We work exactly as in Schnorr protocol. In particular, let’s assume that for the same values \\(r_g, r_A\\), \\(\\mathcal{P}^*\\) can convince the verifier for two different challenges \\(c_1, c_2\\). We now have: \\[ \\begin{aligned} g^{z_1} &amp;= r_g\\cdot B^{c_1}, \\qquad g^{z_2} = r_g\\cdot B^{c_2}\\\\ A^{z_1} &amp;= r_A\\cdot C^{c_1}, \\qquad A^{z_2} = r_A\\cdot C^{c_2}\\\\ \\end{aligned} \\] Let’ divide the two equations. We get: \\[ \\begin{aligned} g^{z_1-z_2} &amp;= B^{c_1-c_2} \\Leftrightarrow g^{(z_1-z_2)(c_1-c_2)^{-1}} = B\\\\ A^{z_1-z_2} &amp;= C^{c_1-c_2} \\Leftrightarrow A^{(z_1-z_2)(c_1-c_2)^{-1}} = C \\end{aligned} \\] Therefore, setting \\(x = (z_1-z_2)(c_1-c_2)^{-1}\\) we get that \\(B = g^x\\) and \\(C=A^x\\) and therefore \\((g, A, B, C)\\) is a DH tuple. A malicious prover can then only answer for a single challenge \\(c\\) each time if it does not know the witness which means it can convince \\(\\mathcal{V}\\) only with negligible probaiblity. Finally, for (honest verifier) zero knowledge we also act as in the Schnorr protocol: sample \\(c,z\\gets \\mathbb{Z}_q\\) at random and compute \\[ r_g = g^{z}\\cdot B^{-c}, \\qquad r_A = A^{z}\\cdot C^{-c} \\] and output the tuple \\((r_g, r_A, c, z)\\) as the transcript. The above protocol is known as the Chaum-Pedersen protocol. "],["ring-theory.html", "A Ring theory", " A Ring theory A ring is a slightly more complex algebraic structure, since it is equipped with two operations, the first of them with the exact same properties of a commutative group operation, whereas the second does not require the existence of inverse. Definition A.1 A ring52 is a triple \\((\\mathbb{K},\\circ,\\ast)\\), where \\(\\mathbb{K}\\) is a set and \\(\\circ\\) and \\(\\ast\\) are operations on \\(\\mathbb{K}\\), that is, functions \\[\\begin{aligned} \\circ : \\phantom{a} &amp; \\mathbb{K}\\times \\mathbb{K}&amp; \\rightarrow &amp; \\phantom{a}\\mathbb{K}&amp; \\qquad\\qquad &amp; \\ast :&amp; \\mathbb{K}\\times\\mathbb{K}&amp; \\rightarrow &amp; \\mathbb{K}\\\\ &amp; (x,y) &amp; \\mapsto &amp; \\phantom{a} x\\circ y, &amp; &amp; &amp; (x,y) &amp; \\mapsto &amp; x\\ast y, \\end{aligned}\\] which additionally satisfy the following properties: \\((\\mathbb{K},\\circ)\\) is a commutative group. \\((\\mathbb{K},\\ast)\\) satisfies the following properties: Associative law: \\((x\\ast y)\\ast z = x\\ast(y\\ast z)\\) for all \\(x,y,z\\in \\mathbb{K}\\). Existence of identity: there exists \\(e\\in \\mathbb{K}\\) such that \\(e\\ast x=x\\ast e=x\\) for all \\(x\\in \\mathbb{K}\\). Such element \\(e\\) is called the identity element of \\(\\ast\\). Commutativity: \\(x\\ast y=y \\ast x\\) for all \\(x,y\\in \\mathbb{K}\\). Distributive law: \\(x\\ast(y\\circ z)=(x\\ast y)\\circ(x\\ast z)\\) for all \\(x,y,z\\in\\mathbb{K}.\\)* Since we have two operations at the same time, we will adopt the additive and multiplicative notation from groups to represent each of them, respectively. That is, we will think of the first operation of a ring as a form of addition and the second as a form of multiplication. On input \\(x,y\\), we write the result of the first operation by \\(x+y\\), and the result of the second by \\(xy\\). The identity elements of each operation are denoted by \\(0\\) and \\(1\\), respectively. The inverse of \\(x\\) with respect to the first operation is denoted by \\(-x\\). The inverse of \\(x\\) with respect to the second operation, if exists, is denoted by \\(x^{-1}\\). The following table summarizes the notation: Operation Operation on input \\(x,y\\) Identity element Inverse of \\(x\\) \\(+\\) \\(x+y\\) \\(0\\) \\(-x\\) \\(\\cdot\\) \\(xy\\) \\(1\\) \\(x^{-1}\\) We consider some examples: \\((\\mathbb{Z},+,\\cdot)\\), with usual integer addition and multiplication, is a ring, since \\((\\mathbb{Z},+)\\) is a group with identity \\(0\\), \\((\\mathbb{Z},\\cdot)\\) verifies associativity, commutativity and existence of identity (\\(1\\)), and it is easy to verify distributivity. Note, however, that \\(\\mathbb{Z}\\) does not contain multiplicative inverses, since for example there is no \\(x\\in\\mathbb{Z}\\) such that \\(2x=1\\). For similar reasons, \\((\\mathbb{Z}_n,+,\\cdot)\\), with modular addition and multiplication, is a ring, for \\(n\\in\\mathbb{N}\\). \\((\\mathbb{Q}[X],+,\\cdot)\\), where \\(\\mathbb{Q}[X]\\) is the set of polynomials with rational coefficients in variable \\(X\\), \\(+\\) is point-wise addition, and \\(\\cdot\\) is point-wise multiplication, is also a ring. The identity elements are the constant polynomials \\(p(X)=0\\) and \\(q(X)=1\\), respectively. Consider the ring \\(\\mathbb{Z}_9\\), with addition and multiplication modulo \\(9\\). The following table gives the multiplicative inverses of each element: \\(x\\) \\(0\\) \\(1\\) \\(2\\) \\(3\\) \\(4\\) \\(5\\) \\(6\\) \\(7\\) \\(8\\) \\(x^{-1}\\) \\(-\\) \\(1\\) \\(5\\) \\(-\\) \\(7\\) \\(2\\) \\(-\\) \\(4\\) \\(8\\) Note that some elements are not invertible, in this case \\(0\\), \\(3\\) and \\(6\\). Those elements in a ring that happen to have a multiplicative inverse receive a special name. Definition A.2 Let \\(\\mathbb{K}\\) be a ring. An element \\(x\\in\\mathbb{K}\\) that has a multiplicative inverse is called a unit. We denote the set of units of \\(\\mathbb{K}\\) by \\(\\mathbb{K}^*\\). Note that the only thing that \\((\\mathbb{K},\\cdot)\\) was missing to be a group is existence of inverses. By restricting ourselves to the subset \\(\\mathbb{K}^*\\) of invertible elements, we have that \\((\\mathbb{K}^*,\\cdot)\\) is a group. Note that, in the previous lesson, we defined \\(\\mathbb{Z}_n^*\\) to be the set of invertible elements of \\(\\mathbb{Z}_n\\), so in this new formulation, we can state that \\(\\mathbb{Z}_n^*\\) is actually the set of units of \\(\\mathbb{Z}_n\\). Moreover, we now know that \\(\\mathbb{Z}_n^*\\) is a group with multiplication modulo \\(n\\). In light of the discussion above, we start this section by asking ourselves the following question. Is it possible to have a ring \\(\\mathbb{K}\\) in which all the elements are invertible? This will never be the case, unless the ring only contains one element, since the additive identity \\(0\\) cannot have a multiplicative inverse otherwise. To see this, assume that there exists \\(x\\in\\mathbb{K}\\) such that \\(0\\cdot x = 1\\). First, \\[0x=(1+(-1))x=x+(-x)=0,\\] using the distributive law and the properties of the multiplicative identity \\(1\\). Then we have that \\(1=0\\). But then, this means that \\[x=1x=0x=0,\\] using the properties of the multiplicative and additive identities. Thus, we conclude that every element is the same. And rings with just one element are not very interesting. So, except for the trivial case, we know that \\(0\\) cannot have an inverse. What about the rest of the elements? In this case, the answer is affirmative: there exist rings in which every non-zero element is invertible, and we call those fields. Definition A.3 A field is a ring in which every non-zero element has a multiplicative inverse in the ring. In an abstract algebra context, the definition of a ring is more general, and what we are defining is known as a commutative ring with unity. Nevertheless, we stick to this definition for simplicity, since it is the only type of ring relevant to us.↩︎ "],["primality-testing.html", "B Primality testing", " B Primality testing Many modern cryptographic schemes require some very large prime numbers, so we are interested in generating these numbers efficiently. The basic strategy is very simple: pick a random number of the desired size, and check whether it is prime. If it is not, pick another. A consequence of the prime number theorem (Proposition 6.3) is that, on average, we need about \\(\\lambda\\) tries to find a prime of bitlength \\(\\lambda\\), so that’s not too bad. But what about recognizing whether a number is prime or not? This part is solved with a primality test. In this section, we look at some of them. The naive way to check primality is to check if the candidate integer \\(n=O(2^\\lambda)\\) is divisible by any other smaller integer. If that’s not the case, then necessarily \\(n\\) is prime. So, in general, this algorithm runs in time \\(O(n)\\), which is exponential in \\(\\lambda\\). We can do slightly better by observing that it is enough to check divisors up to \\(\\sqrt{n}\\). Proposition B.1 Let \\(n\\in\\mathbb{N}\\). If \\(n\\) is composite, then \\(n\\) has a non-trivial divisor \\(d\\leq\\sqrt{n}\\). Proof. We prove the result by contradiction. Assume that \\(n\\) is composite and has no non-trivial divisor smaller than \\(\\sqrt{n}\\). Since \\(n\\) is composite, it has at least two non-trivial divisors \\(a,b\\in\\mathbb{N}\\), such that \\(ab=n\\). But every divisor is larger than \\(\\sqrt{n}\\), therefore \\[n=ab&gt;\\sqrt{n}\\cdot\\sqrt{n}=n,\\] and thus \\(n&gt;n\\), which is a contradiction. Still, our primality test would run in time \\(O(\\sqrt{n})\\), which is still exponential in \\(\\lambda\\). We need to look for a more efficient approach. We turn our attention to Fermat’s little theorem: Proposition B.2 (Fermat's little theorem) Let \\(p\\) be a prime number. Then, for any \\(x\\in\\mathbb{Z}\\) such that \\(p\\nmid x\\), we have that \\[x^{p-1}\\equiv1\\pmod{p}.\\] This theorem tells us about a condition that all prime numbers verify, which gives us an easy condition to test and discard composite numbers. Given a candidate \\(n\\), choose some \\(x\\in\\mathbb{Z}\\) such that \\(\\gcd(n,x)=1\\), and compute \\[x^{n-1}\\bmod{n}.\\] If the result is not \\(1\\), then we conclude that \\(n\\) is not a prime. This is known as the Fermat primality test, and it is very efficient. But what happens if the result is \\(1\\)? Does this mean that \\(n\\) is a prime? Unfortunately, it is not as simple as this. For example, take \\(n=91\\) and \\(x=3\\). It is easy to verify that \\[3^{90}\\pmod{91}=1,\\] but \\(91=7\\cdot13\\), so it is not a prime. We call the numbers that produce these “false positives” pseudoprimes. Definition B.1 Let \\(n\\in\\mathbb{Z}\\) be a composite number, and let \\(x\\in\\mathbb{Z}\\) such that \\(\\gcd(x,n)=1\\). We call \\(n\\) an \\(x\\)-Fermat pseudoprime if \\[x^{n-1}\\equiv1\\pmod{n}.\\] Moreover, for any \\(x\\) there are many \\(x\\)-Fermat pseudoprimes. Okay, so everybody don’t panic. Fermat’s little theorem tell us that primes verify the condition \\[x^{p-1}\\equiv1\\pmod{p}\\] for every \\(x\\) such that \\(\\gcd(p,x)=1\\). So surely we can find some other base \\(x\\) so that \\(91\\) cannot fool the test. Indeed, \\[5^{90}\\bmod{91} = 64.\\] So that’s it, we are now sure that \\(91\\) is not a prime, because it does not pass the test for \\(x=5\\). Problem solved. Or is it? What if there are some truly evil numbers that can fool the Fermat test for all the possible bases? Definition B.2 Let \\(n\\in\\mathbb{Z}\\) be a composite number. We say that \\(n\\) is a Carmichael number if it is a \\(x\\)-Fermat pseudoprime for every \\(x\\in\\mathbb{Z}\\) such that \\(\\gcd(n,x)=1\\). Unfortunately, there are too many of these Carmichael numbers. For large values of \\(B\\in\\mathbb{N}\\), the number of Carmichael numbers up to \\(B\\) is approximately \\(B^{2/7}\\). This means that a significant amount of numbers will fool our test, even if we run it for all the possible bases. We need a better test. The solution lies on a strengthened version of Fermat’s little theorem. Proposition B.3 Let \\(p\\in\\mathbb{N}\\) be an odd prime, and let \\(p-1=2^st\\) such that \\(2\\nmid t\\). If \\(\\gcd(p,x)=1\\), then either \\[x^t\\equiv1\\pmod{p},\\] or there exists \\(i\\in\\{0,1,\\dots,s-1\\}\\) such that \\[x^{2^it}\\equiv-1\\pmod{p}.\\] Again, the idea is to test our numbers against this result, and see if they satisfy the equations. This version is called the strong Fermat primality test. Observe that, in the worst case, we need to perform \\(s\\) checks, and if \\(p\\) has bitlength \\(\\lambda\\), then \\(s=O(\\lambda)\\), and each check amounts to modular exponentiation, so overall this is efficient. As before, this new test also produces false positives. Definition B.3 Let \\(n\\in\\mathbb{Z}\\) be an odd composite integer, and let \\(n-1=2^st\\). Let \\(x\\in\\mathbb{Z}\\) such that \\(\\gcd(x,n)=1\\). We call \\(n\\) a strong \\(x\\)-Fermat pseudoprime if \\[x^{t-1}\\equiv1\\pmod{n},\\] or there exists \\(i\\in\\{0,1,\\dots,s-1\\}\\) such that \\[x^{2^it}\\equiv-1\\pmod{n}.\\] Exercise B.1 Check that \\(2047\\) is a strong \\(2\\)-Fermat pseudoprime, because it satisfies the first condition, and that \\(91\\) is a strong \\(10\\)-Fermat pseudoprime, because it satisfies the second condition. So why bother with the strong Fermat test, if it has the same flaw? The key point is that, even if we have strong pseudoprimes, the strong Fermat test does not have its version of Carmichael numbers. That is, for any composite number \\(n\\) we will always be able to find a base such that \\(n\\) does not pass the strong test, showing that it is indeed composite. Moreover, most of the bases will do! Proposition B.4 (Monier--Rabin) Let \\(n&gt;9\\) be an odd composite integer. Then, the number of bases \\(x\\) smaller than \\(n\\) such that \\(n\\) is a strong \\(x\\)-Fermat pseudoprime is at most \\(\\frac14\\varphi(n)\\). This is a very strong result. Since \\(\\varphi(n)&lt;n\\), this tells us that, by picking a base in \\(\\mathbb{Z}_n\\) at random, there is only a probability of \\(1/4\\) that the test lies. Of course, we want a smaller probability, so we repeat the test many times. By using \\(\\lambda\\) iterations for different random bases, we bring the error probability down to \\(1/4^\\lambda\\). Observe that this decreases exponentially in \\(\\lambda\\)! To give some concrete numbers, for \\(\\lambda=1024\\), we have that \\[\\frac{1}{4^\\lambda}\\approx 3.094346\\cdot 10^{-67},\\] and this looks like a failure probability that we can live with. Asymptotically, we have an algorithm that runs \\(\\lambda\\) iterations, and each of these is computing \\(O(\\lambda)\\) exponentiations, so in total we have a running cost of \\(O(\\lambda^2)\\) modular exponentiations. This test is known as the Miller–Rabin primality test, and it is in essence what is used in practice to check for primality, due to its high efficiency and almost-perfect reliability. If you are still concerned about the error probability, there are some alternatives that never produce erroneous results, while still running in polynomial time. However, they are much slower than the Miller–Rabin test, which is why they are not often used in practice.53 Agrawal, M., Kayal, N., &amp; Saxena, N. (2004). PRIMES is in P. Annals of mathematics, 781-793.↩︎ "],["polynomial-interpolation.html", "C Polynomial interpolation", " C Polynomial interpolation We start from a very well-known idea: two different points in a plane determine a line. Or equivalently: given two different points in a plane, there is exactly one line that contains both. More formally, let \\((x_0,y_0)\\) and \\((x_1,y_1)\\) be points in \\(\\mathbb{R}^2\\), such that \\(x_0\\neq x_1\\).54 Then there is a unique polynomial \\(p(X)\\) of degree at most \\(1\\) such that \\[p(x_0)=y_0,\\qquad \\text{and} \\qquad p(x_1)=y_1.\\] Although can find this polynomial by finding the slope of the line, we will show another approach that yields the same result, that we will later generalize to a higher number of points. Consider the following simpler problem. Problem 7. Given \\(x_0,x_1\\in\\mathbb{R}\\), find a polynomial \\(\\ell_0(X)\\) of degree at most \\(1\\) such that \\[\\ell_0(x_0)=1,\\qquad\\text{ and }\\ell_0(x_1)=0.\\] The simplest idea to achieve the second condition is to make \\(x_1\\) a root of the polynomial. Note that, since the degree must be at most \\(1\\), the polynomial will have at most one root, making \\(x_1\\) the only root. Let us then consider the polynomial \\[\\hat\\ell_0(X)=X-x_1.\\] Clearly this polynomial satisfies the second condition, but it fails at the first, since \\[\\hat\\ell_0(x_1)=x_0-x_1,\\] which might be different from \\(1\\). Thus, to ensure the second condition, we rescale the polynomial by dividing by the value at \\(x_0\\): \\[\\ell_0(X)=\\frac{X-x_1}{x_0-x_1}.\\] This new polynomial is indeed a solution for the problem. Similarly, we can find another polynomial \\(\\ell_1(X)\\) that is \\(0\\) at \\(x_0\\) and \\(1\\) at \\(x_1\\). Finally, the solution to our original problem will be \\[p(X)=y_0\\ell_0(X)+y_1\\ell_1(X).\\] Exercise C.1 Check that this polynomial has degree at most \\(1\\), that \\(p(x_0)=y_0\\) and \\(p(x_1)=y_1\\). It is easy to check that this polynomial verifies the conditions above. We say that this line is an interpolation of the points. This idea can be generalized to polynomial of any degree. That is, there is a unique parabola (polynomial of degree \\(2\\)) that contains three given points in the plane. There is a degree-\\(3\\) polynomial that contains four given points, and so on. Moreover, this also works over a finite field \\(\\mathbb{F}_p\\) for any prime \\(p\\), when the operations are considered modulo \\(p\\) and division is inversion modulo \\(p\\). Proposition C.1 Let \\(n\\in\\mathbb{N}\\), and let \\(p\\in\\mathbb{Z}\\) be a prime number. Consider \\(n+1\\) pairs \\((x_i,y_i)\\), for \\(i=0,\\dots,n\\), of points in \\(\\mathbb{F}_p\\), such that the \\(x_i\\) are pairwise different. Then, there is a unique polynomial \\(p(X)\\) of degree at most \\(n\\) such that \\[p(x_i)=y_i,\\qquad\\text{ for all }i=0,\\dots,n.\\] The polynomial \\(p(X)\\) is called the Lagrange interpolation polynomial of degree \\(n\\) relative to the points \\((x_i,y_i)\\) for \\(i=0,\\dots,n\\). The strategy to build this polynomial explicitly is to generalize what we have done before for two points. That is, we first want to solve the following problem. Problem 8. Given \\(x_0,\\dots,x_n\\in\\mathbb{F}_p\\), find polynomials \\(\\ell_i(X)\\), for \\(i=0,\\dots,n\\), of degree at most \\(n\\) such that \\[\\ell_i(x_i)=1,\\qquad\\text{ and }\\ell_i(x_j)=0, \\qquad \\text{ for all }j\\neq i.\\] That is, we want polynomials such that each of them is \\(1\\) at at one of the points and \\(0\\) at the rest of the points. For the polynomial \\(\\ell_i(X)\\), we ensure that all \\(x_j\\neq x_i\\) are roots of the polynomial. We consider: \\[\\hat\\ell_i(X)=\\prod_{j\\neq i}(X-x_j).\\] This polynomial has degree \\(n\\), since it has \\(n\\) factors, each of degree \\(1\\). Moreover, it clearly satisfies that \\(\\hat\\ell_i(x_j)=0\\) if \\(j\\neq i\\). However, we still run into the problem that it produces the wrong value at \\(x_i\\). Indeed, \\[\\hat\\ell_i(x_i)=\\prod_{j\\neq i}(x_i-x_j).\\] We solve the issue by dividing the polynomial by this value, and thus we consider the polynomial \\[\\ell_i(X)=\\frac{\\prod_{j\\neq i}(X-x_j)}{\\prod_{j\\neq i}(x_i-x_j)}=\\prod_{j\\neq i}\\frac{(X-x_j)}{x_i-x_j}.\\] The polynomials \\(\\ell_i(X)\\) are known as the Lagrange basis polynomials, as we can now use them as a basis for building any interpolation polynomial relative to the points \\(x_0,\\dots,x_n\\). Indeed, we consider the linear combination \\[p(X)=\\sum_{i=0}^n y_i\\ell_i(X).\\] It is easy to check that this polynomial satisfies that \\[p(x_i)=y_i\\] for all \\(i=0,\\dots,n\\). Moreover, since the degree of the \\(\\ell_i(X)\\) is \\(n\\), and addition of the polynomials or multiplication by constant do not increase the degree (although they might reduce it), the polynomial \\(p(X)\\) will have degree at most \\(m\\). Therefore, this is the Lagrange interpolation polynomial that we were looking for. Exercise C.2 Compute the Lagrange interpolation polynomial corresponding to the points \\((0,1),(1,-1),(2,2),(3,8)\\). We require the \\(x\\)-coordinates to be different because we want to formulate this in terms of functions, and a function cannot have two outputs for the same input.↩︎ "],["refreshers.html", "D Refreshers D.1 Set notation D.2 Probability theory D.3 Asymptotic notation D.4 Polynomial division", " D Refreshers D.1 Set notation A set is a well-defined collection of objects. Such objects are said to be elements of the set, or that they belong to the set. For example, the set of the vowels is \\[V=\\{\\text{a},\\text{e},\\text{i},\\text{o},\\text{u}\\}.\\] In the above line we are giving a name to the set, \\(V\\), and we are specifying the list of its elements: a, e, i, o and u. When describing a set explicitly, we write the list of its elements between braces \\(\\{\\hspace{0.1cm} \\}\\). Two sets are equal if they have exactly the same elements. An element cannot belong ‘twice’ to a set. Therefore, we can say that \\[V=\\{\\text{a},\\text{e},\\text{i},\\text{o},\\text{u}\\}=\\{\\text{u},\\text{o},\\text{i},\\text{e},\\text{a}\\}=\\{\\text{a},\\text{a},\\text{e},\\text{e},\\text{e},\\text{i},\\text{o},\\text{u}\\}.\\] The symbol \\(\\in\\) indicates membership of an element in a set. For example, we can write a \\(\\in V\\), because a belongs to the set \\(V\\). On the other hand, we have that b \\(\\not\\in V\\), since b is not any of the elements of \\(V\\). There are some number sets that show up very frequently, so we give them special names. \\(\\mathbb{N}\\): set of natural numbers. \\(\\mathbb{Z}\\): set of integer numbers. \\(\\mathbb{Q}\\): set of rational numbers. \\(\\mathbb{R}\\): set of real numbers. \\(\\mathbb{C}\\): set of complex numbers. Observe that, unlike in the prior examples, all these sets are infinite, that is, they have an infinite number of elements. Obviously we cannot describe an infinite set explicitly, as we have done with the set of vowels. What we can do is refer to other sets that we have already defined and define restrictions on them. For example, we can define the set of even natural numbers as the set of natural numbers that are multiples of \\(2\\). Formally, we write this set as \\[\\{n\\in \\mathbb{N}\\mid n=2k\\text{ for some }k\\in\\mathbb{N}\\}.\\] Here we have introduced some new notation. Instead of explicitly enumerating all the elements of a set, we give some conditions. We read the description above as ‘the set of elements of the form indicated on the left of the vertical line, such that they verify the condition on the right’. In this case, the set of natural numbers such that they are of the form \\(2k\\) for some natural value of \\(k\\). Similarly, we can define the set of all real numbers greater than \\(5\\) in the following way: \\[\\{x\\in \\mathbb{R}\\mid x&gt;5\\}.\\] We denote the size (number of elements) of a set \\(S\\) by \\(\\#S\\). We can produce new sets by considering the product of known sets: given two sets \\(S,T\\), we define the set \\(S\\times T\\) as the set whose elements are the pairs \\((s,t)\\), where \\(s\\in S\\) and \\(t\\in T\\). For example, \\[\\{0,1\\}\\times\\{0,1,2\\}=\\{(0,0),(0,1),(0,2),(1,0),(1,1),(1,2)\\}.\\] Observe that \\(\\#(X\\times Y)=\\#X\\cdot\\#Y\\). It is easy to generalize this definition to products of three or more sets. In particular, given a set \\(S\\), we define \\[S^n=\\underbrace{S\\times S\\times \\dots \\times S}_{n\\text{ times}}\\] In this course, we will often use the set \\(\\{0,1\\}\\) of possible bits, the set \\(\\{0,1\\}^\\ell\\) of possible bitstrings of length \\(\\ell\\), and the set \\(\\{0,1\\}^*\\) of bitstrings of any length. Observe that \\[\\#\\{0,1\\}=2,\\qquad\\#\\{0,1\\}^\\ell=2^\\ell,\\qquad \\#\\{0,1\\}^*=\\infty.\\] Exercise D.1 Write these sets using implicit notation: The set of complex numbers with real part equal to \\(1\\). The set of pairs, where the first component of the pair is a rational number and the second component is an odd natural number. The set of bitstrings of length \\(10\\) with exactly \\(5\\) zeros. D.2 Probability theory We will deal with probability distributions over finite sets. In particular, recall that the uniform distribution over a set \\(S\\) is the probability distribution that assigns the same probability \\(1/\\#S\\) to each element of \\(S\\). We denote sampling an element \\(x\\) from the uniform distribution over \\(S\\) by \\[x\\gets S.\\] For example, the notation \\[\\mathbf b\\gets \\{0,1\\}^{128}\\] means that \\(\\mathbf b\\) is a uniformly random bitstring of length \\(128\\). Given an event \\(A\\), we denote the probability of \\(A\\) by \\(\\Pr[A]\\). Given two events \\(A,B\\), we denote the probability of \\(A\\) conditioned on \\(B\\) by \\(\\Pr[A|B]\\), and if \\(\\Pr[B]\\neq0\\) we have that \\[\\Pr[A|B]=\\frac{\\Pr[A\\cap B]}{\\Pr[B]},\\] where \\(\\Pr[A\\cap B]\\) means the probability of both \\(A\\) and \\(B\\) happening. Recall that, if \\(A\\) and \\(B\\) are independent events, then \\[\\Pr[A\\cap B]=\\Pr[A]\\cdot\\Pr[B].\\] Exercise D.2 Compute the probability of a random bitstring of length \\(4\\) being the string \\(1110\\). D.3 Asymptotic notation Asymptotic notation allows us to easily express the asymptotic behaviour of functions, that is, how the function changes for arbitrarily large inputs, with respect to some other function. For example, take the functions defined by \\[f(x)=x,\\qquad\\qquad g(x)=x^2.\\] Both tend to infinity as \\(x\\) tends to infinity, but the second does it “faster”. More precisely, let \\[f,g:\\mathbb{N}\\rightarrow\\mathbb{N}\\] be two functions. We write \\[f(x)=O(g(x))\\] when there is some \\(N,M\\in\\mathbb{N}\\) such that, for all \\(x&gt;N\\), we have \\[f(x)\\leq M\\cdot g(x).\\] We read this as “\\(f\\) is big-O of \\(g\\)”. Then, back to the initial example, we can say write \\[x=O(x^2).\\] Note that, because the big-O notation “absorbs” constants into \\(M\\), we can write \\[2x=O(x),\\] even though \\(2x\\geq x\\) for \\(x\\in\\mathbb{N}\\). Big-O notation is useful for representing bounds on the growth speed of a function. By saying, for example, that a function \\(f\\) satisfies \\[f(x)=O(x^3),\\] we are saying that, at worst, the function \\(f\\) grows as fast as a cubic polynomial. Therefore, in particular, it will not grow as fast as a polynomial of degree \\(4\\), or an exponential function like \\(2^x\\). We recall that logarithmic functions grow slower than polynomials of any degree, and polynomials of any degree grow slower than exponential functions. Given two polynomials of different degrees, the one with the higher degree grows faster. Exercise D.3 Decide whether each of these statements is true or false. \\(10^{10}x^3=O(x^4)\\). \\(10^x=O(x^4)\\). \\(\\log(x)=O(x\\log x)\\). \\(4^x=O(2^x)\\). D.4 Polynomial division Consider two polynomials \\[\\begin{split} &amp; A(X)=a_kX^k+a_{k-1}X^{k-1}+\\dots+a_1X+a_0, \\\\ &amp; B(X)=b_{\\ell}X^{\\ell}+b_{\\ell-1}X^{\\ell-1}+\\dots+b_1X+b_0, \\\\ \\end{split}\\] where \\(k\\geq\\ell\\). Then, we have the following result. Proposition D.1 Let \\(A(X)\\) and \\(B(X)\\) be the polynomials defined above. Then, there exist polynomials \\(Q(X)\\) and \\(R(X)\\) such that \\[A(X)=B(X)Q(X)+R(X),\\] where the degree of \\(R\\) is smaller than \\(\\ell\\). In this case, \\(Q\\) is called the quotient and \\(R\\) is called the remainder of the division. The algorithm is very similar to the algorithm of integer division. The idea is to try to find factors such that, when multiplied by \\(B(X)\\), allow us to remove the highest-degree term left in \\(A(X)\\), and use these terms to build \\(Q(X)\\). Although very simple, the idea is cumbersome to explain for general polynomials, so we illustrate it with an example. Let \\[A(X)=3X^5+X^4+2X^2+7,\\qquad B(X)=X^3+X^2+X.\\] We arrange them in the usual diagram: \\[\\begin{array}{rrrrrrl} 3X^5 &amp; +x^4 &amp; &amp; +2x^2 &amp; &amp; +7 \\qquad &amp; |\\quad X^3+X^2+X\\\\ &amp;&amp;&amp;&amp;&amp;&amp; \\overline{\\phantom{|\\quad X^3+X^2+X}} \\\\ \\end{array}\\] We want to remove \\(3X^5\\), so we try to find a factor \\(f(X)\\) such that \\(f(X)X^3=3X^5\\). Thus, we choose \\(f(X)=3X^2\\), multiply it by \\(B(X)\\), and subtract the result from \\(A(X)\\), obtaining a lower-degree polynomial: \\[\\begin{array}{rrrrrrl} 3X^5 &amp; +X^4 &amp; &amp; +2X^2 &amp; &amp; +7 \\qquad &amp; |\\quad X^3+X^2+X\\\\ &amp;&amp;&amp;&amp;&amp;&amp; \\overline{\\phantom{|\\quad X^3+X^2+X}} \\\\ &amp; -2X^4 &amp; -3X^3 &amp; +2X^2 &amp; &amp;+ 7 \\qquad &amp; 3X^2\\\\ \\end{array}\\] We now look at the new polynomial, \\(-2X^4-3X^3+2X^2+7\\), and again try to remove the highest-degree monomial, which we achieve by multiplying \\(X^3\\) by \\(-2X\\). \\[\\begin{array}{rrrrrrl} 3X^5 &amp; +X^4 &amp; &amp; +2X^2 &amp; &amp; +7 \\qquad &amp; |\\quad X^3+X^2+X\\\\ &amp;&amp;&amp;&amp;&amp;&amp; \\overline{\\phantom{|\\quad X^3+X^2+X}} \\\\ &amp; -2X^4 &amp; -3X^3 &amp; +2X^2 &amp; &amp;+ 7 \\qquad &amp; 3X^2-2X\\\\ &amp;&amp;&amp;&amp;&amp;&amp; {\\phantom{|\\quad X^3+X^2+X}} \\\\ &amp; &amp; -X^3 + 4X^2 &amp;&amp;&amp;+ 7 \\qquad &amp; \\\\ \\end{array}\\] Again, we want to remove the term \\(-X^3\\), so we multiply \\(B(X)\\) by \\(-1\\): \\[\\begin{array}{rrrrrrl} 3X^5 &amp; +X^4 &amp; &amp; +2X^2 &amp; &amp; +7 \\qquad &amp; |\\quad X^3+X^2+X\\\\ &amp;&amp;&amp;&amp;&amp;&amp; \\overline{\\phantom{|\\quad X^3+X^2+X}} \\\\ &amp; -2X^4 &amp; -3X^3 &amp; +2X^2 &amp; &amp;+ 7 \\qquad &amp; 3X^2-2X-1\\\\ &amp;&amp;&amp;&amp;&amp;&amp; {\\phantom{|\\quad X^3+X^2+X}} \\\\ &amp; &amp; -X^3 + 4X^2 &amp;&amp;&amp;+ 7 \\qquad &amp; \\\\ &amp;&amp;&amp;&amp;&amp;&amp; {\\phantom{|\\quad X^3+X^2+X}} \\\\ &amp; &amp; &amp; 5X^2 &amp; +X &amp; +7 \\qquad &amp; \\\\ \\end{array}\\] Since the degree of the current dividend is smaller than the degree of the divisor, we are done. We conclude that the quotient of the division of \\(A(X)\\) by \\(B(X)\\) is \\[3X^2-2X-1,\\] and the remainder is \\[5X^2+X+7.\\] Note that the above example works in \\(\\mathbb{Q}\\), but in general operations on the coefficients must take into account which field we are in. For example, if our coefficients are in \\(\\mathbb{F}_5\\), we cannot “divide” by \\(3\\), but we can find the inverse of \\(3\\) modulo \\(5\\), and we must ensure that every coefficient is reduced modulo \\(5\\). "],["sagemath-cookbook.html", "E SageMath Cookbook Installing and running E.1 Basic Operations E.2 Bits, Bytes and Encoding E.3 Symmetric Key Cryptography E.4 Number theory E.5 Groups and Fields E.6 Polynomials E.7 RSA E.8 Safe Prime Groups E.9 Diffie Hellman Key Exchange E.10 ElGamal over Safe Prime Groups E.11 Elliptic Curve Cryptography: secp256k1 Curve E.12 Secret Sharing E.13 Threshold Cryptography E.14 Zero Knowledge Proofs", " E SageMath Cookbook In this appendix we provide a SageMath “cookbook” that contains small self-contained code snippets that are usefull for the course. You can refer to the docs for a more in-depth documentation. We emphasize that the “recipes” presented in this Appendix should under no circumstances be used in the real world. Creating secure cryptographic protocols is a notoriously hard task which needs to follow very specific guidelines. These code snippets make a lot of simplifications and are meant to be used only for educational purposes in the context of this course. Installing and running Installation depends on the operating system you are using. Pleaser refer to this guide for detailed instructions. To launch SageMath simply type sage on a terminal/command prompt. If you are familiar with python’s Jupyter Notebook you can run sage -n jupyter to use it. When in the sage environment, you can simply start coding interactively. You can also edit and save .sage files with any text editor/IDE. To load .sage files, open an interactive sage shell and type load('filename.sage'). You can look into the Guided Tour to get familiarized with sage. If you have little familiarity with python, chances are that you won’t need to study anything more to understand and write code! E.1 Basic Operations In this subsection we give a short overview for handling basic arithmetic and comparison operations. Arithmetic Operations In the next recipe we show how to perform arithmetic operations in sage. Note that exponentiation can also be done using the ^ operator (in contrast to python that uses the ** operator). a = 42 b = 9 # Basic arithmetic operations add = a + b assert add == 51 sub = a - b assert sub == 33 mul = a * b assert mul == 378 div = a / b assert div == 14/3 # We can convert a to a float div_float = float(a / b) quot = a // b assert quot == 4 mod = a % b assert mod == 6 # can also use a**4 exp = 2^10 assert exp == 1024 # the second argument is the base. # default is e lg = log(1024, 2) assert lg == 10 # computing n-th roots # sqrt is for symbolic calculations sq4 = 81^(1/4) assert sq4 == 3 Comparison Operations As demonstrated in the next recipe, comparison operations work as expected. res = (1 &gt; 1) assert not res res = (1 &gt;= 1) assert res res = (12 &lt; 15) assert res res = (10 &lt;= 15) assert res res = (20 == 21) assert not res res = (True != False) assert res E.2 Bits, Bytes and Encoding In this subsection we present ways to handle low level builiding blocks such as bits, bytes and ASCII Encoding. Bitwise Operations The next recipe shows how to perform bitwise logical operations (AND, OR, XOR etc). # 0b0101 represent a binary number # 0x65 represents a hexadecimal number # to see different representations use the functions bin(n) and hex(n) # a = 11, b = 12 a = 0b1011 b = 0b1100 # AND log_and = a &amp; b assert log_and == 0b1000 # OR log_or = a | b assert log_or == 0b1111 # XOR # SageMath uses &quot;^^&quot; for xor since &quot;^&quot; is used for exponentiation log_xor = a ^^ b assert log_xor == 0b0111 # left and right shift lshift = a &lt;&lt; 2 assert lshift == 0b101100 rshift = a &gt;&gt; 2 assert rshift == 0b10 Encode/Decode ASCII The next recipe shows how to encode/decode to ASCII. You can find an ASCII table here. from sage.crypto.util import bin_to_ascii, ascii_to_bin msg = &quot;I need to be encoded&quot; ascii_bits = ascii_to_bin(msg) msg_prime = bin_to_ascii(ascii_bits) assert msg == msg_prime Sampling Random Bitstring The next recipe shows how to sample random bitstrings. There are many ways to do this actually. We use a more functional style when possible to keep the code cleaner. import random length = 128 def rand_bitstring(n): &quot;&quot;&quot; Samples n random bits and represents them as str. &quot;&quot;&quot; # loop length times and in each loop sample a 0/1 value. # Then join the sampled values return &#39;&#39;.join(str(random.choice([0, 1])) for _ in range(n)) Sampling Random Bytes Same thing, but this time with bytes. n = 32 def rand_bytes(n): &quot;&quot;&quot; Samples n random byte. &quot;&quot;&quot; # loop length times and in each loop sample a [0..256) value. # Then join the sampled values return b&#39;&#39;.join(randrange(0,256).to_bytes() for _ in range(n)) Xoring bytes In the next recipe we show how we can XOR two byte strings. Recall that for binary numbers, one can directly use the operator ^^ (but be carefull to first convert to numbers if needed!). def xor_bytes(lbytes, rbytes): &quot;&quot;&quot; XORs two sets of bytes. It assumes they have the same length. &quot;&quot;&quot; if len(lbytes) != len(rbytes): raise ValueError(&quot;Bytes must have equal length&quot;) # Apply the XOR (^^) operator for each pair of bytes # and collect the result in bytes return bytes(x ^^ y for x, y in zip(lbytes, rbytes)) E.3 Symmetric Key Cryptography In this subsection we present various recipes that are related to symmetric key cryptography. One-Time-Pad The next recipe shows how to implement OTP in sage. As you can see, it is quite easy! The only difficulty is handling the types (ascii encodings, bytes etc). Depending on semantics, you might want to modify the following code snippet, for example you might want to directly apply one-time-pad on binary numbers. import random # For ascii encoding/decoding from sage.crypto.util import bin_to_ascii, ascii_to_bin def xor(a, b): &quot;&quot;&quot; Helper function to apply the xor function to two strings representing binary numbers. It returns an str with the result. &quot;&quot;&quot; # apply the function xor (^^) elementwise to a and b and collects the result # we need first to convert the elements to ints to apply the ^^ function # note that the bigger list is trimmed by the list map c = list(map(lambda a_i, b_i: str(int(a_i) ^^ int(b_i)), a, b)) return &quot;&quot;.join(c) def otp_keygen(length): &quot;&quot;&quot; Sample an OTP key. This is a random bitstring of length &quot;length&quot; &quot;&quot;&quot; # loop length times and in each loop sample a 0,1 value. # Then join the sampled values return &#39;&#39;.join(str(random.choice([0, 1])) for _ in range(length)) def otp_encrypt(key, message): &quot;&quot;&quot; One time pad encryption. The key is an str representing binary values and the message an str. The outputted ciphertext is an ASCII encoded str. &quot;&quot;&quot; # Get the ascii bits of the message message_bits = str(ascii_to_bin(message)) if len(key) &lt; len(message_bits): raise ValueError(&quot;OTP Key is too small for the message&quot;, message) return # apply the bitwise xor function ciphertext = xor(message_bits, key) # We decode the ciphertext so that the types of m, c are the same. #This way we can implement decryption by simply calling encrypt(key, ciphertext) return bin_to_ascii(&quot;&quot;.join(ciphertext)) def otp_decrypt(key, ciphertext): &quot;&quot;&quot; One time pad decryption. This simply calls encrypt on the same arguments. &quot;&quot;&quot; return otp_encrypt(key, ciphertext) Here is how one could use the above otp implementation. # We use the otp.sage recipe load(&#39;otp.sage&#39;) # sample a long OTP key k = otp_keygen(32768) m = &quot;I need to be encrypted with perfect secrecy!&quot; # encrypt c = otp_encrypt(k, m) # decrypt m_prime = otp_decrypt(k, c) assert m == m_prime PRNG: SHA256 The next recipe shows how to construct a PRNG. There are various ways to do this. A simple one, that is presented here, consists of taking a seed s and compute H(s||1)||H(s||2)||...||H(s||k) where \\(H\\) is a cryptographic hash function. You choose an appropriate k to define the desired PRNG output length. For the hash function, we use the SHA256 hash function. The implementation is taken from the hashlib library. You can use a hash function of your choice instead, just make sure it is cryptographically secure! from hashlib import sha256 def sample_seed(n): &quot;&quot;&quot; Samples a random seed. The length represents the number of bytes of the seed. The output is given in bytes. &quot;&quot;&quot; # sample n random bytes return b&#39;&#39;.join(randrange(0,256).to_bytes() for _ in range(n)) def sha256_prg(seed, length): &quot;&quot;&quot; Uses hash function sha256 to construct a PRNG. sha256 maps bytes of arbitrary lengths to a fixed 256 bit (=32 byte) output. To construct a PRNG using a seed s compute: sha256(seed||0), sha256(seed||1), ..., sha256(seed||k) until the length is greater than or equal to length. Then truncate any remaining bytes to get the desired output. &quot;&quot;&quot; # Initialize output as an empty byte string output = b&#39;&#39; # The counter concatenated to seed in each iteration counter = 0 while len(output) &lt; length: # prepare the input seed || counter hash_input = seed + counter.to_bytes() # hash (seed || counter) using sha256 hash_output = sha256(hash_input).digest() # add to output and prepare counter for next round output += hash_output counter += 1 return output[:length] Here how you can actually use the above code. # We use the sha256_prng.sage recipe load(&#39;sha256_prng.sage&#39;) # sample a 256bit (=32 byte) length seed seed = sample_seed(32) # output a 8192bit (1024 bytes) pseudorandom byte stream output = sha256_prg(seed, 1024) Block Ciphers: AES128 In the next recipe, we show how to use AES128 block cipher via the Crypto.Cipher implementation. As we have discussed, you should NEVER use block ciphers in ECB mode! In the following implementation we seem to be using AES with ECB mode but this is not the case. We simply do that to get access to the “pure” (i.e. no mode of operation) AES block cipher. Whenever we use it, we always encrypt a single block (equivalent to running AES). The reason for doing it this way, is because the libraries that implement block ciphers also implement modes of operations and the interface requires to choose which mode we want to use. On our usecase, we want to demonstrate later how to implement modes of operations ourselves and utilizing the existing library this way is the easiest way to do this. Alternatively, we could re-implement AES, but this is a fairly complex construction on its own. # We use the Crypto implementation of AES from Crypto.Cipher import AES # We use the random_bytes.sage and xor_bytes recipes load(&#39;random_bytes.sage&#39;) load(&#39;xor_bytes.sage&#39;) def aes_keygen(): &quot;&quot;&quot; Samples random 16 bytes (=128 bits) for an AES128 key &quot;&quot;&quot; return rand_bytes(16) def aes_enc(key, message_block): &quot;&quot;&quot; Runs the AES128 encryption on message_block with the specified key. The length of message_block must be 16. &quot;&quot;&quot; if len(message_block) != 16: raise ValueError(&quot;AES message must be exactly 16 bytes!&quot;) # CAUTION: We do not actually use ECB mode! The cipher object needs a mode in order # to be initialized and since we want the &quot;pure&quot; permutation we initialize with ECB Mode # and always use it to encrypt blocks of size exactly 16. This is equivalent to running # the AES128 block cipher cipher = AES.new(key, AES.MODE_ECB) return cipher.encrypt(message_block) def aes_dec(key, ciphertext_block): if len(ciphertext_block) != 16: raise ValueError(&quot;AES message must be exactly 16 bytes!&quot;) # CAUTION: We do not actually use ECB mode! The cipher object needs a mode in order # to be initialized and since we want the &quot;pure&quot; permutation we initialize with ECB Mode # and always use it to encrypt blocks of size exactly 16. This is equivalent to running # the AES128 block cipher cipher = AES.new(key, AES.MODE_ECB) return cipher.decrypt(ciphertext_block) Note in the above how we make sure that we always encrypt one block (32 bytes). Next we show how to use the above code. # We use the aes128.sage recipe load(&#39;aes128.sage&#39;) # sample an aes128 key k = aes_keygen() # choose a random 16-byte long message to encrypt m = rand_bytes(16) # encrypt c = aes_enc(k, m) # decrypt m_prime = aes_dec(k, c) assert m == m_prime Modes of Operation: AES128-CBC Next we show how to use AES128 in CBC mode. Note that we do not handle padding for the sake of simplicity. We make sure that we always encrypt a multiple of block size bytes (32 in this case). # We use the aes128.sage recipe load(&#39;aes128.sage&#39;) def aes_cbc_enc(key, message): &quot;&quot;&quot; Runs the AES128 encryption on a message using CBC mode with the specified key. The length of message must be a multiple of 16. &quot;&quot;&quot; if len(message) % 16 != 0: raise ValueError(&quot;Message length must be a multiple of 16&quot;) block_size = 16 # Sample a random IV iv = rand_bytes(block_size) n_blocks = len(message) / block_size # Initialize ciphertext with the iv (corresponding to c_0) ciphertext = iv # We need to compute c_i = Enc_k(m_i XOR c_{i-1}) for i in range(n_blocks): # c_{i-1} c_prev = ciphertext[i * block_size : (i+1) * block_size] # m_i m_chunck = message[i * block_size : (i+1) * block_size] # m_i XOR c_{i-1} block_input = xor_bytes(c_prev, m_chunck) # Enc(k, m_i XOR c_{i-1}) and add it to the ciphertext ciphertext += aes_enc(key, block_input) return ciphertext def aes_cbc_dec(key, ciphertext): &quot;&quot;&quot; Runs the AES128 decryption on a message using CBC mode with the specified key. The length of the ciphertext must be a multiple of 16. &quot;&quot;&quot; if len(ciphertext) % 16 != 0: raise ValueError(&quot;ciphertext length must be a multiple of 16&quot;) block_size = 16 # To compute the number of block we subtract 1 corresponding to the IV n_blocks = len(ciphertext) / block_size - 1 message = b&#39;&#39; # We need to compute m_i = Dec_k(c_i) XOR c_{i-1} for i in range(n_blocks): # c_{i-1} c_prev = ciphertext[i * block_size : (i + 1) * block_size] # c_{i} c = ciphertext[(i + 1) * block_size : (i + 2) * block_size] # Dec_k(c_i) dec_c = aes_dec(key, c) # Dec_k(c_i) XOR c_{i-1} and add it to the message message += xor_bytes(dec_c, c_prev) return message Next, we demonstrate how to use the above code. # We use the aes_cbc.sage recipe load(&#39;aes_cbc.sage&#39;) # sample an aes128 key k = aes_keygen() # choose a random 100 * 16-byte long message to encrypt m = rand_bytes(100 * 16) # encrypt c = aes_cbc_enc(k, m) # decrypt m_prime = aes_cbc_dec(k, c) assert m == m_prime Modes of Operation: AES128-Counter We do the same thing but for counter mode of operation. # We use the aes128.sage recipe load(&#39;aes128.sage&#39;) def aes_counter_enc(key, message): &quot;&quot;&quot; Runs the AES128 encryption on a message using counter mode with the specified key. The length of message must be a multiple of 16. &quot;&quot;&quot; if len(message) % 16 != 0: raise ValueError(&quot;Message length must be a multiple of 16&quot;) block_size = 16 # Sample a random IV used as the counter iv = rand_bytes(block_size) n_blocks = len(message) / block_size # Initialize ciphertext with the counter ciphertext = iv # We need to compute c_i = m_i XOR Enc_k(counter + i) for i in range(n_blocks): # m_i m_chunk = message[i * block_size : (i+1) * block_size] # counter + i counter = (int.from_bytes(iv) + i).to_bytes(16) # Enc_k(counter + i) encrypted_counter = aes_enc(key, counter) # m_i XOR Enc_k(counter + i) ciphertext += xor_bytes(m_chunk, encrypted_counter) return ciphertext def aes_counter_dec(key, ciphertext): &quot;&quot;&quot; Runs the AES128 decryption on a ciphertext using counter mode with the specified key. The length of ciphertext must be a multiple of 16. &quot;&quot;&quot; if len(ciphertext) % 16 != 0: raise ValueError(&quot;Ciphertext length must be a multiple of 16&quot;) block_size = 16 # extract the iv iv = ciphertext[:block_size] # To compute the number of block we subtract 1 corresponding to the counter n_blocks = len(ciphertext) / block_size - 1 message = b&#39;&#39; # We need to compute m_i = c_i XOR Enc_k(counter + i) # Note that counter mode does not need decryption. for i in range(n_blocks): # c_i c_chunk = ciphertext[(i + 1) * block_size : (i + 2) * block_size] # counter + i counter = (int.from_bytes(iv) + i).to_bytes(16) # Enc_k(counter + i) encrypted_counter = aes_enc(key, counter) # c_i XOR Enc_k(counter + i) message += xor_bytes(c_chunk, encrypted_counter) return message And here is how you can use the above code: # We use the aes128.sage recipe load(&#39;aes_counter.sage&#39;) # sample an aes128 key k = aes_keygen() # choose a random 100 * 16-byte long message to encrypt m = rand_bytes(100 * 16) # encrypt c = aes_counter_enc(k, m) # decrypt m_prime = aes_counter_dec(k, c) assert m == m_prime Hash Functions Finally, we show how to use hash functions with the help of hashlib library. The library supports various different functions so you can easily change the hash function choice if you need to. import hashlib # Hashlib supports various hash functions. List them algorithms = hashlib.algorithms_available msg = b&quot;I need to be hashed. Please use sha256 to hash me&quot; # if it is an str instead of bytes use msg.encode() sha256_digest = hashlib.sha256(msg).hexdigest() E.4 Number theory In this subsection we demonstrate how to use sage for some very basic number theory. Sampling Random Primes A recipe to sample random primes of specific bitlength (meaning they need at least these many bits to be represented). bit_length = 1024 lower_bound = 2^(bit_length-1) upper_bound = 2^(bit_length)-1 p = random_prime(lower_bound, upper_bound) Factoring The next recipe factors numbers. Try to see how big numbers you can sample in a reasonable amount of time! # We can simply use the built in function factor n = 720 factorization = factor(n) Primality Testing We next show how to do primality testing in sage. Note that we simply use the in keyword that checks membership in a set. assert (7 in Primes()) assert not(8 in Primes()) Extended Euclidian Algorithm The next recipe shows how to run the (Extended) Euclidian Algorithm in sage. a = 20 b = 35 # Compute the greatest common divisor of a and b d = gcd(a, b) assert d == 5 # Check coprimality are_coprimes = (gcd(a,b) == 1) assert are_coprimes == false # Compute d, x, y s.t. d = ax + by and d is the greatest # common divisor of a and b (d, x, y) = xgcd(a, b) assert d == x*a + y*b Modular Arithmetic Finally, we show how to do efficient modular arithmetic in sage. It is basically enough to define the correct ring (numbers mod n with addition and multiplication mod n) and then work just as you would in integer arithmetic. You don’t even need to care about efficiency, sage knows that it should use -for example- fast exponentiation when you exponentiate on this ring. # Define integers ring Z17 Z17 = Integers(17) # Define elements a = Z17(3) b = Z17(15) # Addition add = a + b assert add == 1 # Subtraction sub = a - b assert sub == 5 # Multiplication mul = a * b assert mul == 11 # Inversion inv_a = a^(-1) assert inv_a == 6 # Exponentiation exp = a^3 assert exp == 10 # Solve linear equation 15x = 3 mod n sol = a * b^(-1) E.5 Groups and Fields We next show how to use sage to work with groups and fields. This is actually more useful in symbolic calculations rather than with cryptography, but it is useful to familiarize with the interface a bit. Additive Groups We showcase how to handle groups whose operation is denoted additively. # Define the group (Z11, +) G = AdditiveAbelianGroup([11]) # list the elements of G list(G) # get the default generator g = G.gens()[0] # identity element e = G.identity() # group operation h1 = g + g h2 = 7*g # equals g + g + g + g + g+ g + g # adding the identity element h3 = h2 + e assert h3 == h2 # inversion invh1 = -h1 assert invh1 + h1 == e # order of G ordG = G.order() assert ordG == 11 # random element hrandom = G.random_element() Unit Groups Same thing for unit groups. For our purposes we only care for \\(\\mathbb{Z}_n^*\\) unit groups, meaning the invertible elements \\(\\mod n\\). Note that sage treats these elements abstractly (as $f, f^2,…) for a formal variable f representing a group generator. We use the inject_variables command to bring this symbol in scope. We can then see the actual \\(\\mathbb{Z}_p^*\\) element corresponding to \\(f^i\\) as Zn(f^i) where Zn is our ring. # Define the ring (Z11, +, *) Z11 = Integers(11) # Define the group (Z11, *) G = Integers(11).unit_group() # list the elements of G w.r.t. a generator. # Note that this gives a list [1, f, f^2, ...] list(G) # To see actual elements, we need to map f^i to an element # in the ring # This brings to scope the variable f G.inject_variables() # We can now map f to a ring element h = Z11(f^2) assert h == 4 # get the default generator g = G.gens()[0] assert g == f # get the first generator # g = generators[0] # identity element e = G.identity() # group operation h1 = g * g assert Z11(h1) == 4 h2 = g^7 # equals g * g * g * g * g * g * g assert Z11(h2) == (2^7) % 11 h3 = h2 * e assert h3 == h2 # inversion invh3 = h3^(-1) assert invh3 * h3 == e # order of G ordG = G.order() assert ordG == 10 # random element hrandom = G.random_element() # order of an element ord_h3 = h3.order() # list subgroups # Note that G is of order 10 = 2 * 5. It has: # - a (trivial) subgroup of size 1 # - a subgroup of size 2 # - a subgroup of size 5 # - a subgroup of size 2*5 # The output gives a list: # [Multiplicative Abelian subgroup isomorphic to C2 x C5 generated by {f}, # Multiplicative Abelian subgroup isomorphic to C5 generated by {f^2}, # Multiplicative Abelian subgroup isomorphic to C2 generated by {f^5}, # Trivial Abelian subgroup] G.subgroups() # Get the subgroupof of order 5 H = G.subgroups()[2] # Get the subgroup defined by f^5 # We rename the generator to h5 since we already have f H5 = G.subgroup([f^5], &#39;h5&#39;) H5.inject_variables() assert h5 == f^5 # membership in subgroup assert (f^10 in H5) assert not(f^8 in H5) Finite Fields Finally we show how to work with finite fields. Recall that a field is a prime field (\\(\\mathbb{Z}_p\\) for a prime \\(p\\)) or it is represented by polynomials. In the second case, we can choose the formal variable symbol. # Define the prime field Z11 F11 = GF(11) # Define the galois field 5^4 # An optional argument can be given to define the variable name F = GF(5^4, &#39;x&#39;) # get the modulus used F.modulus() # get additive and multiplicative generator g_add = F.gen() g_mul = F11.multiplicative_generator() # we can do the normal operations as usual f1 = list(F)[2] f2 = list(F)[7] f3 = list(F)[10] f4 = (f1 + f2) * f3^(-1) # Define field elements. Note they are reduced mod F.modulus() p = F(&quot;x^10 + 3*x^2 + 1&quot;) q = F(&quot;3*x^5 + 1&quot;) r = p*q + f2^(-1) # additive and multiplicative order of elements ord_f1 = f1.order() mord_f1 = f1.multiplicative_order() # random element frandom = F.random_element() E.6 Polynomials In this subsection we show how to use sage to handle polynomials. Basic Polynomials Operations We first show how to define and do basic operations on polynomials. Note that whenever you work with polynomials, you should define a field over which the coefficients live, for example \\(\\mathbb{Z}_2\\) or \\(\\mathbb{R}\\). For our use-cases, we will work with polynomials with coefficients in \\(\\mathbb{Z}_p\\) for prime \\(p\\). # Define the polynomial ring over a field # Here we define F_17[x] (polynomials over GF(17) with formal variable x) R.&lt;x&gt; = PolynomialRing(GF(17)) f = x^3 + 2*x^2 + 1 g = 4*x^4 + 3*x +6 h = 8*x + 1 # We can do operations as expected p = f^2 - g + h*g # We can also compute quotients and remainders quot = p//h rem = p % h assert p == h * quot + rem # Evaluate a polynomial v = h(x=3) assert v == (8*3 + 1) % 17 q = x^2 - 1 Extended Euclidian Algorithm for Polynomials We next show how to run the Extended Euclidian Algorithm on polynomials. Note that sage is clever enough to know how to do this on each own, simply use the xgcd algorithm and give appropriate inputs. If the inputs are polynomials, it will run the polynomial version and if integers the integer version! # Define the polynomial ring over Integers R.&lt;x&gt; = PolynomialRing(ZZ) p = x^8 + 6*x^4 + x q = 4*x^2 # Find the greatest common divisor d = gcd(p, q) # Run the Extended Euclidian Algorithm (dd, xx, yy) = xgcd(p, q) assert dd == xx*p + yy*q Polynomial Interpolation Finally, we show how to interpolate polynomials. Recall this means finding the minimal degree polynomial that “crosses” a set of points. We also show how to find Lagrange polynomials. This is interpolation over specific sets: given x coordinates, the \\(i\\)-th Lagrange polynomial is 0 in all the points except the \\(i\\)-th, where it evaluates to 1. # Define the polynomial space R = PolynomialRing(GF(17), &#39;x&#39;) # Define the set of points points = [(5,1), (6,8), (9,12)] # Find the interpolating polynomial p = R.lagrange_polynomial(points) # Verify the result assert_cond = (p.degree() &lt; 3 and p(x=5) == 1 and p(x=6) == 8 and p(x=9) == 12) assert assert_cond # Given a set of x coordinates, find the lagrange polynomials x_coord = [5, 6, 9] # l5 points = [(5,1), (6,0), (9,0)] l1 = R.lagrange_polynomial(points) points = [(5,0), (6,1), (9,0)] l2 = R.lagrange_polynomial(points) points = [(5,0), (6,0), (9,1)] l3 = R.lagrange_polynomial(points) assert p == l1 * 1 + l2 * 8 + l3 * 12 E.7 RSA In this subsection we present various recipes related to RSA. Note that some parts (for example choosing a modulus) are used in all of them. Sampling an RSA Modulus We first show how to sample an RSA modulus. This is basically sampling two fixed length prime number and multiplying them. # Define the security parameter (bits of the modulus) secparam = 2048 # To achieve secparam modulus n = p*q, p and q must be secpara/2 bits long prime_length = secparam/2 lower_bound = 2^(prime_length-1) upper_bound = 2^(prime_length)-1 p = random_prime(lower_bound, upper_bound) q = random_prime(lower_bound, upper_bound) N = p*q Sampling an RSA key-pair We next show how to sample a full RSA key-pair. We need to sample the modulus as before and additionally compute the \\(e,d\\) pair of RSA. def rsa_keygen(secparam): &quot;&quot;&quot; Takes the secparam and returns an RSA secret/public key pair The output is given as two dictionaries: sk = {&quot;p&quot;: p, &quot;q&quot;: q, &quot;d&quot;: d} pk = {&quot;N&quot;: N, &quot;e&quot;: e} &quot;&quot;&quot; # Sample an RSA modulus prime_length = secparam // 2 lower_bound = 2^(prime_length-1) upper_bound = 2^(prime_length)-1 p = random_prime(lower_bound, upper_bound) q = random_prime(lower_bound, upper_bound) N = p * q #compute phi(N) phi = (p - 1) * (q - 1) # Sample a random e that is invertible mod phi_N. # One can also use other ways (e.g. e = 2^16 - 1) e = randrange(1, N) while gcd(e, phi) != 1: e = randrange(1, N) d = pow(e, -1, phi) # p, q are not actually needed sk = {&quot;p&quot;: p, &quot;q&quot;: q, &quot;d&quot;: d} pk = {&quot;N&quot;: N, &quot;e&quot;: e} return (sk, pk) Textbook RSA Encryption In the next recipe we implement the “textbook” RSA encryption scheme. Recall that this is completely insecure (any deterministic encryption scheme is!). # We use the rsa_keygen.sage recipe load(&#39;rsa_keygen.sage&#39;) def textbook_rsa_encrypt(pk, message): &quot;&quot;&quot; Encrypts an RSA message as m^e mod N. The message must be an Integers(N) element. We assume the pk is of the form output by rsa_keygen. &quot;&quot;&quot; return message^(pk[&#39;e&#39;]) def textbook_rsa_decrypt(sk, ciphertext): &quot;&quot;&quot; Decrypts an RSA ciphertext as c^d mod N. The ciphertext must be an Integers(N) element. We assume the sk is of the form output by rsa_keygen. &quot;&quot;&quot; return ciphertext^(sk[&#39;d&#39;]) Here is how to use the above for encryption/decryption. # We use the textbook_rsa.sage recipe load(&#39;textbook_rsa.sage&#39;) # Create a key sk, pk = rsa_keygen(2048) # Define ZN N = pk[&#39;N&#39;] ZN = Integers(N) # Define the message to be encrypted m = ZN(42) # encrypt c = textbook_rsa_encrypt(pk, m) # decrypt m_prime = textbook_rsa_decrypt(sk, c) assert m == m_prime RSA-FDH Signatures The next recipe implements RSA full domain hash signatures. We use the SHA256 from hashlib for the hash function. import hashlib # We use the rsa_keygen.sage recipe to sample keys load(&#39;rsa_keygen.sage&#39;) def rsa_sign(sk, m): &quot;&quot;&quot; Signs a message using RSA full domain hash with sha256 as the hash function. The message m can be any string. We assume the sk is of the form output by rsa_keygen. &quot;&quot;&quot; # hash the message digest = hashlib.sha256(m.encode()).hexdigest() # convert the digest to an integer by reading it as a hex number digest = int(&#39;0x&#39;+ digest, 16) # and finally convert it to Zn element p, q = sk[&#39;p&#39;], sk[&#39;q&#39;] # represent the number as a ZN element N = p * q ZN = Integers(N) digest = ZN(digest) d = sk[&#39;d&#39;] return digest^d def rsa_verify(pk, m, signature): &quot;&quot;&quot; Verifies a signature using RSA full domain hash with sha256 as the hash function. The message m can be any string. We assume the pk is of the form output by rsa_keygen. &quot;&quot;&quot; # hash the message digest = hashlib.sha256(m.encode()).hexdigest() # convert the digest to an integer by reading it as a hex number digest = int(&#39;0x&#39;+ digest, 16) # and finally convert it to Zn element p, q = sk[&#39;p&#39;], sk[&#39;q&#39;] N = p * q ZN = Integers(N) digest = ZN(digest) e = pk[&#39;e&#39;] return signature^e == digest Next we show how to use the above to sign messages and verify signatures. # We use the rsa_sha_signature.sage recipe to sample keys load(&#39;rsa_sha256_signatures.sage&#39;) # Create a key sk, pk = rsa_keygen(2048) m = &quot;&quot;&quot;This is a message whose authenticity is crucial. My ElGamal key is: ({&#39;p&#39;: 8607589379706225605009318235670632502463991112012865771933407304935190017403, &#39;q&#39;: 4303794689853112802504659117835316251231995556006432885966703652467595008701, &#39;g&#39;: 3814814891600025220956964856350040994409974231368858236343451345844148462986, &#39;h&#39;: 5088131483015987373087418941552710750500206632061904706006805181556567516887}, 3659275803126012824071303245940415271044329304500778431458890743996297880993)&quot;&quot;&quot; # sign signature = rsa_sign(sk, m) # verify result = rsa_verify(pk, m, signature) assert result E.8 Safe Prime Groups In this subsection we implement recipes to handle “safe prime” groups. Recall that these are subgroups of \\(\\mathbb{Z}_p^*\\) for p prime, which have prime order \\(q\\). The relation between \\(p,q\\) is \\(p = kq+1\\) for some small \\(k\\) (we always use \\(k=2\\)). Sampling Safe Primes The next recipe shows how to sample safe primes. We do this by simply sampling a prime \\(q\\) and seeing if \\(2q+1\\) is also a prime. We repeat until we find one. def sample_prime(length): &quot;&quot;&quot; Samples a prime with length bits. &quot;&quot;&quot; lower_bound = 2^(length-1) upper_bound = 2^(length)-1 return random_prime(lower_bound, upper_bound) def sample_safe_prime(length): &quot;&quot;&quot; Samples a prime number of the form p = 2q + 1 s.t. q is also prime. p must have length bits. &quot;&quot;&quot; p = sample_prime(length) while not (((p - 1) // 2) in Primes()): p = sample_prime(length) return p Next we show how to use the above recipe. # We use the safe_primes.sage recipe load(&#39;safe_primes.sage&#39;) safe_prime = sample_safe_prime(128) assert (safe_prime in Primes()) q = (safe_prime - 1) // 2 assert (q in Primes()) Sampling a safe prime subgroup The next recipe samples the actual subgroup defined by a safe prime. Note that we also need to fix a generator g when defining the group. We represent the group by the tuple \\((p, q, g)\\), although \\(p\\) is actually not needed. def safeprime_group(sec_param): &quot;&quot;&quot; Samples a group of prime order q, which is a subgroup of Zp for p = 2q+1 and a generator. The output contains a dictionary with the description of the group and the subgroup and its generator &quot;&quot;&quot; # sample a safe prime lower_bound = 2^(sec_param-1) upper_bound = 2^(sec_param)-1 p = random_prime(lower_bound, upper_bound) while not (((p - 1) // 2) in Primes()): p = random_prime(lower_bound, upper_bound) # Define the units mod p Zp = Integers(p) # Sample random group elements until one has order q g = Zp.random_element() q = (p - 1) // 2 while g.multiplicative_order() != q: g = Zp.random_element() return {&#39;p&#39;: p, &#39;q&#39;: q, &#39;g&#39;: g} Here is how to use the above to sample a group # We use the sample_safeprime_group.sage recipe load(&#39;sample_safeprime_group.sage&#39;) # sample the groups G = safeprime_group(256) # access the elements p, q, g p = G[&#39;p&#39;] q = G[&#39;q&#39;] g = G[&#39;g&#39;] E.9 Diffie Hellman Key Exchange We next present the recipe for a Diffie Hellman key exchange over safe prime groups. # We use the sample_safeprime_group.sage recipe load(&#39;sample_safeprime_group.sage&#39;) def ddh_key_part(params): &quot;&quot;&quot; Computes a random a and the public g^a. params should contain the description of a safeprime group &quot;&quot;&quot; p, q, g = (params[&#39;p&#39;], params[&#39;q&#39;], params[&#39;g&#39;]) Zp = Integers(p) # secret part a = randrange(1,q) A = (g^a) return (a, A) def ddh_final_key(params, a, B): &quot;&quot;&quot; Given the secret part a and the recieved public part B computes the common key K=B^a &quot;&quot;&quot; p, q, g = (params[&#39;p&#39;], params[&#39;q&#39;], params[&#39;g&#39;]) Zp = Integers(p) return B^a Next we show how Alice a Bob would use the above to agree on a common key. # We use the ddh.sage recipe load(&#39;ddh.sage&#39;) # Alice choses parameters and sends them to bob params = safeprime_group(256) # Alice computes (a, A=g^a) and sends A to Bob (a, A) = ddh_key_part(params) # Bob computes (b, B=g^b) and sends B to Alice (b, B) = ddh_key_part(params) # Alice computes the final key K_alice = B^a # Bob computes the final key K_bob = A^b assert K_alice == K_bob E.10 ElGamal over Safe Prime Groups In this section we present recipes for the two variants (normal and lifted) of ElGamal encryption scheme over safe prime groups. Sampling ElGamal Keys First, we show a recipe for creating an ElGamal key pair. This is as simple as sampling a safeprime group and setting \\((sk = x, pk = g^x)\\) for a random key. # We use the sample_safeprime_group.sage recipe to sample parameters load(&#39;sample_safeprime_group.sage&#39;) # Sampling of keys is the same for both ElGamal and lifted ElGamal def elgamal_keygen(sec_param): # sample the group params = safeprime_group(256) (p, q, g) = (params[&#39;p&#39;], params[&#39;q&#39;], params[&#39;g&#39;]) # secret key x = randrange(1,q) # public key h = g^x pk = {&#39;p&#39;: p, &#39;q&#39;: q, &#39;g&#39;: g, &#39;h&#39;: h } return (pk, x) ElGamal Encryption In the next recipe, we implement ElGamal over a same prime group. # We use the sample_elgamal_keys.sage recipe load(&#39;sample_elgamal_keys.sage&#39;) def elgamal_encrypt(pk, m): &quot;&quot;&quot; Encrypts a message using ElGamal. Takes as input the public key pk containing p,q,g,h where - p, q are primes s.t. p = 2q + 1 - g is a generator of the q-size size subgroup of Zp - h is an element of the subgroup and the message m which is an element of the subgroup We assume the pk is a dictionary as defined in sample_elgamal_keys.sage &quot;&quot;&quot; p, q, g, h = (pk[&#39;p&#39;], pk[&#39;q&#39;], pk[&#39;g&#39;], pk[&#39;h&#39;]) Zp = Integers(p) # sample randomness r = randrange(1,q) c = (g^r, m*(h^r)) return c def elgamal_decrypt(sk, c): &quot;&quot;&quot; Decrypts a ciphertext using ElGamal. &quot;&quot;&quot; # We get p by computing the additive order of c[0]. Alternatively, # we can add this information on sk or use the pk in the decryption. p = c[0].additive_order() Zp = Integers(p) c1, c2 = c m = c2*c1^(-sk) return m Here is how one can use the above recipe to encrypt/decrypt using ElGamal. # We use the elgamal.sage recipe load(&#39;elgamal.sage&#39;) # Create an ElGamal key pair (pk, sk) = elgamal_keygen(256) # parse the public key p, q, g, h = (pk[&#39;p&#39;], pk[&#39;q&#39;], pk[&#39;g&#39;], pk[&#39;h&#39;]) Zp = Integers(p) # sample a random message # recall the message should be a *subgroup* element m = Zp.random_element() while m.multiplicative_order() != q: m = Zp.random_element() # encrypt the message c = elgamal_encrypt(pk, m) # decrypt the message m_prime = elgamal_decrypt(sk, c) assert m == m_prime Lifted ElGamal Encryption Next we present the lifted version of ElGamal. Essentially, the only changes are that: (1) the message is now a \\(\\mathbb{Z}_q\\) element in the exponent, (2) the decryption needs to bruteforce over a known domain to find the discrete log. # We use the elgamal_keys.sage recipe load(&#39;sample_elgamal_keys.sage&#39;) def lifted_elgamal_encrypt(pk, m): &quot;&quot;&quot; Encrypts a message using ElGamal. Takes as input the public key pk containing p,q,g,h where - p, q are primes s.t. p = 2q + 1 - g is a generator of the q-size size subgroup of Zp - h is an element of the subgroup and the message m which is an element of the subgroup &quot;&quot;&quot; p, q, g, h = (pk[&#39;p&#39;], pk[&#39;q&#39;], pk[&#39;g&#39;], pk[&#39;h&#39;]) Zp = Integers(p) # sample randomness r = randrange(1,q) c = (g^r, (g^m)*(h^r)) return c def lifted_elgamal_decrypt(sk, c, list): &quot;&quot;&quot; Decrypts a ciphertext using lifted ElGamal. It takes as additional input a list containing all the possible messages on which it bruteforces. &quot;&quot;&quot; # We get p by computing the additive order of c[0] (=p). Alternatively, # we can add this information on sk or use the pk in the decryption. p = c[0].additive_order() Zp = Integers(p) (c1, c2) = c g_pow_m = c2*c1^(-sk) # bruteforce to find the discrete logarithm for m in list: if g^m == g_pow_m: return m Here is how one can use the lifted version of ElGamal. # We use the lifted_elgamal_keys.sage recipe load(&#39;lifted_elgamal.sage&#39;) # Create an ElGamal key pair (pk, sk) = elgamal_keygen(256) # parse the public key p, q, g, h = (pk[&#39;p&#39;], pk[&#39;q&#39;], pk[&#39;g&#39;], pk[&#39;h&#39;]) Zp = Integers(p) Zq = Integers(q) # define the possible messages to be [0..1000) message_list = list(range(1000)) # choose a random message. Recall the message should be a Zq element! m = Zq(randrange(0,1000)) # encrypt the message c = lifted_elgamal_encrypt(pk, m) # decrypt the message m_prime = lifted_elgamal_decrypt(sk, c, message_list) assert m == m_prime E.11 Elliptic Curve Cryptography: secp256k1 Curve In these section we look into Elliptic Curve Cryptography. Specifically, we look into implementations over the secp256k1 curve. To understand the relevant recipes you need to keep in mind two things: The group defined by the curve is denoted additively, The operations are exactly the same as the ones in safeprimes groups. This means that if one replaces the group operation of multiplication \\(\\mod p\\) in the DH protocol for example with the group operation of the curve, one gets the DH protocol over the curve! Keep in mind how notation changes: Multiplication becomes addition. Exponentiation (many multiplications) becomes multiplication (many additions). The “exponents” in \\(\\mathbb{Z}_q\\) remain in \\(\\mathbb{Z}_q\\) (the order of the group in both cases). The subgroup elements of safeprimes (subset of \\(\\mathbb{Z}_p\\)) become points in the Elliptic Curve (pairs of elements of \\(\\mathbb{Z}_p\\) for a different prime \\(p\\)). Keep in mind that you do not need to know how the group operation works. You can simply forget about this and apply the operations correctly to get the corresponding protocols. secp256k1 Basic Operations The next recipe shows how to “sample” the curve. The sample is in quotations because the curve is actually fixed. You can find the curve specifications here. # secp256k1 is the elliptic curve defined by the equation # y^2 = x^3 + ax + b with a=0, b=7 over the prime field Fp # for p = 2^256 - 2^32 - 2^9 - 2^8 - 2^7 - 2^6 - 2^4 - 1 # # The curve contains the points (x, y) that satisfy the equation # y^2 = x^3 + ax + b over Fp # The number of these points are q for a prime q. This defines another # field Fq # # - The field Fp (where the coordinates are defined) is called *base field* # - The field Fq is called *scalar field* # # These points define an abelian group under elliptic curve addition operation # # The specifications of the curve define the element # G= ( # 0x79BE667EF9DCBBAC55A06295CE870B07029BFCDB2DCE28D959F2815B16F81798, # 0x483ADA7726A3C4655DA4FBFC0E1108A8FD17B448A68554199C47D08FFB10D4B8 # ) # as the generator of the group. # # The specifications of the curve can be found in # https://neuromancer.sk/std/secg/secp256k1 def secp256k1_params(): # first define the base field Fp p = 2^256 - 2^32 - 2^9 - 2^8 - 2^7 - 2^6 - 2^4 - 1 Fp = GF(p) # we next define the elliptic curve E = EllipticCurve(Fp, [0, 7]) # the scalar field contains the number of elements in the group (i.e. points) q = E.order() # finally, we define the generator Gx = 0x79BE667EF9DCBBAC55A06295CE870B07029BFCDB2DCE28D959F2815B16F81798 Gy = 0x483ADA7726A3C4655DA4FBFC0E1108A8FD17B448A68554199C47D08FFB10D4B8 G = E(Gx, Gy) return (E, p, q, G) We next show how to sample the curve and make basic operations on it. # We use the secp256k1.sage recipe load(&#39;secp256k1.sage&#39;) # Define the curve (E, p, q, g) = secp256k1_params() # Let&#39;s first sample two random points by computing # xi and setting hi = xi g # scalars live in Fq Fq = GF(q) x1 = Fq.random_element() x2 = Fq.random_element() h1 = x1 * g h2 = x2 * g # Let&#39;s verify they are on curve assert h1 in E assert h2 in E # We can add group elements # this is &quot;equivalent&quot; to multiplying in safeprime groups h3 = h1 + h2 # and we we can do scalars operation # this is &quot;equivalent&quot; to exponentiation in safeprime groups h4 = 2 * h3 assert h4 == h3 + h3 # We can also invert elements h4inv = -h4 # The identity element is the point in infinity. e = E(0) assert h4inv + h4 == e Diffie Hellman Key Exchange over secp256k1 The next recipe implements the Diffie Hellman Key Exchange over secp256k1. Note that this is the same as in the safeprime groups if one turns “multiplications” into “additions”. # This is ddh over secp256k1. It works exactly as the ddh in safeprime groups # except that we are in the group. # We use the secp256k1.sage recipe load(&#39;secp256k1.sage&#39;) # We don&#39;t need sample params, we get them ready by secp256k1 specs def secp256k1_ddh_key_part(): &quot;&quot;&quot; Computes a random a in Fq and the public a*g. &quot;&quot;&quot; (E, p, q, g) = secp256k1_params() Fq = GF(q) # secret part a = Fq.random_element() A = a*g return (a, A) def secp256k1_ddh_final_key(a, B): &quot;&quot;&quot; Given the secret part a and the recieved public part B computes the common key K=a*B &quot;&quot;&quot; return a*B Next we show how Alice and Bob can use the above to agree on a key over the curve. # We use the secp256k1_ddh.sage recipe load(&#39;secp256k1_ddh.sage&#39;) # Alice computes (a, A=a*g) and sends A to Bob (a, A) = secp256k1_ddh_key_part() # Bob computes (b, B=b*g) and sends B to Alice (b, B) = secp256k1_ddh_key_part() # Alice computes the final key K_alice = a*B # Bob computes the final key K_bob = b*A assert K_alice == K_bob ElGamal Encryption over secp256k1 Next, we implement ElGamal over the curve. Again, this is implemented as ElGamal over safe prime groups, the only thing changing is the group! One can similarly implement the lifted version. Recall that for this one should use bruteforce to do the final decryption. # We use the secp256k1.sage recipe load(&#39;secp256k1.sage&#39;) def secp256k1_elgamal_keygen(): &quot;&quot;&quot; samples the elgamal key. We don&#39;t need to take the security parameter as input since we use a fixed curve &quot;&quot;&quot; # define the group (E, p, q, g) = secp256k1_params() # secret key Fq = GF(q) x = Fq.random_element() # public key h = x*g return (h, x) def secp256k1_elgamal_encrypt(pk, m): &quot;&quot;&quot; Encrypts a message using ElGamal over secp256k1. &quot;&quot;&quot; # define the group (E, p, q, g) = secp256k1_params() # sample randomness # secret key Fq = GF(q) r = Fq.random_element() c = (r*g, r*pk + m) return c def secp256k1_elgamal_decrypt(sk, c): &quot;&quot;&quot; Decrypts a ciphertext using ElGamal over secp256k1. &quot;&quot;&quot; c1, c2 = c m = c2-sk*c1 return m We next show how encryption and decryption works. # We use the secp256k1_elgamal.sage recipe load(&#39;secp256k1_elgamal.sage&#39;) # Define the curve (E, p, q, g) = secp256k1_params() # Create an ElGamal key pair (pk, sk) = secp256k1_elgamal_keygen() # sample a random message # the message in a curve element m = 42*g # encrypt the message c = secp256k1_elgamal_encrypt(pk, m) # decrypt the message m_prime = secp256k1_elgamal_decrypt(sk, c) assert m == m_prime E.12 Secret Sharing In this subsection we present implementations for the two variants of secret sharing we have seen. Simple Secret Sharing Next, we implement the simple secret sharing (share \\(s\\) as \\(s_1, s\\oplus s_1\\) for a random \\(s_1\\)). def simple_share(s): &quot;&quot;&quot; Shares a secret s to 2 parties s.t. both need to collaborate to reconstruct. The secret is a string. &quot;&quot;&quot; # convert the secret to bytes s_encoded = s.encode() # get the length of the secret length = len(s_encoded) # sample random bytes of the same length as s. This is the first share s1 = b&#39;&#39;.join(randrange(0,256).to_bytes() for _ in range(length)) # create the second share as s XOR s1 s2 = bytes(x ^^ y for x, y in zip(s1, s_encoded)) return (s1, s2) def simple_reconstruct(s1, s2): &quot;&quot;&quot; Reconstructs a shared secret. It takes the shares (byte streams) of both parties and gets the secret &quot;&quot;&quot; s_encoded = bytes(x ^^ y for x, y in zip(s1, s2)) return s_encoded.decode() We showcase next the above by sharing our secret message. # We use the simple_secret_sharing.sage recipe load(&#39;simple_secret_sharing.sage&#39;) # Choose a secret s = &quot;The secret number is 42.&quot; # share to the parties (s1, s2) = simple_share(s) # reconstruct s_prime = simple_reconstruct(s1, s2) assert s == s_prime Shamir Secret Sharing The next recipe implements Shamir’s secret sharing scheme. It would be useful to recall the polynomial recipes before looking into the next one. def shamir_share(F, s, t, n): &quot;&quot;&quot; Shares a secret s over field F. t &lt;= n is the threshold. The public points used are always 1, ..., n &quot;&quot;&quot; # Choose a random polynomial of degree t-1 # We do this by sampling t-1 random coefficients coeff = list(F.random_element() for _ in range(t-1)) # Define the polynomial space R.&lt;x&gt; = PolynomialRing(F) # construct the polynomial p = s for i in range(t-1): p += coeff[i] * (x^(i+1)) # evaluate the polynomial at every point 1,...,n return list((i, p(x=i)) for i in range(1, n+1)) def shamir_reconstruct(F, points): &quot;&quot;&quot; Reconstructs a shared secret over field F. It takes the shares (points) of enough parties and gets the secret &quot;&quot;&quot; # Define the polynomial space R.&lt;x&gt; = PolynomialRing(F) # interpolate points to find the polynomial p = R.lagrange_polynomial(points) # evaluate at 0 to learn the secret return p(x=0) We next show how to share a secret using this scheme. # We use the shamir_secret_sharing.sage recipe load(&#39;shamir_secret_sharing.sage&#39;) # Define a field bit_length = 128 lower_bound = 2^(bit_length-1) upper_bound = 2^(bit_length)-1 p = random_prime(lower_bound, upper_bound) F = GF(p) # sample a random secret s = F.random_element() # share to 10 parties s.t. at least 4 are needed to reconstruct shares = shamir_share(F, s, 4, 10) # parties 4, 7, 9, 10 try to reconstruct the secret # recall we have points 1, 2, ... 10 but we count from 0 points = [shares[3], shares[6], shares[8], shares[9]] s_prime = shamir_reconstruct(F, points) assert s == s_prime E.13 Threshold Cryptography In this subsection we combine Shamir’s secret sharing with ElGamal encryption to implement threshold ElGamal over safe prime groups. Try to make them work over secp256k1! Threshold ElGamal We next present the threshold ElGamal recipe. Note how the parties do not need to reconstruct the secret key to decrypt. # We use the elgamal.sage and shamir_secret_sharing.sage recipes. # Recall that encryption is exactly as ElGamal load(&#39;shamir_secret_sharing.sage&#39;) load(&#39;elgamal.sage&#39;) def compute_weight(F, other_x, x_i, y_i): &quot;&quot;&quot; Helper function to copmute the &quot;weight&quot;. This equals to y_i * l(0) where l is the lagrange polynomial over F associated with all the x points. Specifically, it is the polynomial that: - is 0 on all x in other_x - is 1 on x_i &quot;&quot;&quot; # Create the points points = [(x, 0) for x in other_x] # 0 on other_x points.append((x_i, 1)) # 1 on x_i # interpolate to find l and evaluate to 0 R.&lt;x&gt; = PolynomialRing(F) l = R.lagrange_polynomial(points)(x=0) # return the result return l*y_i def elgamal_partial_decrypt(c1, w): &quot;&quot;&quot; Partially decrypts an ElGamal ciphertext. Takes as input the weight w = l * y_i, and the first part of the ciphertext c1 and outputs an updated c1^w. &quot;&quot;&quot; return c1^w Next we show how to use the above recipe to threshold-decrypt an ElGamal encrypted message. # We use the threshold_elgamal.sage recipe load(&#39;threshold_elgamal.sage&#39;) # Create an ElGamal key pair (pk, sk) = elgamal_keygen(256) # parse the public key p, q, g, h = (pk[&#39;p&#39;], pk[&#39;q&#39;], pk[&#39;g&#39;], pk[&#39;h&#39;]) Zp = Integers(p) # get also the field F_q # The key is an F_q element so this is the correct field F = GF(q) # Secret share the public key to 12 parties, 3 of which # must collaborate to decrypt shares = shamir_share(F, sk, 3, 12) # encrypt a random message # recall the message should be a *subgroup* element m = Zp.random_element() while m.multiplicative_order() != q: m = Zp.random_element() # encrypt the message (c1, c2) = elgamal_encrypt(pk, m) # parties 1, 6, 7 want to decrypt c # Party 1 computes the partial decryption y1 = shares[0][1] w1 = compute_weight(F, [6,7], 1, y1) # Partial decryption c1_1 = elgamal_partial_decrypt(c1, w1) # Party 6 computes the partial decryption y6 = shares[5][1] w6 = compute_weight(F, [1,7], 6, y6) # Partial decryption c1_6 = elgamal_partial_decrypt(c1, w6) # Party 7 computes the partial decryption y7 = shares[6][1] w7 = compute_weight(F, [1,6], 7, y7) # Partial decryption c1_7 = elgamal_partial_decrypt(c1, w7) # The parties share the partial decryptions with each other # Each party can now combine them alpha = c1_1 * c1_6 * c1_7 # and decrypt m_prime = c2 * alpha^(-1) # in case of lifted el gamal everything is the shame, except that # now we need to brute force c2 * c1^(-1) to get the actual message assert m == m_prime E.14 Zero Knowledge Proofs In this section we present the Schnorr protocol for proving knowledge of discrete logarithms. We do this over secp256k1 just for the fun of it. It is easy to adapt it to safe prime groups. Schnorr Protocol over secp256k1 We next present the prove and verify algorithms for the Schnorr protocol. To derive the Fiat-Shamir challenge we use the SHA256 implementation of hashlib. Note how we hash the curve specs and the statement to derive the challenge. The prover should always hash all the information exchanged by both parties! # HVZK proof to prove knowledge of the discrete logarithm of an # element in secp256k1 curve. SHA256 is used as a hash function # import sha256 from hashlib import sha256 ### def schnorr_prove(h, x): &quot;&quot;&quot; Create a proof pi that h = x*g &quot;&quot;&quot; # Define the curve and the scalar field (E, p, q, g) = secp256k1_params() Fq = GF(q) # We first sample a random element r and the element r*g r = Fq.random_element() u = r * g # We know should get the random challenge. We compute it # by hashing H(&quot;secp256k1&quot;, h, u). hash_input = &#39;secp256k1&#39; + str(h) + str(u) digest = sha256(hash_input.encode()).hexdigest() # q has ~256 bits and the digest is exactly 256 bits. We therefore # keep only 252 bits of the ouptut to create c. Note that we encode # the message as an Fq element c = Fq(int(&#39;0x&#39;+ digest[:-1], 16)) # Finally we create the last message z z = r + c*x # We output the proof. Note that c is not included since the verifier # can compute it on its own proof = (u, z) return proof def schnorr_verify(h, proof): &quot;&quot;&quot; verify a proof of knowledge of dlog of h &quot;&quot;&quot; # Define the curve and the scalar field (E, p, q, g) = secp256k1_params() Fq = GF(q) (u, z) = proof # We compute the hash as the prover given hash_input = &#39;secp256k1&#39; + str(h) + str(u) digest = sha256(hash_input.encode()).hexdigest() c = Fq(int(&#39;0x&#39;+ digest[:-1], 16)) # Finally we do the final verification by checking the # equation zg = ch+u return z*g == c*h + u Next we present an actual run of the protocol for a random statement. Note that this is exactly the proof of knowledge of an ElGamal secret key over secp256k1! # We use the schnorr recipe load(&#39;schnorr.sage&#39;) # define the curve and Fq (E, p, q, g) = secp256k1_params() Fq = GF(q) # sample a random element x = Fq.random_element() # define the statement h h = x*g # Create a proof of knowledge of dlog of h proof = schnorr_prove(h, x) # verify the proof assert schnorr_verify(h, proof) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
